{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from util.webscraper import WebScraper\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.2/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the data from the API. As the API is not yet published, both the API-Url and the query to get information on edition-software need to be specified in your .env file. (consult the README for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api_url and query\n",
    "api_url = os.environ['API_URL']\n",
    "query = os.environ['QUERY']\n",
    "\n",
    "# get data from api\n",
    "api_response = requests.get(api_url + query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got the data from the API, we can load it into a dataframe to prepare it to be used as a knowledge base for rag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43 entries, 0 to 42\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   id                43 non-null     object\n",
      " 1   slug              43 non-null     object\n",
      " 2   brand_name        43 non-null     object\n",
      " 3   concept_doi       0 non-null      object\n",
      " 4   description       40 non-null     object\n",
      " 5   description_url   3 non-null      object\n",
      " 6   description_type  43 non-null     object\n",
      " 7   get_started_url   41 non-null     object\n",
      " 8   image_id          31 non-null     object\n",
      " 9   is_published      43 non-null     bool  \n",
      " 10  short_statement   43 non-null     object\n",
      " 11  created_at        43 non-null     object\n",
      " 12  updated_at        43 non-null     object\n",
      " 13  closed_source     43 non-null     bool  \n",
      "dtypes: bool(2), object(12)\n",
      "memory usage: 4.2+ KB\n"
     ]
    }
   ],
   "source": [
    "edition_software_info = json.loads(api_response.text)\n",
    "edition_software_info = pd.DataFrame(edition_software_info)\n",
    "edition_software_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief inspection allows us to formulate some initial tasks and questions for this experiment.\n",
    "\n",
    "- **Preprocessing:** As we can see, not a single entry contains a associated concept_doi. We might consider dropping the column.\n",
    "- **Impact of using short descriptions only:** Three entries are missing the in depth description. We can assume that rag won't be too useful for these entries. \n",
    "- **Impact of additional information:** Only three have a description-url. Down the road, we need to evaluate, if adding info from this source improves the performance of the rag-system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove Artefacts\n",
    "\n",
    "Both the `description` and `short_statement` columns seem to be of particular interest for the task at hand. To asses necessary preprocessing step, we'll need to take a closer look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>short_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>The Research Software Directory is a content management system that is tailored to research software.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                             short_statement  \n",
       "0                                                                                                                          Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.  \n",
       "1                                  Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.  \n",
       "2          CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.  \n",
       "3  LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.  \n",
       "4                                                                                                                                                                                                      The Research Software Directory is a content management system that is tailored to research software.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = edition_software_info[[\"description\", \"short_statement\"]]\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(descriptions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `description` column contains some formatting artefacts like `\\n` and markdown syntax like `**` and `#`. Let's clean them up.\n",
    "While we're at it, we can also remove double whitespaces etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autodone</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LaTeX</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Software Directory</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    brand_name  \\\n",
       "0                  Transkribus   \n",
       "1                     Autodone   \n",
       "2                     CollateX   \n",
       "3                        LaTeX   \n",
       "4  Research Software Directory   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)   \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = '\\\\n+'\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "pattern = r'[*#]+|\\s-+\\s|]]' #\\[\\]()<>\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description_clean\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"brand_name\", \"description\", \"description_clean\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fill nan values\n",
    "\n",
    "Before we continue preprocessing the data for later vectorization, we need to check for missing values and replace them with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j42m3p7n5jl51rggmnpn_6940000gn/T/ipykernel_40936/3857300838.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide additional context-information for the retrieval process, we'll scrape all webpages referenced in the software-description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get urls\n",
    "\n",
    "First, we isolate the urls from our description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "      <td>https://autodone.idh.uni-koeln.de/usage,https://autodone.idh.uni-koeln.de/about,https://autodone.idh.uni-koeln.de/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>http://collatex.net/,http://en.wikipedia.org/wiki/Diff,http://en.wikipedia.org/wiki/Textual_criticism,http://en.wikipedia.org/wiki/Philology,http://en.wikipedia.org/wiki/Sequence_alignment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)    \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                           urls  \n",
       "0                                                                                                                                                                                           NaN  \n",
       "1                                                                            https://autodone.idh.uni-koeln.de/usage,https://autodone.idh.uni-koeln.de/about,https://autodone.idh.uni-koeln.de/  \n",
       "2  http://collatex.net/,http://en.wikipedia.org/wiki/Diff,http://en.wikipedia.org/wiki/Textual_criticism,http://en.wikipedia.org/wiki/Philology,http://en.wikipedia.org/wiki/Sequence_alignment  \n",
       "3                                                                                                                                                                                           NaN  \n",
       "4                                                                                                                                                                                           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "\n",
    "urls = edition_software_info[\"description\"].str.extractall(pattern)\n",
    "urls = urls.droplevel(1)\n",
    "urls_grouped = urls.groupby(urls.index).agg((lambda x: ','.join(set(x))))\n",
    "edition_software_info[\"urls\"] = urls_grouped\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"description_clean\", \"urls\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape Webpages\n",
    "\n",
    "Now we scrape the paragraphs from the webpages we found. \n",
    "The webscraper will take the list of urls associated with an entry and will save paragraphs from all webpages as a string in a column of our dataframe. \n",
    "\n",
    "**This might take some time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://autodone.idh.uni-koeln.de/usage with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/about with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/ with parameters tags = ['p']\n",
      "Scraping http://collatex.net/ with parameters tags = ['p']\n",
      "Scraping http://www.tei-c.org/ with parameters tags = ['p']\n",
      "Scraping http://vbd.humnet.unipi.it/ with parameters tags = ['p']\n",
      "Scraping https://sites.fastspring.com/stefanhagel/product/cte with parameters tags = ['p']\n",
      "Scraping http://csel.at/ with parameters tags = ['p']\n",
      "Scraping https://www.oeaw.ac.at/oesterreichische-akademie-der-wissenschaften with parameters tags = ['p']\n",
      "Scraping https://cte.oeaw.ac.at/ with parameters tags = ['p']\n",
      "Scraping https://opensource.org/licenses/EUPL-1.2 with parameters tags = ['p']\n",
      "Scraping https://phylipweb.github.io/phylip/general.html with parameters tags = ['p']\n",
      "Scraping https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html with parameters tags = ['p']\n",
      "HTTP error occurred for https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html: 404 Client Error: Not Found for url: https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html\n",
      "Scraping https://campus.dariah.eu/resource/posts/how-to-publish-your-dictionary-data-with-publex with parameters tags = ['p']\n",
      "Scraping http://publex.uni-trier.de/dictionary-network with parameters tags = ['p']\n",
      "HTTP error occurred for http://publex.uni-trier.de/dictionary-network: 503 Server Error: Service Unavailable for url: http://publex.uni-trier.de/dictionary-network\n",
      "Scraping https://bitbucket.org/tcdh/publex-allentities/src/master/ with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/publex with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/komplett with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/transcribo with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514 with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud with parameters tags = ['p']\n",
      "Scraping https://fud.uni-trier.de/community/referenzen/ with parameters tags = ['p']\n",
      "Scraping https://textgridrep.de/ with parameters tags = ['p']\n",
      "Scraping https://ride.i-d-e.de/issues/issue-11/omeka/ with parameters tags = ['p']\n",
      "Scraping https://viscoll.org with parameters tags = ['p']\n",
      "Scraping http://scta.lombardpress.org with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/check.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/download.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/witnesses.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/print.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/image.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/stats.php with parameters tags = ['p']\n"
     ]
    }
   ],
   "source": [
    "webscraper = WebScraper(tags = [\"p\"], exclude = [\"wikipedia\"])\n",
    "edition_software_info[\"webpages_text\"] = edition_software_info[\"urls\"].apply(lambda x: webscraper.scrape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>webpages_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://autodone.idh.uni-koeln.de/usage,https:...</td>\n",
       "      <td>On this page you will find instructions on how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://collatex.net/,http://en.wikipedia.org/w...</td>\n",
       "      <td>“In a language, in the system of language, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                urls  \\\n",
       "0                                                NaN   \n",
       "1  https://autodone.idh.uni-koeln.de/usage,https:...   \n",
       "2  http://collatex.net/,http://en.wikipedia.org/w...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                       webpages_text  \n",
       "0                                                NaN  \n",
       "1  On this page you will find instructions on how...  \n",
       "2  “In a language, in the system of language, the...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info[[\"urls\", \"webpages_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is collected from the webpages, we can take a look at the average length of the texts received for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       13.000000\n",
       "mean     15504.153846\n",
       "std      17593.002197\n",
       "min        640.000000\n",
       "25%       2300.000000\n",
       "50%       6739.000000\n",
       "75%      23578.000000\n",
       "max      55063.000000\n",
       "Name: webpages_text, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = edition_software_info[\"webpages_text\"].apply(lambda x: len(x) if not pd.isna(x) else 0)\n",
    "length[length>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking only at entries, that we were able to collected webpage text for, we have an average character count of about 15.000 per entry. \n",
    "The standard deviation is quite large compared to the mean, indicating that there is a high degree of variability in character counts.\n",
    "\n",
    "The distribution is skewed towards entries with lower character counts, while some outliers with a high character counts pull the mean upwards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, 'data/edition_software_info.csv')\n",
    "edition_software_info.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can preprocess the newfound text using the function we defined earlier. Again, we have to replace missing values with empty strings.\n",
    "\n",
    "First, let us reimport the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, \"data/edition_software_info.csv\")\n",
    "edition_software_info = pd.read_csv(path)\n",
    "edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]] = edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove links\n",
    "\n",
    "First, we'll remove all links from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Erkennen, Transkribieren und Durchsuchen von...\n",
       "1    autodone is a service for the automated, time-...\n",
       "2    [CollateX]() is a software to  1. read  multip...\n",
       "Name: description_preprocessed, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_clean\"].str.replace(pattern, '', regex=True)\n",
    "edition_software_info[\"description_preprocessed\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove Stopwords and Punctuation\n",
    "\n",
    "For the later vectorisation of the texts, we remove both german and english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(stopwords: List[str], text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    \n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    erkennen transkribieren durchsuchen historisch...\n",
       "1    autodone service automated timecontrolled publ...\n",
       "2    collatex software 1 read multiple ≥ 2 versions...\n",
       "3    mathematiker donald e knuth entwickelte ende s...\n",
       "4                                                     \n",
       "Name: description_preprocessed, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get stopwords\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "stopwords_combined = stopwords_german.union(stopwords_english)\n",
    "\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: preprocess(stopwords_combined, x))\n",
    "edition_software_info[\"description_preprocessed\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     \n",
       "1    page find instructions use autodone based scre...\n",
       "2    “in language system language differences”– jac...\n",
       "3                                                     \n",
       "4                                                     \n",
       "Name: webpages_text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info[\"webpages_text_preprocessed\"] = edition_software_info[\"webpages_text\"].apply(lambda x: preprocess(stopwords_combined, x) if not pd.isna(x) else \"\")\n",
    "edition_software_info[\"webpages_text_preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lemmatize\n",
    "\n",
    "Finally, we can lemmatize our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef lemmatize_english_text(text):\\n    doc = nlp_en(text)\\n    return \\' \\'.join([token.lemma_ for token in doc])\\n\\ndef lemmatize_german_text(text):\\n    doc = nlp_de(text)\\n    return \\' \\'.join([token.lemma_ for token in doc])\\n\\nedition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_english_text(x))\\nedition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_german_text(x))\\nedition_software_info[\"description_preprocessed\"].head()\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def lemmatize_english_text(text):\n",
    "    doc = nlp_en(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def lemmatize_german_text(text):\n",
    "    doc = nlp_de(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_english_text(x))\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_german_text(x))\n",
    "edition_software_info[\"description_preprocessed\"].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 1: TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with a simple TF-IDF vectorization.\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)** is a weighting scheme that weights the cells of a term-document matrix by their potential to be discriminatory.\n",
    "\n",
    "To do so, we first calculate the **term frequency (TF)**. The term frequency represents the number of instances of a given word $t$ in a document $d$.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Count of } t \\text{ in } d}{\\text{Total number of words in } d}\n",
    "$$\n",
    "\n",
    "This term frequency is then multiplied by the **inverse document frequency (IDF)**. The IDF is calculated by counting all documents that contain a term $t$ (the document frequency $\\text{df}(t)$). Then, we divide the total number of documents $N$ in the corpus by $\\text{df}(t)$.\n",
    "\n",
    "This inverse frequency is chosen over the regular frequency to **downweight** terms that appear in many documents, since these terms are less likely to be useful for distinguishing between documents.\n",
    "\n",
    "Usually, we also take the logarithm of the IDF to smooth out the very large values that can occur when a term appears in only a few documents. This ensures that rare terms are not excessively weighted.\n",
    "\n",
    "$$\n",
    "\\text{df}(t) = \\text{Document frequency of a term } t\n",
    "$$\n",
    "$$\n",
    "N = \\text{Number of documents}\n",
    "$$\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{\\text{df}(t)}\\right)\n",
    "$$\n",
    "\n",
    "Finally, we calculate the **TF-IDF** by multiplying the term frequency $\\text{TF}(t, d)$ with the inverse document frequency $\\text{IDF}(t)$.\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "The resulting value can be interpreted as a measure of the importance of the term in a document relative to the entire corpus. Terms that are frequent in a document but rare across the corpus will have higher TF-IDF scores, indicating their importance.\n",
    "\n",
    "\n",
    "**N-grams:**\n",
    "\n",
    "To capture not just the importance of single words but also some of the **context** in which they are used, we can apply TF-IDF to **n-grams**. N-grams are contiguous sequences of $n$ words that appear together in a text. The size of the sequence, $n$, is a hyperparameter that can be adjusted depending on the specific task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Fit TF-IDF Vectorizer\n",
    "First, we fit the vectorizer on the preprocessed descriptions. \n",
    "This way, the vectorizer can transform text into numerical feature vectors based on the learned vocabulary and its distribution over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12</th>\n",
       "      <th>12 languages</th>\n",
       "      <th>12 languages comprehensive</th>\n",
       "      <th>12 languages comprehensive righttoleft</th>\n",
       "      <th>1514</th>\n",
       "      <th>1514 digital</th>\n",
       "      <th>1514 digital marburg</th>\n",
       "      <th>1514 digital marburg büchner</th>\n",
       "      <th>19042024</th>\n",
       "      <th>19042024 official</th>\n",
       "      <th>...</th>\n",
       "      <th>überführt neben klassischen</th>\n",
       "      <th>überführt neben klassischen pdfformat</th>\n",
       "      <th>überschaubaren</th>\n",
       "      <th>überschaubaren teil</th>\n",
       "      <th>überschaubaren teil funktionen</th>\n",
       "      <th>überschaubaren teil funktionen aufgrund</th>\n",
       "      <th>überwiegende</th>\n",
       "      <th>überwiegende teil</th>\n",
       "      <th>überwiegende teil funktionen</th>\n",
       "      <th>überwiegende teil funktionen editionen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    12  12 languages  12 languages comprehensive  \\\n",
       "0  0.0           0.0                         0.0   \n",
       "1  0.0           0.0                         0.0   \n",
       "2  0.0           0.0                         0.0   \n",
       "3  0.0           0.0                         0.0   \n",
       "4  0.0           0.0                         0.0   \n",
       "\n",
       "   12 languages comprehensive righttoleft  1514  1514 digital  \\\n",
       "0                                     0.0   0.0           0.0   \n",
       "1                                     0.0   0.0           0.0   \n",
       "2                                     0.0   0.0           0.0   \n",
       "3                                     0.0   0.0           0.0   \n",
       "4                                     0.0   0.0           0.0   \n",
       "\n",
       "   1514 digital marburg  1514 digital marburg büchner  19042024  \\\n",
       "0                   0.0                           0.0  0.000000   \n",
       "1                   0.0                           0.0  0.064201   \n",
       "2                   0.0                           0.0  0.000000   \n",
       "3                   0.0                           0.0  0.000000   \n",
       "4                   0.0                           0.0  0.000000   \n",
       "\n",
       "   19042024 official  ...  überführt neben klassischen  \\\n",
       "0           0.000000  ...                          0.0   \n",
       "1           0.064201  ...                          0.0   \n",
       "2           0.000000  ...                          0.0   \n",
       "3           0.000000  ...                          0.0   \n",
       "4           0.000000  ...                          0.0   \n",
       "\n",
       "   überführt neben klassischen pdfformat  überschaubaren  überschaubaren teil  \\\n",
       "0                                    0.0             0.0                  0.0   \n",
       "1                                    0.0             0.0                  0.0   \n",
       "2                                    0.0             0.0                  0.0   \n",
       "3                                    0.0             0.0                  0.0   \n",
       "4                                    0.0             0.0                  0.0   \n",
       "\n",
       "   überschaubaren teil funktionen  überschaubaren teil funktionen aufgrund  \\\n",
       "0                             0.0                                      0.0   \n",
       "1                             0.0                                      0.0   \n",
       "2                             0.0                                      0.0   \n",
       "3                             0.0                                      0.0   \n",
       "4                             0.0                                      0.0   \n",
       "\n",
       "   überwiegende  überwiegende teil  überwiegende teil funktionen  \\\n",
       "0           0.0                0.0                           0.0   \n",
       "1           0.0                0.0                           0.0   \n",
       "2           0.0                0.0                           0.0   \n",
       "3           0.0                0.0                           0.0   \n",
       "4           0.0                0.0                           0.0   \n",
       "\n",
       "   überwiegende teil funktionen editionen  \n",
       "0                                     0.0  \n",
       "1                                     0.0  \n",
       "2                                     0.0  \n",
       "3                                     0.0  \n",
       "4                                     0.0  \n",
       "\n",
       "[5 rows x 18228 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(edition_software_info[\"description_preprocessed\"])\n",
    "\n",
    "# display the resulting matrix\n",
    "tfidf_matrix_beautify = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_matrix_beautify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Inspect the tf-idf representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in this dataframe is a unique word, while each row is a document. The cells denote the number of occurances of a word in a document, weighted by the words potential to be distinctive.\n",
    "\n",
    "Let's take a look at the tf-idf filtered words for each description (You can find them in the column \"tf_idf_filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>tf_idf_filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>[(dokumenten, 0.24192478472674683), (durchsuchen, 0.12096239236337342), (erkennen, 0.12096239236337342), (erkennung, 0.12096239236337342), (transkribieren, 0.12096239236337342)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "      <td>[(twitter, 0.1926029459944442), (autodone, 0.1284019639962961), (usage, 0.11567598600740456), (service, 0.10664676093079592)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>[(differences, 0.12889590979174362), (computational, 0.10345985709520088), (similarities, 0.10345985709520088), (similarities differences, 0.10345985709520088), (tokens, 0.10345985709520088)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>[(jahre, 0.1928958299837409)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)    \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                             tf_idf_filtered_words  \n",
       "0                [(dokumenten, 0.24192478472674683), (durchsuchen, 0.12096239236337342), (erkennen, 0.12096239236337342), (erkennung, 0.12096239236337342), (transkribieren, 0.12096239236337342)]  \n",
       "1                                                                    [(twitter, 0.1926029459944442), (autodone, 0.1284019639962961), (usage, 0.11567598600740456), (service, 0.10664676093079592)]  \n",
       "2  [(differences, 0.12889590979174362), (computational, 0.10345985709520088), (similarities, 0.10345985709520088), (similarities differences, 0.10345985709520088), (tokens, 0.10345985709520088)]  \n",
       "3                                                                                                                                                                    [(jahre, 0.1928958299837409)]  \n",
       "4                                                                                                                                                                                               []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the threshold for TF-IDF scores\n",
    "threshold = 0.1\n",
    "\n",
    "# Filter words with TF-IDF scores greater than the threshold for each document\n",
    "def filter_words_by_threshold(row, threshold):\n",
    "    filtered_words = [(word, score) for word, score in zip(tfidf_matrix_beautify.columns, row) if score > threshold]\n",
    "    return sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Apply the function to each row of the TF-IDF DataFrame\n",
    "filtered_words = tfidf_matrix_beautify.apply(lambda row: filter_words_by_threshold(row, threshold), axis=1)\n",
    "\n",
    "# Create a dataframe to display the filtered words\n",
    "filtered_words_df = pd.DataFrame(filtered_words, columns=[\"tf_idf_filtered_words\"])\n",
    "tfidf_display = pd.concat([edition_software_info[\"description_clean\"], filtered_words_df], axis=1)\n",
    "    \n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(tfidf_display[[\"description_clean\", \"tf_idf_filtered_words\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test tfidf-representation\n",
    "\n",
    "This cell will return the most relevant documents from our dataset based on a comparison of their tf-idf representations and a query. The query can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)</td>\n",
       "      <td>0.091537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Scripto is an open-source tool that permits registered users to view digital files and transcribe them with an easy-to-use toolbar, rendering that text searchable. The tool includes a versioning history and editorial controls to make public contributions more manageable, and supports the transcription of a wide range of file types (both images and documents). There are two versions of Scripto, each of which works with a different version of Omeka. Scripto for Omeka Classic creates a single transcription project for the content of your Omeka Classic site. Scripto for Omeka S enables the creation of multiple projects built from shared items in your Omeka S installation.</td>\n",
       "      <td>0.066523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>The Virtual Manuscript Room Collaborative Research Environment (VMR CRE) brings community and a toolbox of powerful research components to support all stages of research and production of a digital edition. Beginning with the popular open-source portal, Liferay, the VMR CRE integrates 30 DH components to naturally support: Cataloging witnesses; managing and displaying images; producing well-formed TEI transcriptions using a web-based WYSIWYM editor and storing those transcriptions to a versioned transcription repository; community volunteer task assignment and project management; automatic realtime collation of witnesses; regularization and apparatus editing; online publishing of the final results-- as a traditional apparatus, or with interactive tools which let users choose different ways to visualize the data produced in the edition.    Overview A walk through the workflow at the Institut für neutestamentliche Textforschung (INTF), in their efforts to edit the Editio Critica Maior (ECM), provides opportunity to touch on many components available in the VMR CRE.  Work can be divided into 9 discrete stages, progressively: 1) witness cataloging; 2) witness selection; 3) image management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus.   Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page.  For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features.  These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF.  A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be.  Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility.  For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John.  A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present.  If a region box is captured, a search query can specify to show the region box clips in the result.  For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips.   Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham.  This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component.  The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes.  The content may then be obtained as EpiDoc influenced TEI. The VMR CRE saves content in a versioned transcription repository backed by Git.  A user may have access to create and edit their own personal transcription, a project-wide transcription, or a site-wide (= published) transcription-- each having version history. The VMR CRE also includes a palaeography tool to assist a transcriber when encountering rare symbols, abbreviations, or ligatures.  If a portion of the unknown text can be identified, the researcher can enter one or more letters and will be presented with images of text instances elsewhere which include these letters, offering possibilities.  As more and more rare text items are tagged, the system grows more helpful. Quality assurance for the ECM requires that a transcription for a manuscript be produced independently by two transcribers.  The products are then compared to each other and differences are reviewed by a manager and reconciled to produce a final transcription.  The VMR CRE provides tools to facilitate this reconciliation work. Collation and Visualization Collation is a key component to find differences in text witnesses when producing a critical edition.  Collation facilities in the VMR CRE are performed by CollateX.  Collation and regularization of uninteresting differences is an iterative cycle in digital editing and the VMR CRE ties these two actions together with an intuitive visual interface.  Visualization of a collation, either during the editing process or for the reader, can be rendered as a variant graph, an alignment table, or as a traditional negative apparatus.    Web Services, Open Programmatic Access The VMR CRE Web Services API layer is primarily useful for exposing the functionality of the VMR CRE to other research projects wishing to access the functionality or contribute to the dataset through their own systems and tools.  The VMR CRE Web Services API generally uses noun/verb nomenclature organized by category.  This means that the last 2 segments of an API URL will consist first of the type of object the call will affect, and second, of the action to be performed on the object.  Any path before the final two segments are merely for organizational purposes.  This is different from a strict REST convention which confines the action to one of 6 HTTP verbs.  The VMR CRE places no semantic meaning on the HTTP verb.  Both GET and POST HTTP verbs are accepted as identical, relegating the verb, or semantic action to the final segment of the URL.  This allows easy testing and examples for every action directly within a web browser.  Parameters to a service request are passed as standard HTTP FORM POST parameters or as query string parameters.</td>\n",
       "      <td>0.065983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          description_clean  \\\n",
       "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)    \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Scripto is an open-source tool that permits registered users to view digital files and transcribe them with an easy-to-use toolbar, rendering that text searchable. The tool includes a versioning history and editorial controls to make public contributions more manageable, and supports the transcription of a wide range of file types (both images and documents). There are two versions of Scripto, each of which works with a different version of Omeka. Scripto for Omeka Classic creates a single transcription project for the content of your Omeka Classic site. Scripto for Omeka S enables the creation of multiple projects built from shared items in your Omeka S installation.    \n",
       "42  The Virtual Manuscript Room Collaborative Research Environment (VMR CRE) brings community and a toolbox of powerful research components to support all stages of research and production of a digital edition. Beginning with the popular open-source portal, Liferay, the VMR CRE integrates 30 DH components to naturally support: Cataloging witnesses; managing and displaying images; producing well-formed TEI transcriptions using a web-based WYSIWYM editor and storing those transcriptions to a versioned transcription repository; community volunteer task assignment and project management; automatic realtime collation of witnesses; regularization and apparatus editing; online publishing of the final results-- as a traditional apparatus, or with interactive tools which let users choose different ways to visualize the data produced in the edition.    Overview A walk through the workflow at the Institut für neutestamentliche Textforschung (INTF), in their efforts to edit the Editio Critica Maior (ECM), provides opportunity to touch on many components available in the VMR CRE.  Work can be divided into 9 discrete stages, progressively: 1) witness cataloging; 2) witness selection; 3) image management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus.   Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page.  For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features.  These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF.  A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be.  Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility.  For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John.  A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present.  If a region box is captured, a search query can specify to show the region box clips in the result.  For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips.   Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham.  This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component.  The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes.  The content may then be obtained as EpiDoc influenced TEI. The VMR CRE saves content in a versioned transcription repository backed by Git.  A user may have access to create and edit their own personal transcription, a project-wide transcription, or a site-wide (= published) transcription-- each having version history. The VMR CRE also includes a palaeography tool to assist a transcriber when encountering rare symbols, abbreviations, or ligatures.  If a portion of the unknown text can be identified, the researcher can enter one or more letters and will be presented with images of text instances elsewhere which include these letters, offering possibilities.  As more and more rare text items are tagged, the system grows more helpful. Quality assurance for the ECM requires that a transcription for a manuscript be produced independently by two transcribers.  The products are then compared to each other and differences are reviewed by a manager and reconciled to produce a final transcription.  The VMR CRE provides tools to facilitate this reconciliation work. Collation and Visualization Collation is a key component to find differences in text witnesses when producing a critical edition.  Collation facilities in the VMR CRE are performed by CollateX.  Collation and regularization of uninteresting differences is an iterative cycle in digital editing and the VMR CRE ties these two actions together with an intuitive visual interface.  Visualization of a collation, either during the editing process or for the reader, can be rendered as a variant graph, an alignment table, or as a traditional negative apparatus.    Web Services, Open Programmatic Access The VMR CRE Web Services API layer is primarily useful for exposing the functionality of the VMR CRE to other research projects wishing to access the functionality or contribute to the dataset through their own systems and tools.  The VMR CRE Web Services API generally uses noun/verb nomenclature organized by category.  This means that the last 2 segments of an API URL will consist first of the type of object the call will affect, and second, of the action to be performed on the object.  Any path before the final two segments are merely for organizational purposes.  This is different from a strict REST convention which confines the action to one of 6 HTTP verbs.  The VMR CRE places no semantic meaning on the HTTP verb.  Both GET and POST HTTP verbs are accepted as identical, relegating the verb, or semantic action to the final segment of the URL.  This allows easy testing and examples for every action directly within a web browser.  Parameters to a service request are passed as standard HTTP FORM POST parameters or as query string parameters.     \n",
       "\n",
       "    similarity_score  \n",
       "22          0.091537  \n",
       "38          0.066523  \n",
       "42          0.065983  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I am looking for a transcription tool'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# Transform the query to TF-IDF space\n",
    "query_tfidf = tfidf_vectorizer.transform([query]) \n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities[0]\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['description_clean', 'similarity_score']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 2: Aggregated Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create document representations by aggregating the word2vec embeddings of each word in a description. \n",
    "\n",
    "Word2vec encodes the meaning of the words by capturing their semantic relationships based on the context in which they appear. By aggregating the word2vec embeddings of each word in a description, we can create a document representation that retains the semantic information and provides a more nuanced understanding of the content.\n",
    "\n",
    "From a computational perspective, these representations are shorter and denser than tf-idf representations, making them more suitable for computations such as similarity measures, clustering, or classification tasks. The dense nature of word2vec embeddings allows for efficient storage and faster processing compared to sparse representations like tf-idf. Additionally, because word2vec captures the meaning and context of words, it can provide more meaningful insights into the relationships between different documents or terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "path = os.path.join(current_path, \"models/word2vec-google-news-300.bin\")\n",
    "\n",
    "# Load the model if it is already in our project. If not, download it.\n",
    "if os.path.isfile(path):\n",
    "    print(\"Model found. Loading...\")\n",
    "    word2vec_model = KeyedVectors.load(path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    word2vec_model.save(path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #TODO: Add some preprocessing\\ndef preprocess_word2vec(text: str) -> str:\\n    pass\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #TODO: Add some preprocessing\n",
    "def preprocess_word2vec(text: str) -> str:\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word2vec_vector(words, model):\n",
    "    words = words.split()\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in model]\n",
    "    \n",
    "    if not valid_words:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the vectors of the valid words to create a document representation\n",
    "    vectors = [model[word] for word in valid_words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply the function to create aggregated vectors\n",
    "word2vec = edition_software_info['description_preprocessed'].apply(lambda x: get_word2vec_vector(x, word2vec_model))\n",
    "\n",
    "# Convert the Series of 1D arrays to a 2D numpy array (to calculate the cosine similarity later on)\n",
    "word2vec_array = np.array(word2vec.tolist())\n",
    "len(word2vec_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Word2Vec Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>T-Pen</td>\n",
       "      <td>T‑PEN... Is an open and general tool for scholars of any technical expertise level Allows transcriptions to be created, manipulated, and viewed in many ways Collaborate with others through simple project management Exports transcriptions as a pdf, XML(plaintext) for further processing, or contribute to a collaborating institution with a click Respects existing and emerging standards for text, image, and annotation data storage Avoids prejudice in data, allowing users to find new ways to work   The Transcription for Paleographical and Editorial Notation (T‑PEN) project was coordinated by the Center for Digital Theology at Saint Louis University (SLU) and funded by the Andrew W. Mellon Foundation and the NEH. The Electronic Norman Anonymous Project developed several abilities at the core of this project's functionality.</td>\n",
       "      <td>0.655957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Transcribo</td>\n",
       "      <td>[Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)</td>\n",
       "      <td>0.652115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TEITOK</td>\n",
       "      <td>TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation, initially developed at the Centro de Linguística da Universidade de Lisboa, later at CELGA-ILTEC, and currently maintained at the ÚFAL institute of Charles University, Prague. The system has a modular design with numerous modules making serving a wide range of different corpus types.  Below are some examples of some of those, and the type of corpora TEITOK can deal with. More modules are added frequently, and it is possible to add custom modules as well.  Historical Corpora  For historical corpora, TEITOK provides the option to have an alignment between the transcription and the facsimile image, it provides the option to work with multiple orthographic realizations to combine several editions of a text into a single XML file, and it provides the option to create a searchable document map to see where in the world several phenomena are more frequent.  TEITOK is freely available for anybody who wishes to create richly annotated textual corpora, and runs on any LINUX based web server.    Features    Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment   Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus   Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments   Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations   Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map   Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression   Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the original project.</td>\n",
       "      <td>0.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Scripto</td>\n",
       "      <td>Scripto is an open-source tool that permits registered users to view digital files and transcribe them with an easy-to-use toolbar, rendering that text searchable. The tool includes a versioning history and editorial controls to make public contributions more manageable, and supports the transcription of a wide range of file types (both images and documents). There are two versions of Scripto, each of which works with a different version of Omeka. Scripto for Omeka Classic creates a single transcription project for the content of your Omeka Classic site. Scripto for Omeka S enables the creation of multiple projects built from shared items in your Omeka S installation.</td>\n",
       "      <td>0.631786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Monasterium - Collaborative Archive</td>\n",
       "      <td>MOM-CA is a collaborative database system which allows for the research and editing of medieval and early modern charters.  By and large you can find the charters organized according to archival \"fonds\"; charters that originate from different sources are subsumed under \"Collections\".   General users can:  browse  search  Registered users can:  bookmark documents save and edit documents (description, transcription) with EditMOM3 create indices manipulate images (annotate images, create collections of image fragments, manipulate image fragments) create own charter collections  Special users can:  review user changes (moderators only) manage metadata on archives and collections (metadata manager only) import new data (metadata manager only) translate system messages (translators only) manage users (user manager only) manage static texts (like help, introduction etc., html-authors only)</td>\n",
       "      <td>0.629680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             brand_name  \\\n",
       "27                                T-Pen   \n",
       "22                           Transcribo   \n",
       "31                               TEITOK   \n",
       "38                              Scripto   \n",
       "40  Monasterium - Collaborative Archive   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             description_clean  \\\n",
       "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              T‑PEN... Is an open and general tool for scholars of any technical expertise level Allows transcriptions to be created, manipulated, and viewed in many ways Collaborate with others through simple project management Exports transcriptions as a pdf, XML(plaintext) for further processing, or contribute to a collaborating institution with a click Respects existing and emerging standards for text, image, and annotation data storage Avoids prejudice in data, allowing users to find new ways to work   The Transcription for Paleographical and Editorial Notation (T‑PEN) project was coordinated by the Center for Digital Theology at Saint Louis University (SLU) and funded by the Andrew W. Mellon Foundation and the NEH. The Electronic Norman Anonymous Project developed several abilities at the core of this project's functionality.    \n",
       "22  [Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)    \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation, initially developed at the Centro de Linguística da Universidade de Lisboa, later at CELGA-ILTEC, and currently maintained at the ÚFAL institute of Charles University, Prague. The system has a modular design with numerous modules making serving a wide range of different corpus types.  Below are some examples of some of those, and the type of corpora TEITOK can deal with. More modules are added frequently, and it is possible to add custom modules as well.  Historical Corpora  For historical corpora, TEITOK provides the option to have an alignment between the transcription and the facsimile image, it provides the option to work with multiple orthographic realizations to combine several editions of a text into a single XML file, and it provides the option to create a searchable document map to see where in the world several phenomena are more frequent.  TEITOK is freely available for anybody who wishes to create richly annotated textual corpora, and runs on any LINUX based web server.    Features    Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment   Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus   Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments   Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations   Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map   Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression   Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the original project.      \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Scripto is an open-source tool that permits registered users to view digital files and transcribe them with an easy-to-use toolbar, rendering that text searchable. The tool includes a versioning history and editorial controls to make public contributions more manageable, and supports the transcription of a wide range of file types (both images and documents). There are two versions of Scripto, each of which works with a different version of Omeka. Scripto for Omeka Classic creates a single transcription project for the content of your Omeka Classic site. Scripto for Omeka S enables the creation of multiple projects built from shared items in your Omeka S installation.    \n",
       "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            MOM-CA is a collaborative database system which allows for the research and editing of medieval and early modern charters.  By and large you can find the charters organized according to archival \"fonds\"; charters that originate from different sources are subsumed under \"Collections\".   General users can:  browse  search  Registered users can:  bookmark documents save and edit documents (description, transcription) with EditMOM3 create indices manipulate images (annotate images, create collections of image fragments, manipulate image fragments) create own charter collections  Special users can:  review user changes (moderators only) manage metadata on archives and collections (metadata manager only) import new data (metadata manager only) translate system messages (translators only) manage users (user manager only) manage static texts (like help, introduction etc., html-authors only)    \n",
       "\n",
       "    similarity_score  \n",
       "27          0.655957  \n",
       "22          0.652115  \n",
       "31          0.635900  \n",
       "38          0.631786  \n",
       "40          0.629680  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I need help digitizing a document'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# get vector representation of the query using word2vec\n",
    "query_word2vec = get_word2vec_vector(query, word2vec_model)\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_word2vec = query_word2vec.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_word2vec, word2vec_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"brand_name\",\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 3: FastText\n",
    "\n",
    "One downside of pretrained Word2Vec representations is their inability to handle unknown words. \n",
    "FastText models clost this gap, by encoding words both as embeddings and collections of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

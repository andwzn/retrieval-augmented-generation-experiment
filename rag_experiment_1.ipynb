{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and create frequently used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from util.webscraper import WebScraper\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to load the FastText models\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"File was already downloaded.\")\n",
    "            return None\n",
    "        \n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"The file has been downloaded and saved as: {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred while downloading the file: {e}\")\n",
    "        \n",
    "def load_word_vectors(file_path: str):\n",
    "    \"\"\"Load word vectors from a file.\"\"\"\n",
    "    try:\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        print(\"Vectors loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the vectors: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.2/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the data from the API. As the API is not yet published, both the API-Url and the query to get information on edition-software need to be specified in your .env file. (consult the README for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api_url and query\n",
    "api_url = os.environ['API_URL']\n",
    "query = os.environ['QUERY']\n",
    "\n",
    "# get data from api\n",
    "api_response = requests.get(api_url + query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got the data from the API, we can load it into a dataframe to prepare it to be used as a knowledge base for rag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43 entries, 0 to 42\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   id                43 non-null     object\n",
      " 1   slug              43 non-null     object\n",
      " 2   brand_name        43 non-null     object\n",
      " 3   concept_doi       0 non-null      object\n",
      " 4   description       40 non-null     object\n",
      " 5   description_url   3 non-null      object\n",
      " 6   description_type  43 non-null     object\n",
      " 7   get_started_url   41 non-null     object\n",
      " 8   image_id          31 non-null     object\n",
      " 9   is_published      43 non-null     bool  \n",
      " 10  short_statement   43 non-null     object\n",
      " 11  created_at        43 non-null     object\n",
      " 12  updated_at        43 non-null     object\n",
      " 13  closed_source     43 non-null     bool  \n",
      "dtypes: bool(2), object(12)\n",
      "memory usage: 4.2+ KB\n"
     ]
    }
   ],
   "source": [
    "edition_software_info = json.loads(api_response.text)\n",
    "edition_software_info = pd.DataFrame(edition_software_info)\n",
    "edition_software_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief inspection allows us to formulate some initial tasks and questions for this experiment.\n",
    "\n",
    "- **Preprocessing:** As we can see, not a single entry contains a associated concept_doi. We might consider dropping the column.\n",
    "- **Impact of using short descriptions only:** Three entries are missing the in depth description. We can assume that rag won't be too useful for these entries. \n",
    "- **Impact of additional information:** Only three have a description-url. Down the road, we need to evaluate, if adding info from this source improves the performance of the rag-system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove Artefacts\n",
    "\n",
    "Both the `description` and `short_statement` columns seem to be of particular interest for the task at hand. To asses necessary preprocessing step, we'll need to take a closer look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>short_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>The Research Software Directory is a content management system that is tailored to research software.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                             short_statement  \n",
       "0                                                                                                                          Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.  \n",
       "1                                  Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.  \n",
       "2          CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.  \n",
       "3  LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.  \n",
       "4                                                                                                                                                                                                      The Research Software Directory is a content management system that is tailored to research software.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = edition_software_info[[\"description\", \"short_statement\"]]\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(descriptions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `description` column contains some formatting artefacts like `\\n` and markdown syntax like `**` and `#`. Let's clean them up.\n",
    "While we're at it, we can also remove double whitespaces etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autodone</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LaTeX</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Software Directory</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    brand_name  \\\n",
       "0                  Transkribus   \n",
       "1                     Autodone   \n",
       "2                     CollateX   \n",
       "3                        LaTeX   \n",
       "4  Research Software Directory   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)   \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = '\\\\n+'\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "pattern = r'[*#]+|\\s-+\\s|]]' #\\[\\]()<>\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description_clean\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"brand_name\", \"description\", \"description_clean\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fill nan values\n",
    "\n",
    "Before we continue preprocessing the data for later vectorization, we need to check for missing values and replace them with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j42m3p7n5jl51rggmnpn_6940000gn/T/ipykernel_87849/3857300838.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide additional context-information for the retrieval process, we'll scrape all webpages referenced in the software-description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get urls\n",
    "\n",
    "First, we isolate the urls from our description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "      <td>https://autodone.idh.uni-koeln.de/usage,https://autodone.idh.uni-koeln.de/,https://autodone.idh.uni-koeln.de/about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Textual_criticism,http://collatex.net/,http://en.wikipedia.org/wiki/Sequence_alignment,http://en.wikipedia.org/wiki/Diff,http://en.wikipedia.org/wiki/Philology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)    \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                           urls  \n",
       "0                                                                                                                                                                                           NaN  \n",
       "1                                                                            https://autodone.idh.uni-koeln.de/usage,https://autodone.idh.uni-koeln.de/,https://autodone.idh.uni-koeln.de/about  \n",
       "2  http://en.wikipedia.org/wiki/Textual_criticism,http://collatex.net/,http://en.wikipedia.org/wiki/Sequence_alignment,http://en.wikipedia.org/wiki/Diff,http://en.wikipedia.org/wiki/Philology  \n",
       "3                                                                                                                                                                                           NaN  \n",
       "4                                                                                                                                                                                           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "\n",
    "urls = edition_software_info[\"description\"].str.extractall(pattern)\n",
    "urls = urls.droplevel(1)\n",
    "urls_grouped = urls.groupby(urls.index).agg((lambda x: ','.join(set(x))))\n",
    "edition_software_info[\"urls\"] = urls_grouped\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"description_clean\", \"urls\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape Webpages\n",
    "\n",
    "Now we scrape the paragraphs from the webpages we found. \n",
    "The webscraper will take the list of urls associated with an entry and will save paragraphs from all webpages as a string in a column of our dataframe. \n",
    "\n",
    "**This might take some time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://autodone.idh.uni-koeln.de/usage with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/ with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/about with parameters tags = ['p']\n",
      "Scraping http://collatex.net/ with parameters tags = ['p']\n",
      "Scraping http://www.tei-c.org/ with parameters tags = ['p']\n",
      "Scraping http://vbd.humnet.unipi.it/ with parameters tags = ['p']\n",
      "Scraping https://opensource.org/licenses/EUPL-1.2 with parameters tags = ['p']\n",
      "Scraping https://www.oeaw.ac.at/oesterreichische-akademie-der-wissenschaften with parameters tags = ['p']\n",
      "Scraping https://cte.oeaw.ac.at/ with parameters tags = ['p']\n",
      "Scraping http://csel.at/ with parameters tags = ['p']\n",
      "Scraping https://sites.fastspring.com/stefanhagel/product/cte with parameters tags = ['p']\n",
      "Scraping https://phylipweb.github.io/phylip/general.html with parameters tags = ['p']\n",
      "Scraping https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html with parameters tags = ['p']\n",
      "HTTP error occurred for https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html: 404 Client Error: Not Found for url: https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/publex with parameters tags = ['p']\n",
      "Scraping https://bitbucket.org/tcdh/publex-allentities/src/master/ with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/komplett with parameters tags = ['p']\n",
      "Scraping http://publex.uni-trier.de/dictionary-network with parameters tags = ['p']\n",
      "HTTP error occurred for http://publex.uni-trier.de/dictionary-network: 503 Server Error: Service Unavailable for url: http://publex.uni-trier.de/dictionary-network\n",
      "Scraping https://campus.dariah.eu/resource/posts/how-to-publish-your-dictionary-data-with-publex with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/transcribo with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514 with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud with parameters tags = ['p']\n",
      "Scraping https://fud.uni-trier.de/community/referenzen/ with parameters tags = ['p']\n",
      "Scraping https://textgridrep.de/ with parameters tags = ['p']\n",
      "Scraping https://ride.i-d-e.de/issues/issue-11/omeka/ with parameters tags = ['p']\n",
      "Scraping https://viscoll.org with parameters tags = ['p']\n",
      "Scraping http://scta.lombardpress.org with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/check.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/image.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/print.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/download.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/witnesses.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/stats.php with parameters tags = ['p']\n"
     ]
    }
   ],
   "source": [
    "webscraper = WebScraper(tags = [\"p\"], exclude = [\"wikipedia\"])\n",
    "edition_software_info[\"webpages_text\"] = edition_software_info[\"urls\"].apply(lambda x: webscraper.scrape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>webpages_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://autodone.idh.uni-koeln.de/usage,https:...</td>\n",
       "      <td>On this page you will find instructions on how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://en.wikipedia.org/wiki/Textual_criticism...</td>\n",
       "      <td>“In a language, in the system of language, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                urls  \\\n",
       "0                                                NaN   \n",
       "1  https://autodone.idh.uni-koeln.de/usage,https:...   \n",
       "2  http://en.wikipedia.org/wiki/Textual_criticism...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                       webpages_text  \n",
       "0                                                NaN  \n",
       "1  On this page you will find instructions on how...  \n",
       "2  “In a language, in the system of language, the...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info[[\"urls\", \"webpages_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is collected from the webpages, we can take a look at the average length of the texts received for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       13.000000\n",
       "mean     15502.692308\n",
       "std      17592.276342\n",
       "min        640.000000\n",
       "25%       2300.000000\n",
       "50%       6739.000000\n",
       "75%      23559.000000\n",
       "max      55063.000000\n",
       "Name: webpages_text, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = edition_software_info[\"webpages_text\"].apply(lambda x: len(x) if not pd.isna(x) else 0)\n",
    "length[length>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking only at entries, that we were able to collected webpage text for, we have an average character count of about 15.000 per entry. \n",
    "The standard deviation is quite large compared to the mean, indicating that there is a high degree of variability in character counts.\n",
    "\n",
    "The distribution is skewed towards entries with lower character counts, while some outliers with a high character counts pull the mean upwards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, 'data/edition_software_info.csv')\n",
    "edition_software_info.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can preprocess the newfound text using the function we defined earlier. Again, we have to replace missing values with empty strings.\n",
    "\n",
    "First, let us reimport the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, \"data/edition_software_info.csv\")\n",
    "edition_software_info = pd.read_csv(path)\n",
    "edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]] = edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove links\n",
    "\n",
    "First, we'll remove all links from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Erkennen, Transkribieren und Durchsuchen von...\n",
       "1    autodone is a service for the automated, time-...\n",
       "2    [CollateX]() is a software to  1. read  multip...\n",
       "Name: description_preprocessed, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_clean\"].str.replace(pattern, '', regex=True)\n",
    "edition_software_info[\"description_preprocessed\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove Stopwords and Punctuation\n",
    "\n",
    "For the later vectorisation of the texts, we remove both german and english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(stopwords: List[str], text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by converting it to lowercase, removing punctuation, and filtering out stopwords.\n",
    "    Args:\n",
    "        stopwords (List[str]): A list of stopwords to be filtered out from the text.\n",
    "        text (str): The input text to be preprocessed.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    \n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    erkennen transkribieren durchsuchen historisch...\n",
       "1    autodone service automated timecontrolled publ...\n",
       "2    collatex software 1 read multiple ≥ 2 versions...\n",
       "3    mathematiker donald e knuth entwickelte ende s...\n",
       "4                                                     \n",
       "Name: description_preprocessed, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get stopwords\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "stopwords_combined = stopwords_german.union(stopwords_english)\n",
    "\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: preprocess(stopwords_combined, x))\n",
    "edition_software_info[\"description_preprocessed\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     \n",
       "1    page find instructions use autodone based scre...\n",
       "2    “in language system language differences”– jac...\n",
       "3                                                     \n",
       "4                                                     \n",
       "Name: webpages_text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info[\"webpages_text_preprocessed\"] = edition_software_info[\"webpages_text\"].apply(lambda x: preprocess(stopwords_combined, x) if not pd.isna(x) else \"\")\n",
    "edition_software_info[\"webpages_text_preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lemmatize\n",
    "\n",
    "Finally, we can lemmatize our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef lemmatize_english_text(text):\\n    doc = nlp_en(text)\\n    return \\' \\'.join([token.lemma_ for token in doc])\\n\\ndef lemmatize_german_text(text):\\n    doc = nlp_de(text)\\n    return \\' \\'.join([token.lemma_ for token in doc])\\n\\nedition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_english_text(x))\\nedition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_german_text(x))\\nedition_software_info[\"description_preprocessed\"].head()\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def lemmatize_english_text(text):\n",
    "    doc = nlp_en(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def lemmatize_german_text(text):\n",
    "    doc = nlp_de(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_english_text(x))\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_german_text(x))\n",
    "edition_software_info[\"description_preprocessed\"].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add Language Information\n",
    "\n",
    "As the entries in our repository are in both english and german, we add information on the texts language to the dataset.\n",
    "\n",
    "To do so, we'll use a fasttext-model for language identification, which can be found [here](https://fasttext.cc/docs/en/language-identification.html).\n",
    "\n",
    "As the model was trained on UTF-8 data, it expects UTF-8 as input. This sould be the case, as pandas `read_csv`-function imports text in UTF-8 by default.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found.\n"
     ]
    }
   ],
   "source": [
    "# download the fasttext model\n",
    "\n",
    "fasttext_path = os.path.join(os.getcwd(), 'models/lid.176.bin')\n",
    "\n",
    "if os.path.isfile(fasttext_path):\n",
    "    print(\"Model found.\")\n",
    "    language_detection_model = fasttext.load_model(fasttext_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_path)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    language_detection_model = fasttext.load_model(fasttext_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model to identify the languages of our entries. This might take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>description_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte E...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EVT (Edition Visualization Technology) is a so...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Der Classical Text Editor (CTE) wird auf Initi...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In vielen Editionsprojekten wird die Datenbank...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td># Benutzerfreundliches Arbeiten  Als zentrale...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description description_lang\n",
       "0  # Erkennen, Transkribieren und Durchsuchen von...               de\n",
       "1  autodone is a service for the automated, time-...               en\n",
       "2  [CollateX](http://collatex.net/) is a software...               en\n",
       "3  Der Mathematiker Donald E. Knuth entwickelte E...               de\n",
       "4                                                                 NaN\n",
       "5                                                                 NaN\n",
       "6  EVT (Edition Visualization Technology) is a so...               en\n",
       "7  Der Classical Text Editor (CTE) wird auf Initi...               de\n",
       "8  In vielen Editionsprojekten wird die Datenbank...               de\n",
       "9   # Benutzerfreundliches Arbeiten  Als zentrale...               de"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fasttext_model = fasttext.load_model(fasttext_path)\n",
    "\n",
    "def identify_language(text):\n",
    "    lang_detected = fasttext_model.predict(text)\n",
    "    return lang_detected[0][0]\n",
    "\n",
    "# clean the webpage text, as the model expect text without newlines\n",
    "edition_software_info.loc[:,\"description\"] = edition_software_info[\"description\"].str.replace(\"\\n\",\" \")\n",
    "\n",
    "# detect message-  and webpage-languages. If the column contains empty text, the language is set to nan\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description\"].apply(lambda x: identify_language(x) if not x == '' else np.nan)\n",
    "\n",
    "# clean the output\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description_lang\"].str.replace(\"__label__\",\"\")\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description_lang\"].str.replace(\"__label__\",\"\")\n",
    "\n",
    "# print the new columns\n",
    "edition_software_info[[\"description\",\"description_lang\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 1: TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with a simple TF-IDF vectorization.\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)** is a weighting scheme that weights the cells of a term-document matrix by their potential to be discriminatory.\n",
    "\n",
    "To do so, we first calculate the **term frequency (TF)**. The term frequency represents the number of instances of a given word $t$ in a document $d$.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Count of } t \\text{ in } d}{\\text{Total number of words in } d}\n",
    "$$\n",
    "\n",
    "This term frequency is then multiplied by the **inverse document frequency (IDF)**. The IDF is calculated by counting all documents that contain a term $t$ (the document frequency $\\text{df}(t)$). Then, we divide the total number of documents $N$ in the corpus by $\\text{df}(t)$.\n",
    "\n",
    "This inverse frequency is chosen over the regular frequency to **downweight** terms that appear in many documents, since these terms are less likely to be useful for distinguishing between documents.\n",
    "\n",
    "Usually, we also take the logarithm of the IDF to smooth out the very large values that can occur when a term appears in only a few documents. This ensures that rare terms are not excessively weighted.\n",
    "\n",
    "$$\n",
    "\\text{df}(t) = \\text{Document frequency of a term } t\n",
    "$$\n",
    "$$\n",
    "N = \\text{Number of documents}\n",
    "$$\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{\\text{df}(t)}\\right)\n",
    "$$\n",
    "\n",
    "Finally, we calculate the **TF-IDF** by multiplying the term frequency $\\text{TF}(t, d)$ with the inverse document frequency $\\text{IDF}(t)$.\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "The resulting value can be interpreted as a measure of the importance of the term in a document relative to the entire corpus. Terms that are frequent in a document but rare across the corpus will have higher TF-IDF scores, indicating their importance.\n",
    "\n",
    "\n",
    "**N-grams:**\n",
    "\n",
    "To capture not just the importance of single words but also some of the **context** in which they are used, we can apply TF-IDF to **n-grams**. N-grams are contiguous sequences of $n$ words that appear together in a text. The size of the sequence, $n$, is a hyperparameter that can be adjusted depending on the specific task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Fit TF-IDF Vectorizer\n",
    "First, we fit the vectorizer on the preprocessed descriptions. \n",
    "This way, the vectorizer can transform text into numerical feature vectors based on the learned vocabulary and its distribution over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12</th>\n",
       "      <th>12 languages</th>\n",
       "      <th>12 languages comprehensive</th>\n",
       "      <th>12 languages comprehensive righttoleft</th>\n",
       "      <th>1514</th>\n",
       "      <th>1514 digital</th>\n",
       "      <th>1514 digital marburg</th>\n",
       "      <th>1514 digital marburg büchner</th>\n",
       "      <th>19042024</th>\n",
       "      <th>19042024 official</th>\n",
       "      <th>...</th>\n",
       "      <th>überführt neben klassischen</th>\n",
       "      <th>überführt neben klassischen pdfformat</th>\n",
       "      <th>überschaubaren</th>\n",
       "      <th>überschaubaren teil</th>\n",
       "      <th>überschaubaren teil funktionen</th>\n",
       "      <th>überschaubaren teil funktionen aufgrund</th>\n",
       "      <th>überwiegende</th>\n",
       "      <th>überwiegende teil</th>\n",
       "      <th>überwiegende teil funktionen</th>\n",
       "      <th>überwiegende teil funktionen editionen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    12  12 languages  12 languages comprehensive  \\\n",
       "0  0.0           0.0                         0.0   \n",
       "1  0.0           0.0                         0.0   \n",
       "2  0.0           0.0                         0.0   \n",
       "3  0.0           0.0                         0.0   \n",
       "4  0.0           0.0                         0.0   \n",
       "\n",
       "   12 languages comprehensive righttoleft  1514  1514 digital  \\\n",
       "0                                     0.0   0.0           0.0   \n",
       "1                                     0.0   0.0           0.0   \n",
       "2                                     0.0   0.0           0.0   \n",
       "3                                     0.0   0.0           0.0   \n",
       "4                                     0.0   0.0           0.0   \n",
       "\n",
       "   1514 digital marburg  1514 digital marburg büchner  19042024  \\\n",
       "0                   0.0                           0.0  0.000000   \n",
       "1                   0.0                           0.0  0.064201   \n",
       "2                   0.0                           0.0  0.000000   \n",
       "3                   0.0                           0.0  0.000000   \n",
       "4                   0.0                           0.0  0.000000   \n",
       "\n",
       "   19042024 official  ...  überführt neben klassischen  \\\n",
       "0           0.000000  ...                          0.0   \n",
       "1           0.064201  ...                          0.0   \n",
       "2           0.000000  ...                          0.0   \n",
       "3           0.000000  ...                          0.0   \n",
       "4           0.000000  ...                          0.0   \n",
       "\n",
       "   überführt neben klassischen pdfformat  überschaubaren  überschaubaren teil  \\\n",
       "0                                    0.0             0.0                  0.0   \n",
       "1                                    0.0             0.0                  0.0   \n",
       "2                                    0.0             0.0                  0.0   \n",
       "3                                    0.0             0.0                  0.0   \n",
       "4                                    0.0             0.0                  0.0   \n",
       "\n",
       "   überschaubaren teil funktionen  überschaubaren teil funktionen aufgrund  \\\n",
       "0                             0.0                                      0.0   \n",
       "1                             0.0                                      0.0   \n",
       "2                             0.0                                      0.0   \n",
       "3                             0.0                                      0.0   \n",
       "4                             0.0                                      0.0   \n",
       "\n",
       "   überwiegende  überwiegende teil  überwiegende teil funktionen  \\\n",
       "0           0.0                0.0                           0.0   \n",
       "1           0.0                0.0                           0.0   \n",
       "2           0.0                0.0                           0.0   \n",
       "3           0.0                0.0                           0.0   \n",
       "4           0.0                0.0                           0.0   \n",
       "\n",
       "   überwiegende teil funktionen editionen  \n",
       "0                                     0.0  \n",
       "1                                     0.0  \n",
       "2                                     0.0  \n",
       "3                                     0.0  \n",
       "4                                     0.0  \n",
       "\n",
       "[5 rows x 18228 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(edition_software_info[\"description_preprocessed\"])\n",
    "\n",
    "# display the resulting matrix\n",
    "tfidf_matrix_beautify = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_matrix_beautify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Inspect the tf-idf representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in this dataframe is a unique word, while each row is a document. The cells denote the number of occurances of a word in a document, weighted by the words potential to be distinctive.\n",
    "\n",
    "Let's take a look at the tf-idf filtered words for each description (You can find them in the column \"tf_idf_filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>tf_idf_filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>[(dokumenten, 0.24192478472674683), (durchsuchen, 0.12096239236337342), (erkennen, 0.12096239236337342), (erkennung, 0.12096239236337342), (transkribieren, 0.12096239236337342)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "      <td>[(twitter, 0.1926029459944442), (autodone, 0.1284019639962961), (usage, 0.11567598600740456), (service, 0.10664676093079592)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>[(differences, 0.12889590979174362), (computational, 0.10345985709520088), (similarities, 0.10345985709520088), (similarities differences, 0.10345985709520088), (tokens, 0.10345985709520088)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>[(jahre, 0.1928958299837409)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)    \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                             tf_idf_filtered_words  \n",
       "0                [(dokumenten, 0.24192478472674683), (durchsuchen, 0.12096239236337342), (erkennen, 0.12096239236337342), (erkennung, 0.12096239236337342), (transkribieren, 0.12096239236337342)]  \n",
       "1                                                                    [(twitter, 0.1926029459944442), (autodone, 0.1284019639962961), (usage, 0.11567598600740456), (service, 0.10664676093079592)]  \n",
       "2  [(differences, 0.12889590979174362), (computational, 0.10345985709520088), (similarities, 0.10345985709520088), (similarities differences, 0.10345985709520088), (tokens, 0.10345985709520088)]  \n",
       "3                                                                                                                                                                    [(jahre, 0.1928958299837409)]  \n",
       "4                                                                                                                                                                                               []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the threshold for TF-IDF scores\n",
    "threshold = 0.1\n",
    "\n",
    "# Filter words with TF-IDF scores greater than the threshold for each document\n",
    "def filter_words_by_threshold(row, threshold):\n",
    "    filtered_words = [(word, score) for word, score in zip(tfidf_matrix_beautify.columns, row) if score > threshold]\n",
    "    return sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Apply the function to each row of the TF-IDF DataFrame\n",
    "filtered_words = tfidf_matrix_beautify.apply(lambda row: filter_words_by_threshold(row, threshold), axis=1)\n",
    "\n",
    "# Create a dataframe to display the filtered words\n",
    "filtered_words_df = pd.DataFrame(filtered_words, columns=[\"tf_idf_filtered_words\"])\n",
    "tfidf_display = pd.concat([edition_software_info[\"description_clean\"], filtered_words_df], axis=1)\n",
    "    \n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(tfidf_display[[\"description_clean\", \"tf_idf_filtered_words\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test tfidf-representation\n",
    "\n",
    "This cell will return the most relevant documents from our dataset based on a comparison of their tf-idf representations and a query. The query can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>VCEditor is a tool for building and visualizing the physical structure of manuscripts using the data model provided by [VisColl: Modeling and Visualizing the Physical Construction of Codex Manuscripts](https://viscoll.org). VCEditor is a continuation of development of VisCodex, developed by the Old Books New Science lab at the University of Toronto, directed by Dr Alexandra Gillespie.  For VCEditor we have made several modification, particularly to the back end, to ensure that the output validates to the most recent version of the VisColl data model.</td>\n",
       "      <td>0.068621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EVT (Edition Visualization Technology) is a software for creating and browsing digital editions of manuscripts based on text encoded according to the [TEI XML](http://www.tei-c.org/) schemas and Guidelines. This tool was born as part of the [Digital Vercelli Book](http://vbd.humnet.unipi.it/) project in order to allow the creation of a digital edition of the Vercelli Book, a parchment codex of the late tenth century, now preserved in the Archivio e Biblioteca Capitolare of Vercelli and regarded as one of the four most important manuscripts of the Anglo-Saxon period as regards the transmission of poetic texts in the Old English language. To ensure that it will be working on all the most recent web browsers, and for as long as possible on the World Wide Web itself, EVT is built on open and standard web technologies such as HTML, CSS and JavaScript. Specific features, such as the magnifying lens, are entrusted to jQuery plug-ins, again chosen among the open source and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble or turn out to be not completely up to the task can be replaced easily.</td>\n",
       "      <td>0.047829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The premise of DM is simple: if you have a collection of digital images and texts, then you should be able to develop a project where you can identify specific moments on these images and texts, annotate them as much as you want, link them together, generate searchable content, collaborate with your friends, and publish your work online for others to see and share. DM was developed on the Heroku cloud-server platform, where installation and administration is straightforward, after which developing individual projects requires no specific IT expertise.    Features   Highlights Mark up your image and text documents with highlights you can then annotate and link together. Identify discrete moments on images and texts with highlight tools including dots, lines, circles, rectangles, other polygons, text tags, and multiple color options.   Annotations Develop your projects and publications with an unlimited number of annotations on individual highlights and entire images and texts. Highlights and entire documents can host an unlimited number of annotations, and annotations themselves can host additional annotation layers.   Links Once you've marked up your text and image documents with highlights and annotations, you can create links between individual highlights and entire documents, and your links are always bi-directional, so you and other viewers can travel back and forth between highlights.</td>\n",
       "      <td>0.032442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        description_clean  \\\n",
       "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          VCEditor is a tool for building and visualizing the physical structure of manuscripts using the data model provided by [VisColl: Modeling and Visualizing the Physical Construction of Codex Manuscripts](https://viscoll.org). VCEditor is a continuation of development of VisCodex, developed by the Old Books New Science lab at the University of Toronto, directed by Dr Alexandra Gillespie.  For VCEditor we have made several modification, particularly to the back end, to ensure that the output validates to the most recent version of the VisColl data model.    \n",
       "6                                                                                                                                                                                        EVT (Edition Visualization Technology) is a software for creating and browsing digital editions of manuscripts based on text encoded according to the [TEI XML](http://www.tei-c.org/) schemas and Guidelines. This tool was born as part of the [Digital Vercelli Book](http://vbd.humnet.unipi.it/) project in order to allow the creation of a digital edition of the Vercelli Book, a parchment codex of the late tenth century, now preserved in the Archivio e Biblioteca Capitolare of Vercelli and regarded as one of the four most important manuscripts of the Anglo-Saxon period as regards the transmission of poetic texts in the Old English language. To ensure that it will be working on all the most recent web browsers, and for as long as possible on the World Wide Web itself, EVT is built on open and standard web technologies such as HTML, CSS and JavaScript. Specific features, such as the magnifying lens, are entrusted to jQuery plug-ins, again chosen among the open source and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble or turn out to be not completely up to the task can be replaced easily.   \n",
       "25  The premise of DM is simple: if you have a collection of digital images and texts, then you should be able to develop a project where you can identify specific moments on these images and texts, annotate them as much as you want, link them together, generate searchable content, collaborate with your friends, and publish your work online for others to see and share. DM was developed on the Heroku cloud-server platform, where installation and administration is straightforward, after which developing individual projects requires no specific IT expertise.    Features   Highlights Mark up your image and text documents with highlights you can then annotate and link together. Identify discrete moments on images and texts with highlight tools including dots, lines, circles, rectangles, other polygons, text tags, and multiple color options.   Annotations Develop your projects and publications with an unlimited number of annotations on individual highlights and entire images and texts. Highlights and entire documents can host an unlimited number of annotations, and annotations themselves can host additional annotation layers.   Links Once you've marked up your text and image documents with highlights and annotations, you can create links between individual highlights and entire documents, and your links are always bi-directional, so you and other viewers can travel back and forth between highlights.    \n",
       "\n",
       "    similarity_score  \n",
       "30          0.068621  \n",
       "6           0.047829  \n",
       "25          0.032442  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I want to work with manuscripts'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# Transform the query to TF-IDF space\n",
    "query_tfidf = tfidf_vectorizer.transform([query]) \n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities[0]\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['description_clean', 'similarity_score']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 2: Aggregated Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create document representations by aggregating the word2vec embeddings of each word in a description. \n",
    "\n",
    "Word2vec encodes the meaning of the words by capturing their semantic relationships based on the context in which they appear. By aggregating the word2vec embeddings of each word in a description, we can create a document representation that retains the semantic information and provides a more nuanced understanding of the content.\n",
    "\n",
    "From a computational perspective, these representations are shorter and denser than tf-idf representations, making them more suitable for computations such as similarity measures, clustering, or classification tasks. The dense nature of word2vec embeddings allows for efficient storage and faster processing compared to sparse representations like tf-idf. Additionally, because word2vec captures the meaning and context of words, it can provide more meaningful insights into the relationships between different documents or terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "path = os.path.join(current_path, \"models/word2vec-google-news-300.bin\")\n",
    "\n",
    "# Load the model if it is already in our project. If not, download it.\n",
    "if os.path.isfile(path):\n",
    "    print(\"Model found. Loading...\")\n",
    "    word2vec_model = KeyedVectors.load(path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    word2vec_model.save(path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #TODO: Add some preprocessing\\ndef preprocess_word2vec(text: str) -> str:\\n    pass\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #TODO: Add some preprocessing\n",
    "def preprocess_word2vec(text: str) -> str:\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the document representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word2vec_vector(words, model):\n",
    "    words = words.split()\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in model]\n",
    "    \n",
    "    if not valid_words:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the vectors of the valid words to create a document representation\n",
    "    vectors = [model[word] for word in valid_words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply the function to create aggregated vectors\n",
    "word2vec = edition_software_info['description_preprocessed'].apply(lambda x: get_word2vec_vector(x, word2vec_model))\n",
    "\n",
    "# Convert the Series of 1D arrays to a 2D numpy array (to calculate the cosine similarity later on)\n",
    "word2vec_array = np.array(word2vec.tolist())\n",
    "len(word2vec_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test Word2Vec Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>TEI Critical Apparatus Toolbox</td>\n",
       "      <td>The TEI Critical Apparatus Toolbox (TEI CAT) is a simple tool offering an easy visualization for TEI XML critical editions. It especially targets the needs of people working on natively-digital editions. Its main purpose is to provide editors with an easy way of visualizing their ongoing work before it is finalized, and also to perform some automatic quality checks on their encoding.   Features The Toolbox lets you... [Check your encoding](http://teicat.huma-num.fr/check.php): offers facilities to display your edition while it is still in the making, and check the consistency of your encoding [Display parallel versions](http://teicat.huma-num.fr/witnesses.php): choose the sigla of the witnesses, and the different versions of the text, following each chosen witness, will be displayed in parallel columns. [Print an edition](http://teicat.huma-num.fr/print.php) of a TEI XML edition, with a TEI-to-LateX and PDF transformation [Annotate an image](http://teicat.huma-num.fr/image.php): lets you easily trace zones on an image to prepare a documentary edition [Get statistics](http://teicat.huma-num.fr/stats.php) on the XML tags effectively used in different parts of your edition, and some word count. A [downloadable version](http://teicat.huma-num.fr/download.php) is coming soon.</td>\n",
       "      <td>0.606167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Publex</td>\n",
       "      <td>As part of the ELEXIS “European Lexicographic Infrastructure” initiative, TCDH developed the [Publex tool](https://tcdh.uni-trier.de/en/projekt/publex), a browser-based publishing software for XML-annotated dictionaries.  With the help of the software and an intuitive user interface, users can upload their dictionary data marked in XML and define the desired formatting for the dictionary by configuring the individual components. With these settings and the attached metadata, the dictionary can finally be published online on a platform provided by ELEXIS. Publex, thus, also enables users who do not have the appropriate infrastructure or technical knowledge to make their dictionaries accessible on the Internet.   Step by step dictionary publishing with Publex A user manual guides you in detail and step by step through the application process ([Publication on DARIAH Campus](https://campus.dariah.eu/resource/posts/how-to-publish-your-dictionary-data-with-publex)). This can be roughly divided into three steps:  1. uploading the data 2. defining the representation 3. publishing the dictionary Users can import their XML dictionary data from a Git repository into Publex and submit metadata about the dictionary. The core of the tool is the definition of styling rules, which define how the individual elements of the dictionary articles should be displayed in the published online version. Upon import, Publex parses the data and captures all tags, attributes, and associated attribute values with which the resource has been tagged. For each of these elements and specific combinations, properties such as font style, font size, color, letterspacing, or text indent can now be specified. In addition, elements can be defined as search categories and added to the lemma list. Special characters can be included and displayed via the [KompLett](https://tcdh.uni-trier.de/en/projekt/komplett) font. For this purpose, the TCDH provides a file with all the [non-Unicode characters defined for the Trier dictionaries](https://bitbucket.org/tcdh/publex-allentities/src/master/). In parallel to the creation of the display rules, the user can look at the Dictionary Preview at any time to see how the defined styling rules are implemented and how the dictionary will look in the publication version. Publex provides different accesses to the dictionary: a lemma list and different search options. The items in the lemma list are sorted alphabetically and can be searched using a search box. Clicking on them displays the linked dictionary item on the screen. The general search can be used to search the dictionary contents in full text, the advanced search offers an AND-linked search over the full text and any number of information fields previously defined as search categories in the styling rules. Once published, the dictionary receives its own URI and also appears, along with its metadata, on the [\"PUBLEX Dictionaries\" page](http://publex.uni-trier.de/dictionary-network) with all previously published dictionaries. It is also possible to set up and use the entire Publex infrastructure on your own server.</td>\n",
       "      <td>0.541687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Comparo</td>\n",
       "      <td>Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text.  The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability.   Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD](https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud) database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital'](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) , further settings can be made in order to adapt the view to the respective needs or preferences of the user, for example the connecting lines between the versions or the internal variances can be hidden as required. In addition, thanks to the data stored in Comparo, the user can choose which of the documents he would like to compare with one another and in this way put together an individual setting based on versions relevant to his research interests.   Technical Details Comparo is a tool developed in the Java programming language, which is used as a Java archive (JAR) regardless of the platform. Fundamental for the application is the availability of an FuD database, which provides the text versions to be compared and is used at the same time to save the compiled comparison results.</td>\n",
       "      <td>0.540854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CATview</td>\n",
       "      <td>CATview processes data generated by collation tools such as LERA, CollateX or Juxta.  It is meant to be embedded in websites as a powerful add-on for digital editions and web-based frameworks for the editing process.   Why use CATview? CATview offers multiple, easily usable functionalities for exploratory analysis of the genesis of a manuscript, in particular:   Macro-Level View of the Text Differences CATview represents text segments (which can be lines, sentences, or paragraphs) as rectangles in a tabular manner whereat (in horizontal mode) a row corresponds to a text witness and a column to aligned segments of the different text witnesses, i.e., segments that are related to each other.   Different Levels of Detail CATview has a built-in zoom that is activated by scrolling with the mouse wheel   Clear Visualization of Textual Differences CATview illustrates the degree of dissimilarity between aligned segments by the rectangles' color. The darker the color of a segment the more different the segment is with respect to the other aligned segments.   Effective Navigation CATview links the rectangles of the overview bar to the respective text segment. The current scroll position in the text is marked by a scroll spy in the overview bar.   Highlighting Search Results CATview visualizes search hits by coloring (yellow) the segments matching the search request.   Easy Restriction of the Text under Consideration CATview offers a comfortable selection of consecutive text segments by drawing a box around them.   Statistical Analysis CATview allows statistical analysis with respect to text excerpts.</td>\n",
       "      <td>0.539170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>VMR CRE</td>\n",
       "      <td>The Virtual Manuscript Room Collaborative Research Environment (VMR CRE) brings community and a toolbox of powerful research components to support all stages of research and production of a digital edition. Beginning with the popular open-source portal, Liferay, the VMR CRE integrates 30 DH components to naturally support: Cataloging witnesses; managing and displaying images; producing well-formed TEI transcriptions using a web-based WYSIWYM editor and storing those transcriptions to a versioned transcription repository; community volunteer task assignment and project management; automatic realtime collation of witnesses; regularization and apparatus editing; online publishing of the final results-- as a traditional apparatus, or with interactive tools which let users choose different ways to visualize the data produced in the edition.    Overview A walk through the workflow at the Institut für neutestamentliche Textforschung (INTF), in their efforts to edit the Editio Critica Maior (ECM), provides opportunity to touch on many components available in the VMR CRE.  Work can be divided into 9 discrete stages, progressively: 1) witness cataloging; 2) witness selection; 3) image management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus.   Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page.  For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features.  These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF.  A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be.  Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility.  For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John.  A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present.  If a region box is captured, a search query can specify to show the region box clips in the result.  For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips.   Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham.  This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component.  The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes.  The content may then be obtained as EpiDoc influenced TEI. The VMR CRE saves content in a versioned transcription repository backed by Git.  A user may have access to create and edit their own personal transcription, a project-wide transcription, or a site-wide (= published) transcription-- each having version history. The VMR CRE also includes a palaeography tool to assist a transcriber when encountering rare symbols, abbreviations, or ligatures.  If a portion of the unknown text can be identified, the researcher can enter one or more letters and will be presented with images of text instances elsewhere which include these letters, offering possibilities.  As more and more rare text items are tagged, the system grows more helpful. Quality assurance for the ECM requires that a transcription for a manuscript be produced independently by two transcribers.  The products are then compared to each other and differences are reviewed by a manager and reconciled to produce a final transcription.  The VMR CRE provides tools to facilitate this reconciliation work. Collation and Visualization Collation is a key component to find differences in text witnesses when producing a critical edition.  Collation facilities in the VMR CRE are performed by CollateX.  Collation and regularization of uninteresting differences is an iterative cycle in digital editing and the VMR CRE ties these two actions together with an intuitive visual interface.  Visualization of a collation, either during the editing process or for the reader, can be rendered as a variant graph, an alignment table, or as a traditional negative apparatus.    Web Services, Open Programmatic Access The VMR CRE Web Services API layer is primarily useful for exposing the functionality of the VMR CRE to other research projects wishing to access the functionality or contribute to the dataset through their own systems and tools.  The VMR CRE Web Services API generally uses noun/verb nomenclature organized by category.  This means that the last 2 segments of an API URL will consist first of the type of object the call will affect, and second, of the action to be performed on the object.  Any path before the final two segments are merely for organizational purposes.  This is different from a strict REST convention which confines the action to one of 6 HTTP verbs.  The VMR CRE places no semantic meaning on the HTTP verb.  Both GET and POST HTTP verbs are accepted as identical, relegating the verb, or semantic action to the final segment of the URL.  This allows easy testing and examples for every action directly within a web browser.  Parameters to a service request are passed as standard HTTP FORM POST parameters or as query string parameters.</td>\n",
       "      <td>0.537587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        brand_name  \\\n",
       "33  TEI Critical Apparatus Toolbox   \n",
       "21                          Publex   \n",
       "23                         Comparo   \n",
       "39                         CATview   \n",
       "42                         VMR CRE   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          description_clean  \\\n",
       "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The TEI Critical Apparatus Toolbox (TEI CAT) is a simple tool offering an easy visualization for TEI XML critical editions. It especially targets the needs of people working on natively-digital editions. Its main purpose is to provide editors with an easy way of visualizing their ongoing work before it is finalized, and also to perform some automatic quality checks on their encoding.   Features The Toolbox lets you... [Check your encoding](http://teicat.huma-num.fr/check.php): offers facilities to display your edition while it is still in the making, and check the consistency of your encoding [Display parallel versions](http://teicat.huma-num.fr/witnesses.php): choose the sigla of the witnesses, and the different versions of the text, following each chosen witness, will be displayed in parallel columns. [Print an edition](http://teicat.huma-num.fr/print.php) of a TEI XML edition, with a TEI-to-LateX and PDF transformation [Annotate an image](http://teicat.huma-num.fr/image.php): lets you easily trace zones on an image to prepare a documentary edition [Get statistics](http://teicat.huma-num.fr/stats.php) on the XML tags effectively used in different parts of your edition, and some word count. A [downloadable version](http://teicat.huma-num.fr/download.php) is coming soon.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        As part of the ELEXIS “European Lexicographic Infrastructure” initiative, TCDH developed the [Publex tool](https://tcdh.uni-trier.de/en/projekt/publex), a browser-based publishing software for XML-annotated dictionaries.  With the help of the software and an intuitive user interface, users can upload their dictionary data marked in XML and define the desired formatting for the dictionary by configuring the individual components. With these settings and the attached metadata, the dictionary can finally be published online on a platform provided by ELEXIS. Publex, thus, also enables users who do not have the appropriate infrastructure or technical knowledge to make their dictionaries accessible on the Internet.   Step by step dictionary publishing with Publex A user manual guides you in detail and step by step through the application process ([Publication on DARIAH Campus](https://campus.dariah.eu/resource/posts/how-to-publish-your-dictionary-data-with-publex)). This can be roughly divided into three steps:  1. uploading the data 2. defining the representation 3. publishing the dictionary Users can import their XML dictionary data from a Git repository into Publex and submit metadata about the dictionary. The core of the tool is the definition of styling rules, which define how the individual elements of the dictionary articles should be displayed in the published online version. Upon import, Publex parses the data and captures all tags, attributes, and associated attribute values with which the resource has been tagged. For each of these elements and specific combinations, properties such as font style, font size, color, letterspacing, or text indent can now be specified. In addition, elements can be defined as search categories and added to the lemma list. Special characters can be included and displayed via the [KompLett](https://tcdh.uni-trier.de/en/projekt/komplett) font. For this purpose, the TCDH provides a file with all the [non-Unicode characters defined for the Trier dictionaries](https://bitbucket.org/tcdh/publex-allentities/src/master/). In parallel to the creation of the display rules, the user can look at the Dictionary Preview at any time to see how the defined styling rules are implemented and how the dictionary will look in the publication version. Publex provides different accesses to the dictionary: a lemma list and different search options. The items in the lemma list are sorted alphabetically and can be searched using a search box. Clicking on them displays the linked dictionary item on the screen. The general search can be used to search the dictionary contents in full text, the advanced search offers an AND-linked search over the full text and any number of information fields previously defined as search categories in the styling rules. Once published, the dictionary receives its own URI and also appears, along with its metadata, on the [\"PUBLEX Dictionaries\" page](http://publex.uni-trier.de/dictionary-network) with all previously published dictionaries. It is also possible to set up and use the entire Publex infrastructure on your own server.    \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text.  The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability.   Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD](https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud) database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital'](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) , further settings can be made in order to adapt the view to the respective needs or preferences of the user, for example the connecting lines between the versions or the internal variances can be hidden as required. In addition, thanks to the data stored in Comparo, the user can choose which of the documents he would like to compare with one another and in this way put together an individual setting based on versions relevant to his research interests.   Technical Details Comparo is a tool developed in the Java programming language, which is used as a Java archive (JAR) regardless of the platform. Fundamental for the application is the availability of an FuD database, which provides the text versions to be compared and is used at the same time to save the compiled comparison results.    \n",
       "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        CATview processes data generated by collation tools such as LERA, CollateX or Juxta.  It is meant to be embedded in websites as a powerful add-on for digital editions and web-based frameworks for the editing process.   Why use CATview? CATview offers multiple, easily usable functionalities for exploratory analysis of the genesis of a manuscript, in particular:   Macro-Level View of the Text Differences CATview represents text segments (which can be lines, sentences, or paragraphs) as rectangles in a tabular manner whereat (in horizontal mode) a row corresponds to a text witness and a column to aligned segments of the different text witnesses, i.e., segments that are related to each other.   Different Levels of Detail CATview has a built-in zoom that is activated by scrolling with the mouse wheel   Clear Visualization of Textual Differences CATview illustrates the degree of dissimilarity between aligned segments by the rectangles' color. The darker the color of a segment the more different the segment is with respect to the other aligned segments.   Effective Navigation CATview links the rectangles of the overview bar to the respective text segment. The current scroll position in the text is marked by a scroll spy in the overview bar.   Highlighting Search Results CATview visualizes search hits by coloring (yellow) the segments matching the search request.   Easy Restriction of the Text under Consideration CATview offers a comfortable selection of consecutive text segments by drawing a box around them.   Statistical Analysis CATview allows statistical analysis with respect to text excerpts.    \n",
       "42  The Virtual Manuscript Room Collaborative Research Environment (VMR CRE) brings community and a toolbox of powerful research components to support all stages of research and production of a digital edition. Beginning with the popular open-source portal, Liferay, the VMR CRE integrates 30 DH components to naturally support: Cataloging witnesses; managing and displaying images; producing well-formed TEI transcriptions using a web-based WYSIWYM editor and storing those transcriptions to a versioned transcription repository; community volunteer task assignment and project management; automatic realtime collation of witnesses; regularization and apparatus editing; online publishing of the final results-- as a traditional apparatus, or with interactive tools which let users choose different ways to visualize the data produced in the edition.    Overview A walk through the workflow at the Institut für neutestamentliche Textforschung (INTF), in their efforts to edit the Editio Critica Maior (ECM), provides opportunity to touch on many components available in the VMR CRE.  Work can be divided into 9 discrete stages, progressively: 1) witness cataloging; 2) witness selection; 3) image management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus.   Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page.  For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features.  These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF.  A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be.  Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility.  For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John.  A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present.  If a region box is captured, a search query can specify to show the region box clips in the result.  For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips.   Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham.  This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component.  The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes.  The content may then be obtained as EpiDoc influenced TEI. The VMR CRE saves content in a versioned transcription repository backed by Git.  A user may have access to create and edit their own personal transcription, a project-wide transcription, or a site-wide (= published) transcription-- each having version history. The VMR CRE also includes a palaeography tool to assist a transcriber when encountering rare symbols, abbreviations, or ligatures.  If a portion of the unknown text can be identified, the researcher can enter one or more letters and will be presented with images of text instances elsewhere which include these letters, offering possibilities.  As more and more rare text items are tagged, the system grows more helpful. Quality assurance for the ECM requires that a transcription for a manuscript be produced independently by two transcribers.  The products are then compared to each other and differences are reviewed by a manager and reconciled to produce a final transcription.  The VMR CRE provides tools to facilitate this reconciliation work. Collation and Visualization Collation is a key component to find differences in text witnesses when producing a critical edition.  Collation facilities in the VMR CRE are performed by CollateX.  Collation and regularization of uninteresting differences is an iterative cycle in digital editing and the VMR CRE ties these two actions together with an intuitive visual interface.  Visualization of a collation, either during the editing process or for the reader, can be rendered as a variant graph, an alignment table, or as a traditional negative apparatus.    Web Services, Open Programmatic Access The VMR CRE Web Services API layer is primarily useful for exposing the functionality of the VMR CRE to other research projects wishing to access the functionality or contribute to the dataset through their own systems and tools.  The VMR CRE Web Services API generally uses noun/verb nomenclature organized by category.  This means that the last 2 segments of an API URL will consist first of the type of object the call will affect, and second, of the action to be performed on the object.  Any path before the final two segments are merely for organizational purposes.  This is different from a strict REST convention which confines the action to one of 6 HTTP verbs.  The VMR CRE places no semantic meaning on the HTTP verb.  Both GET and POST HTTP verbs are accepted as identical, relegating the verb, or semantic action to the final segment of the URL.  This allows easy testing and examples for every action directly within a web browser.  Parameters to a service request are passed as standard HTTP FORM POST parameters or as query string parameters.     \n",
       "\n",
       "    similarity_score  \n",
       "33          0.606167  \n",
       "21          0.541687  \n",
       "23          0.540854  \n",
       "39          0.539170  \n",
       "42          0.537587  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I need a nice presentation search for my letter edition'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# get vector representation of the query using word2vec\n",
    "query_word2vec = get_word2vec_vector(query, word2vec_model)\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_word2vec = query_word2vec.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_word2vec, word2vec_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"brand_name\",\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 3: Aggregated FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One downside of pretrained Word2Vec representations is their inability to handle words not contained in their vocabulary. \n",
    "\n",
    "FastText overcomes this issue by representing words not only as embeddings but also as collections of embedded character n-grams. This approach allows FastText to generate meaningful word vectors for previously unseen words, which is particularly useful when dealing with highly specialized terminologies. In our dataset, which contains such specialized language, FastText may therefore offer more reliable performance compared to traditional Word2Vec models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the models\n",
    "\n",
    "FastText provides pre-aligned word vectors, meaning that word vectors for different languages (like German and English) have already been mapped into a common vector space. This allows words with similar meanings across different languages to have similar vector representations, which is crucial when working with multilingual datasets.\n",
    "\n",
    "Since our dataset contains both German and English texts, we need to download the pre-aligned FastText models for these two languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_file_path: str, extract_to: str) -> None:\n",
    "    \"\"\"Unzip a file to a target directory.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Unzipped {zip_file_path} to {extract_to}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error while unzipping the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found.\n"
     ]
    }
   ],
   "source": [
    "# Check if the english model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future. \n",
    "# This might take a while\n",
    "\n",
    "current_path = os.getcwd()\n",
    "models_dir = os.path.join(current_path, \"models\")\n",
    "fasttext_eng_zip_path = os.path.join(models_dir, \"wiki.en.zip\")\n",
    "fasttext_eng_path_vec = os.path.join(models_dir, \"wiki.en.vec\")\n",
    "fasttext_eng_path_bin = os.path.join(models_dir, \"wiki.en.bin\")\n",
    "\n",
    "if os.path.isfile(fasttext_eng_path_bin):\n",
    "    print(\"Model found.\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model(fasttext_eng_path_bin) #load the full model, including subword information.\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\" #IDEA: Download the bin model, as it contains subword info. Use this subwordinfo to handle all unknown words.\n",
    "    # download the models\n",
    "    download_file(url, fasttext_eng_zip_path)\n",
    "    \n",
    "    print(\"Unzipping the file...\")\n",
    "    unzip_file(fasttext_eng_zip_path, models_dir)    \n",
    "\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model.load(fasttext_eng_path_bin)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_eng.save(fasttext_eng_path_bin)\n",
    "\n",
    "    \n",
    "if aligned_vectors_eng is None:\n",
    "    raise ValueError(\"The FastText model not loaded properly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n"
     ]
    }
   ],
   "source": [
    "# Check if the german model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future.\n",
    "# This might take a while\n",
    "\n",
    "fasttext_de_path_bin = os.path.join(current_path, \"models/wiki_de_align.bin\")\n",
    "fasttext_de_path_vec = os.path.join(current_path, \"models/wiki_de_align.vec\")\n",
    "\n",
    "if os.path.isfile(fasttext_de_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_de = KeyedVectors.load(fasttext_de_path_bin)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_de_path_vec)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_de = load_word_vectors(fasttext_de_path_vec)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_de.save(fasttext_de_path_bin)\n",
    "    \n",
    "if aligned_vectors_de is None:\n",
    "    raise ValueError(\"The FastText model or vectors were not loaded properly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we loaded both models, let's check if they are properly aligned. \n",
    "\n",
    "To do so, we'll pick an english word, get its english vector representation and return the most similar word vector in the german vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German word vector closest to skyscraper: [('hochhaus', 0.5159210562705994), ('wolkenkratzer', 0.4891827404499054), ('bürohochhaus', 0.4611120820045471), ('hochhausturm', 0.43771031498908997), ('wolkenkratzern', 0.43421125411987305), ('hochhäuser', 0.4312896728515625), ('bürohochhäuser', 0.4262181520462036), ('wolkenkratzers', 0.4224645495414734), ('hochhausbau', 0.4211919605731964), ('„wolkenkratzer', 0.42107880115509033)]\n"
     ]
    }
   ],
   "source": [
    "# get the vector-representation of a random english word\n",
    "word_english = 'skyscraper'\n",
    "word_vector_in_english = aligned_vectors_eng.wv[word_english]\n",
    "\n",
    "print(f\"German word vector closest to {word_english}:\", aligned_vectors_de.most_similar(positive=[word_vector_in_english]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Document Representations\n",
    "\n",
    "Now that we confirmed, that the vector spaces are properly aligned, we can create the document representations. \n",
    "As the pre-aligned german model does not contain subword-infomation, we'll use the subword-information contained in the english model to embedd unknown words in both languages.\n",
    "\n",
    "We'll use the preprocessed descriptions we created earlier.\n",
    "\n",
    "Additionaly, we'll print the words we created new wod vectors using the english subword information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Vector based on Subword Information for: mitttels\n",
      "Created Vector based on Subword Information for: texterkennungsmodellen\n",
      "Created Vector based on Subword Information for: kigestützte\n",
      "Created Vector based on Subword Information for: layoutanalyse\n",
      "Created Vector based on Subword Information for: strukturerkennung\n",
      "Created Vector based on Subword Information for: transkriptionseditor\n",
      "Created Vector based on Subword Information for: kigestützten\n",
      "Created Vector based on Subword Information for: kimodelle\n",
      "Created Vector based on Subword Information for: readsearch\n",
      "Created Vector based on Subword Information for: transkribusinhalte\n",
      "Created Vector based on Subword Information for: erkennungsmodelle\n",
      "Created Vector based on Subword Information for: gdpr\n",
      "Created Vector based on Subword Information for: texsystem\n",
      "Created Vector based on Subword Information for: 2e\n",
      "Created Vector based on Subword Information for: 1997\n",
      "Created Vector based on Subword Information for: opensourcelizenz\n",
      "Created Vector based on Subword Information for: eupl12\n",
      "Created Vector based on Subword Information for: pdfformat\n",
      "Created Vector based on Subword Information for: xmlpublikation\n",
      "Created Vector based on Subword Information for: id0features\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: teikonformen\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: teielementen\n",
      "Created Vector based on Subword Information for: xsltanweisungen\n",
      "Created Vector based on Subword Information for: teidaten\n",
      "Created Vector based on Subword Information for: publishing“ansatz\n",
      "Created Vector based on Subword Information for: pdfformat\n",
      "Created Vector based on Subword Information for: xslfo\n",
      "Created Vector based on Subword Information for: 8\n",
      "Created Vector based on Subword Information for: ausgabemodulen\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: codeansicht\n",
      "Created Vector based on Subword Information for: wordartigen\n",
      "Created Vector based on Subword Information for: »autorenansicht«\n",
      "Created Vector based on Subword Information for: sachanmerkungen\n",
      "Created Vector based on Subword Information for: teiauszeichnung\n",
      "Created Vector based on Subword Information for: teikonformen\n",
      "Created Vector based on Subword Information for: xmldatenbank\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: xmldokumente\n",
      "Created Vector based on Subword Information for: ediarumdb\n",
      "Created Vector based on Subword Information for: eingabewerkzeug\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: websiteentwicklung\n",
      "Created Vector based on Subword Information for: ediarumweb\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: insttitutionen\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: neugermanistische\n",
      "Created Vector based on Subword Information for: ediarum\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_fasttext_vector(row, aligned_vectors_de=None, aligned_vectors_eng=None):\n",
    "    \"\"\"\n",
    "    Calculates the FastText vector representation for a given row.\n",
    "    Parameters:\n",
    "    - row: A row of data.\n",
    "    - aligned_vectors_de: Aligned FastText vectors for the German language. Default is None.\n",
    "    - aligned_vectors_eng: Aligned FastText vectors for the English language. Default is None.\n",
    "    Note:\n",
    "    - If the language is not specified or not supported (only \"en\" and \"de\" are supported), it returns a zero vector.\n",
    "    - If a word in the row's description is not found in the aligned vectors, it tries to create a vector based on english subword information.\n",
    "    - If no vectors are found, it returns a zero vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # default size to avoid errors if vectors are None\n",
    "    vector_size = aligned_vectors_de.vector_size if aligned_vectors_de else 300\n",
    "    \n",
    "    # check if language is valid\n",
    "    lang = row.get(\"description_lang\")\n",
    "    if pd.isna(lang) or lang not in [\"en\", \"de\"]:\n",
    "        return np.zeros(vector_size) #Maybe rather use none?\n",
    "    \n",
    "    words = row.get(\"description_preprocessed\", \"\").split()\n",
    "    vectors = []\n",
    "\n",
    "    # process based on language\n",
    "    if lang == \"de\" and aligned_vectors_de:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_de[word])\n",
    "            except KeyError:\n",
    "                print(f\"Created Vector based on Subword Information for: {word}\")                \n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "                #vectors.append(np.zeros(vector_size))\n",
    "                \n",
    "    elif lang == \"en\" and aligned_vectors_eng:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "            except KeyError:\n",
    "                print(f\"Missing Vector for: {word}\")\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros\n",
    "\n",
    "\n",
    "# Apply the function to create aggregated vectors\n",
    "fasttext = edition_software_info.apply(lambda x: get_fasttext_vector(x, aligned_vectors_de, aligned_vectors_eng), axis=1)\n",
    "\n",
    "# Convert the Series of 1D arrays to a 2D numpy array (to calculate the cosine similarity later on)\n",
    "fasttext_array = np.array(word2vec.tolist())\n",
    "len(fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test the FastText Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LaTeX</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>0.099804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>0.090453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TEI Publisher</td>\n",
       "      <td>In vielen Editionsprojekten wird die Datenbanklösung [eXist-db]() eingesetzt. Die darin enthaltenen TEI-konformen Daten sollen i.d.R. zu einem bestimmten Zeitpunkt oder auch fortlaufend aufbereitet und in gedruckter oder digitaler Form ausgegeben werden. Für diese Transformation bietet der TEI Publisher einen komfortable und gleichzeitig mächtigen Weg. Der kann direkt über den eXist-db Marktplatz installiert werden. Um die gewünschte Ausgabe zu erhalten, wird durch Einstellungen mittels grafischer Benutzeroberfläche allen TEI-Elementen das gewünschte Aussehen zugeordnet. Es muss also nicht auf XSLT-Anweisungen zurückgegriffen werden. Die TEI-Daten können gemäß „Single Source Publishing“-Ansatz in verschiedene Ausgabeformate überführt werden. Neben dem klassischen PDF-Format bzw. ePUB und HTML können auch wichtige Zwischenstufen wie XSL-FO oder [LaTeX]() erzeugt werden. Seit Version 8 beherrscht der TEI Publisher außerdem die Ausgabe nach Print CSS.  Die so erstellen Dateien sind auf verschiedenen Geräten und Plattformen nutzbar und bringen für verschiedene Endgeräte jeweils passgenaue Funktionen mit.  Für die Entwicklung von Ausgabemodulen in Projekten, die auf eXist-db aufbauen, erlaubt der TEI Publisher eine schlanke und effiziente Umsetzung, die automatisch von den zahlreichen Verbesserungen aus der Community profitiert.</td>\n",
       "      <td>0.064059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ediarum</td>\n",
       "      <td>Benutzerfreundliches Arbeiten Als zentrale Softwarekomponente der Arbeitsumgebung wird Oxygen XML Author eingesetzt. Die Bearbeiter arbeiten im Oxygen XML Author nicht in einer Codeansicht, sondern in einer benutzerfreundlichen, Word-artigen »Autorenansicht«, die über Cascading Stylesheets (CSS) gestaltet wird. Dem Bearbeiter stehen dabei mehrere Ansichten zur Auswahl, so dass per Mausklick die für den Arbeitsschritt geeigneteste ausgewählt werden kann. Außerdem kann der Endanwender über eine eigene Werkzeugleiste per Knopfdruck Auszeichnungen vornehmen. So können z.B. in Manuskripten Streichungen markiert oder Sachanmerkungen eingegeben werden. Auch Personen- oder Ortsnamen können mit der entsprechenden TEI-Auszeichnung versehen und gleichzeitig über eine komfortable Auswahlliste mit dem jeweiligen Eintrag im zentralen Personen- bzw. Ortsregister verknüpft werden. Der gesamte Text kann dadurch einfach und schnell mit TEI-konformen XML ausgezeichnet werden.   Kollaboratives Arbeiten Die digitale Arbeitsumgebung nutzt die freie XML-Datenbank existdb als zentrales Repositorium für die XML-Dokumente. Die Datenbank ist auf einem Server installiert und online zugänglich. Dadurch können alle Projektmitarbeiter auf ein und denselben Datenbestand zugreifen und zusammenarbeiten. Um die Einrichtung und Konfiguration zu vereinfachen, wurde das Modul ediarum.DB entwickelt.    Website Neben dem eigentlichen Eingabewerkzeug in Oxygen XML Author, wird für die Forschungsvorhaben auch jeweils eine Website auf Basis von eXist, XQuery und XSLT erstellt. In ihr kann von den Wissenschaftlern der aktuelle Datenbestand leicht durchblättert bzw. durchsucht werden. Die Website kann je nach Bedarf nur den Bearbeitern zugänglich oder der gesamten Öffentlichkeit zugänglich gemacht werden. Beispiele für mit ediarum erstellte digitale Editionen finden Sie in der Rubrik Referenzen. Um den Prozess der Website-Entwicklung zu vereinfachen ist derzeit das Modul ediarum.WEB in Entwicklung.   Anpassung an projektspezifische Anforderungen Ziel von ediarum ist es, einen Kern an Funktionen bereitzustellen, der über verschiedene Projekte (und auch Insttitutionen hinweg) entwickelt und eingesetzt werden kann. Dadurch wird der Aufwand für alle Editionsvorhaben, die ediarum einsetzen, verringert. Der ganz überwiegende Teil der Funktionen kann auch für Editionen gleichen Typs (z.B. neugermanistische Editionen) tatsächlich projektübergreifend entwickelt werden. Allerdings gibt es i.d.R. immer einen wenn auch sehr überschaubaren Teil an Funktionen, die aufgrund des spezifischen Editions- oder Forschungskonzept, ergänzt werden müssen. Das ist in ediarum ohne Weiteres möglich. Dadurch kann zwischen notwendiger Standardisierung und ebenso notwendiger Orientierung an Forschungsfragen eine Brücke geschlagen werden.</td>\n",
       "      <td>0.056260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Classical Text Editor (CTE)</td>\n",
       "      <td>Der Classical Text Editor (CTE) wird auf Initiative der [Österreichischen Akademie der Wissenschaften](https://www.oeaw.ac.at/oesterreichische-akademie-der-wissenschaften) und des Editionsprojekts [Corpus Scriptorum Ecclesiasticorum Latinorum (CSEL)](http://csel.at/) seit 1997 entwickelt. Für die Nutzung des CTE muss eine [Lizenz erworben](https://sites.fastspring.com/stefanhagel/product/cte) werden, da das Projekt keine öffentliche Förderung erhält. Perspektivisch wird eine Veröffentlichung gemäß der Opensource-Lizenz [EUPL-1.2](https://opensource.org/licenses/EUPL-1.2) angestrebt. Der CTE schließt für den Bereich der kritischen Ausgaben die Lücke zwischen klassischen Text verarbeitungs programmen (Open Office Writer, Microsoft Word u.a.) auf der einen und Text satz programmen (TeX, TUSTEP Satz) auf der anderen Seite. Autor:innen können mit Hilfe einer graphischen Oberfläche Publikationen erstellen und gleichzeitig die gerade für kritische Ausgaben nötigen typographischen Anforderungen umsetzen. Neben umfangreichen Funktionen im Bereich kritischer Editionen (verschiedene Layouts, beliebig viele Apparate, Varianten etc.) gehören daher auch die Unterstützung von Unicode, komplexen Skripten sowie die Einbindung verschiedener Referenzsysteme zum Funktionsumfang des CTE. Das Ergebnis kann sowohl für die Drucklegung im PDF-Format als auch für eine elektronische Fassung als HTML- oder XML-Publikation exportiert werden. Umgekehrt können Texte in verschiedenen Formaten importiert werden.     Eine detaillierte Auflistung aller Feature des CTE findet man unter https://cte.oeaw.ac.at/?id0=features.</td>\n",
       "      <td>0.053857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    brand_name  \\\n",
       "3                        LaTeX   \n",
       "0                  Transkribus   \n",
       "8                TEI Publisher   \n",
       "9                      ediarum   \n",
       "7  Classical Text Editor (CTE)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   description_clean  \\\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In vielen Editionsprojekten wird die Datenbanklösung [eXist-db]() eingesetzt. Die darin enthaltenen TEI-konformen Daten sollen i.d.R. zu einem bestimmten Zeitpunkt oder auch fortlaufend aufbereitet und in gedruckter oder digitaler Form ausgegeben werden. Für diese Transformation bietet der TEI Publisher einen komfortable und gleichzeitig mächtigen Weg. Der kann direkt über den eXist-db Marktplatz installiert werden. Um die gewünschte Ausgabe zu erhalten, wird durch Einstellungen mittels grafischer Benutzeroberfläche allen TEI-Elementen das gewünschte Aussehen zugeordnet. Es muss also nicht auf XSLT-Anweisungen zurückgegriffen werden. Die TEI-Daten können gemäß „Single Source Publishing“-Ansatz in verschiedene Ausgabeformate überführt werden. Neben dem klassischen PDF-Format bzw. ePUB und HTML können auch wichtige Zwischenstufen wie XSL-FO oder [LaTeX]() erzeugt werden. Seit Version 8 beherrscht der TEI Publisher außerdem die Ausgabe nach Print CSS.  Die so erstellen Dateien sind auf verschiedenen Geräten und Plattformen nutzbar und bringen für verschiedene Endgeräte jeweils passgenaue Funktionen mit.  Für die Entwicklung von Ausgabemodulen in Projekten, die auf eXist-db aufbauen, erlaubt der TEI Publisher eine schlanke und effiziente Umsetzung, die automatisch von den zahlreichen Verbesserungen aus der Community profitiert.   \n",
       "9     Benutzerfreundliches Arbeiten Als zentrale Softwarekomponente der Arbeitsumgebung wird Oxygen XML Author eingesetzt. Die Bearbeiter arbeiten im Oxygen XML Author nicht in einer Codeansicht, sondern in einer benutzerfreundlichen, Word-artigen »Autorenansicht«, die über Cascading Stylesheets (CSS) gestaltet wird. Dem Bearbeiter stehen dabei mehrere Ansichten zur Auswahl, so dass per Mausklick die für den Arbeitsschritt geeigneteste ausgewählt werden kann. Außerdem kann der Endanwender über eine eigene Werkzeugleiste per Knopfdruck Auszeichnungen vornehmen. So können z.B. in Manuskripten Streichungen markiert oder Sachanmerkungen eingegeben werden. Auch Personen- oder Ortsnamen können mit der entsprechenden TEI-Auszeichnung versehen und gleichzeitig über eine komfortable Auswahlliste mit dem jeweiligen Eintrag im zentralen Personen- bzw. Ortsregister verknüpft werden. Der gesamte Text kann dadurch einfach und schnell mit TEI-konformen XML ausgezeichnet werden.   Kollaboratives Arbeiten Die digitale Arbeitsumgebung nutzt die freie XML-Datenbank existdb als zentrales Repositorium für die XML-Dokumente. Die Datenbank ist auf einem Server installiert und online zugänglich. Dadurch können alle Projektmitarbeiter auf ein und denselben Datenbestand zugreifen und zusammenarbeiten. Um die Einrichtung und Konfiguration zu vereinfachen, wurde das Modul ediarum.DB entwickelt.    Website Neben dem eigentlichen Eingabewerkzeug in Oxygen XML Author, wird für die Forschungsvorhaben auch jeweils eine Website auf Basis von eXist, XQuery und XSLT erstellt. In ihr kann von den Wissenschaftlern der aktuelle Datenbestand leicht durchblättert bzw. durchsucht werden. Die Website kann je nach Bedarf nur den Bearbeitern zugänglich oder der gesamten Öffentlichkeit zugänglich gemacht werden. Beispiele für mit ediarum erstellte digitale Editionen finden Sie in der Rubrik Referenzen. Um den Prozess der Website-Entwicklung zu vereinfachen ist derzeit das Modul ediarum.WEB in Entwicklung.   Anpassung an projektspezifische Anforderungen Ziel von ediarum ist es, einen Kern an Funktionen bereitzustellen, der über verschiedene Projekte (und auch Insttitutionen hinweg) entwickelt und eingesetzt werden kann. Dadurch wird der Aufwand für alle Editionsvorhaben, die ediarum einsetzen, verringert. Der ganz überwiegende Teil der Funktionen kann auch für Editionen gleichen Typs (z.B. neugermanistische Editionen) tatsächlich projektübergreifend entwickelt werden. Allerdings gibt es i.d.R. immer einen wenn auch sehr überschaubaren Teil an Funktionen, die aufgrund des spezifischen Editions- oder Forschungskonzept, ergänzt werden müssen. Das ist in ediarum ohne Weiteres möglich. Dadurch kann zwischen notwendiger Standardisierung und ebenso notwendiger Orientierung an Forschungsfragen eine Brücke geschlagen werden.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Der Classical Text Editor (CTE) wird auf Initiative der [Österreichischen Akademie der Wissenschaften](https://www.oeaw.ac.at/oesterreichische-akademie-der-wissenschaften) und des Editionsprojekts [Corpus Scriptorum Ecclesiasticorum Latinorum (CSEL)](http://csel.at/) seit 1997 entwickelt. Für die Nutzung des CTE muss eine [Lizenz erworben](https://sites.fastspring.com/stefanhagel/product/cte) werden, da das Projekt keine öffentliche Förderung erhält. Perspektivisch wird eine Veröffentlichung gemäß der Opensource-Lizenz [EUPL-1.2](https://opensource.org/licenses/EUPL-1.2) angestrebt. Der CTE schließt für den Bereich der kritischen Ausgaben die Lücke zwischen klassischen Text verarbeitungs programmen (Open Office Writer, Microsoft Word u.a.) auf der einen und Text satz programmen (TeX, TUSTEP Satz) auf der anderen Seite. Autor:innen können mit Hilfe einer graphischen Oberfläche Publikationen erstellen und gleichzeitig die gerade für kritische Ausgaben nötigen typographischen Anforderungen umsetzen. Neben umfangreichen Funktionen im Bereich kritischer Editionen (verschiedene Layouts, beliebig viele Apparate, Varianten etc.) gehören daher auch die Unterstützung von Unicode, komplexen Skripten sowie die Einbindung verschiedener Referenzsysteme zum Funktionsumfang des CTE. Das Ergebnis kann sowohl für die Drucklegung im PDF-Format als auch für eine elektronische Fassung als HTML- oder XML-Publikation exportiert werden. Umgekehrt können Texte in verschiedenen Formaten importiert werden.     Eine detaillierte Auflistung aller Feature des CTE findet man unter https://cte.oeaw.ac.at/?id0=features.   \n",
       "\n",
       "   similarity_score  \n",
       "3          0.099804  \n",
       "0          0.090453  \n",
       "8          0.064059  \n",
       "9          0.056260  \n",
       "7          0.053857  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I need to work with images in a browser'\n",
    "\n",
    "# check the queries language, to decide which model to use.\n",
    "query_lang = identify_language(query).replace(\"__label__\",\"\") if not query == '' else np.nan\n",
    "\n",
    "# preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "\n",
    "# create a series that can be used as an input in the \n",
    "query_information = pd.Series({\n",
    "    \"description_lang\": query_lang,\n",
    "    \"description_preprocessed\": query\n",
    "})\n",
    "\n",
    "# get vector representation of the query using fasttext\n",
    "query_fasttext= get_fasttext_vector(query_information, aligned_vectors_de, aligned_vectors_eng)\n",
    "\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_fasttext = query_fasttext.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_fasttext, fasttext_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"brand_name\",\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 4: SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SBERT**: \n",
    "\n",
    "In contrast to the output of regular embedding models, word vectors created using BERT models are contextualized. This means that they can generate multiple word embeddings for the same word, depending on the meaning it takes on in a certain context. \"Bank,\" for example, will have two different representations—one as a river's shore and another as a place to store money.\n",
    "\n",
    "Since BERT models by themselves are notoriously bad for semantic similarity tasks when applied to sentence- or paragraph-level vectors, we will use bi-encoders (also called SBERT) to create document representations instead. These models also create contextualized embeddings but are specifically trained with semantic similarity in mind: A triplet loss function is used to minimize the distance between an \"anchor point\" and a positive sample while maximizing the distance to a negative sample. This forces sentence transformers to produce a vector space where semantically similar sentences are close together, while sentence embeddings of semantically dissimilar sentences are far apart.\n",
    "\n",
    "Specifically, we will use the sentence-transformers library, which builds on the original [SBERT paper](https://arxiv.org/abs/1908.10084).\n",
    "\n",
    "**Reranking:**\n",
    "\n",
    "As it is suggested in the documentation of the sentence transformer module, the similarities calculated using the SBERT-representations will be reranked using a Cross-Encoder. \n",
    "Cross-Encoders tackle the task of calculating similarity as a classification task, classifying two sentences as either \"relevant\" or \"not relevant\" in relation to one another. We'll not rerank the all similarity scores, but only the entries most similar to our query.\n",
    "\n",
    "Detailed information on this retrieve & re-rank process and its benefits can be found [here](https://www.sbert.net/examples/applications/retrieve_rerank/README.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewinzen/opt/anaconda3/envs/retrieval_augmented_generation/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "sbert_model = sentence_transformers.SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "cross_encoder_model = sentence_transformers.CrossEncoder(\"corrius/cross-encoder-mmarco-mMiniLMv2-L12-H384-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Document Representations\n",
    "\n",
    "As creating SBERT embeddings may take some time, we'll first create a function to parallelize the process. The parameter num_chunks can be increased to control the number of concurrent embedding processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to control tokenizers parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "def get_sbert_embeddings(text, model):\n",
    "    \"\"\"\n",
    "    Get Sentence-BERT embeddings for a given text using a specified model.\n",
    "    Parameters:\n",
    "    text (str): The input text to encode.\n",
    "    model: The Sentence-BERT model to use for encoding.\n",
    "    Returns:\n",
    "    numpy.ndarray: The Sentence-BERT embeddings for the input text.\n",
    "    \"\"\"\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "    \n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return default_embedding    \n",
    "    return model.encode(text, convert_to_tensor=False)\n",
    "\n",
    "\n",
    "def compute_sbert_embeddings_in_parallel(df, text_column, model, num_chunks=3):\n",
    "    \"\"\"\n",
    "    Compute SBERT embeddings in parallel for the given text column in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the text data.\n",
    "        text_column (str): The column name of the text data.\n",
    "        model (Any): SBERT model for generating embeddings.\n",
    "        num_chunks (int): The number of chunks to split the data for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the computed SBERT embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to process each chunk of data\n",
    "    def process_chunk(chunk):\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in chunk:\n",
    "            embeddings.append(get_sbert_embeddings(text, model))\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    # Split DataFrame into chunks for parallel processing\n",
    "    df_chunks = np.array_split(df[text_column], num_chunks)\n",
    "\n",
    "    # Process each chunk in parallel\n",
    "    results = Parallel(n_jobs=num_chunks)(\n",
    "        delayed(process_chunk)(chunk) for chunk in df_chunks\n",
    "    )\n",
    "\n",
    "    # Combine and return the results\n",
    "    return np.concatenate(results).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewinzen/opt/anaconda3/envs/retrieval_augmented_generation/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# get vector representation of the query using the sbert model\n",
    "sbert_array = compute_sbert_embeddings_in_parallel(edition_software_info, \"description\", sbert_model, num_chunks=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test SBERT-Embeddings\n",
    "\n",
    "In this cell, we can test similarities between a query and the SBERT-Embeddings, without reranking of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Comparo</td>\n",
       "      <td>Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text.  The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability.   Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD](https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud) database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital'](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) , further settings can be made in order to adapt the view to the respective needs or preferences of the user, for example the connecting lines between the versions or the internal variances can be hidden as required. In addition, thanks to the data stored in Comparo, the user can choose which of the documents he would like to compare with one another and in this way put together an individual setting based on versions relevant to his research interests.   Technical Details Comparo is a tool developed in the Java programming language, which is used as a Java archive (JAR) regardless of the platform. Fundamental for the application is the availability of an FuD database, which provides the text versions to be compared and is used at the same time to save the compiled comparison results.</td>\n",
       "      <td>0.539826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>0.529084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>0.493854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ChrysoCollate</td>\n",
       "      <td>The program offers: two modes: collation mode and edition mode; a collation table with automatic distinctive colours and previsional semi-automatic completion of readings; annotation tools for the collation table, including a system of references to the images of the witnesses that allows you to navigate easily in your textual tradition; a viewer that displays witness pictures while one collates or edits (various formats of images, pdf, or websites); semi-automatic apparatus, according to the readings that are chosen by the editor; a stemma codicum checker; a translation box to manage and synchronise your translation; exportation in various formats (odt, cte, etc.).</td>\n",
       "      <td>0.453502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Transcribo</td>\n",
       "      <td>[Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)</td>\n",
       "      <td>0.396312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       brand_name  \\\n",
       "23        Comparo   \n",
       "0     Transkribus   \n",
       "2        CollateX   \n",
       "34  ChrysoCollate   \n",
       "22     Transcribo   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             description_clean  \\\n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text.  The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability.   Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD](https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud) database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital'](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) , further settings can be made in order to adapt the view to the respective needs or preferences of the user, for example the connecting lines between the versions or the internal variances can be hidden as required. In addition, thanks to the data stored in Comparo, the user can choose which of the documents he would like to compare with one another and in this way put together an individual setting based on versions relevant to his research interests.   Technical Details Comparo is a tool developed in the Java programming language, which is used as a Java archive (JAR) regardless of the platform. Fundamental for the application is the availability of an FuD database, which provides the text versions to be compared and is used at the same time to save the compiled comparison results.    \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The program offers: two modes: collation mode and edition mode; a collation table with automatic distinctive colours and previsional semi-automatic completion of readings; annotation tools for the collation table, including a system of references to the images of the witnesses that allows you to navigate easily in your textual tradition; a viewer that displays witness pictures while one collates or edits (various formats of images, pdf, or websites); semi-automatic apparatus, according to the readings that are chosen by the editor; a stemma codicum checker; a translation box to manage and synchronise your translation; exportation in various formats (odt, cte, etc.).   \n",
       "22  [Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)    \n",
       "\n",
       "    similarity_score  \n",
       "23          0.539826  \n",
       "0           0.529084  \n",
       "2           0.493854  \n",
       "34          0.453502  \n",
       "22          0.396312  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'Ich muss verschiedene Stadien eines Manuskripts vergleichen und verschiedene Versionen desselben Schriftstückes nebeneinande darstellen'\n",
    "\n",
    "query_sbert = get_sbert_embeddings(query, sbert_model)\n",
    "\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_sbert = query_sbert.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_sbert, sbert_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"brand_name\",\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 5 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Test reranked SBERT-Representations\n",
    "\n",
    "Now, we can apply reranking to the similariites calculated above and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>initial_similarity_score</th>\n",
       "      <th>rerank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comparo</td>\n",
       "      <td>Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text.  The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability.   Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD](https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud) database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital'](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) , further settings can be made in order to adapt the view to the respective needs or preferences of the user, for example the connecting lines between the versions or the internal variances can be hidden as required. In addition, thanks to the data stored in Comparo, the user can choose which of the documents he would like to compare with one another and in this way put together an individual setting based on versions relevant to his research interests.   Technical Details Comparo is a tool developed in the Java programming language, which is used as a Java archive (JAR) regardless of the platform. Fundamental for the application is the availability of an FuD database, which provides the text versions to be compared and is used at the same time to save the compiled comparison results.</td>\n",
       "      <td>0.539826</td>\n",
       "      <td>2.767452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>0.493854</td>\n",
       "      <td>1.675979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Versioning Machine</td>\n",
       "      <td>The Versioning Machine is a framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI) Guidelines, and is P5 compatible. While the VM provides for features typically found in critical editions, such as annotation and introductory material, it also takes advantage of the opportunities afforded by electronic publication to allow for the comparison diplomatic versions of witnesses, and the ability to easily compare an image of the manuscript with a diplomatic version. VM 5.0 adds a number of new features, including the ability to resize and reorganize text panels, panning and zooming in the image viewer, and text-audio interlinking. The Versioning Machine’s underlying code has also been completely revised to support enhanced features. The Versioning Machine is a useful tool for textual editors, providing an environment that allows editors to immediately see the consequences of their editorial decisions. The platform also has applications in teaching, translation, and digital publication. The many uses of the Versioning Machine are illustrated in the new VM IN USE section. The Versioning Machine can be used locally on a Mac or a PC, or it can be mounted on the Internet for public access. The documentation provided with the software not only provides information about the use of the software, but builds upon the Critical Apparatus chapter of the TEI Guidelines to give further guidance to those who wish to use this method of encoding</td>\n",
       "      <td>0.273050</td>\n",
       "      <td>0.559483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VMR CRE</td>\n",
       "      <td>The Virtual Manuscript Room Collaborative Research Environment (VMR CRE) brings community and a toolbox of powerful research components to support all stages of research and production of a digital edition. Beginning with the popular open-source portal, Liferay, the VMR CRE integrates 30 DH components to naturally support: Cataloging witnesses; managing and displaying images; producing well-formed TEI transcriptions using a web-based WYSIWYM editor and storing those transcriptions to a versioned transcription repository; community volunteer task assignment and project management; automatic realtime collation of witnesses; regularization and apparatus editing; online publishing of the final results-- as a traditional apparatus, or with interactive tools which let users choose different ways to visualize the data produced in the edition.    Overview A walk through the workflow at the Institut für neutestamentliche Textforschung (INTF), in their efforts to edit the Editio Critica Maior (ECM), provides opportunity to touch on many components available in the VMR CRE.  Work can be divided into 9 discrete stages, progressively: 1) witness cataloging; 2) witness selection; 3) image management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus.   Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page.  For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features.  These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF.  A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be.  Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility.  For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John.  A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present.  If a region box is captured, a search query can specify to show the region box clips in the result.  For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips.   Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham.  This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component.  The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes.  The content may then be obtained as EpiDoc influenced TEI. The VMR CRE saves content in a versioned transcription repository backed by Git.  A user may have access to create and edit their own personal transcription, a project-wide transcription, or a site-wide (= published) transcription-- each having version history. The VMR CRE also includes a palaeography tool to assist a transcriber when encountering rare symbols, abbreviations, or ligatures.  If a portion of the unknown text can be identified, the researcher can enter one or more letters and will be presented with images of text instances elsewhere which include these letters, offering possibilities.  As more and more rare text items are tagged, the system grows more helpful. Quality assurance for the ECM requires that a transcription for a manuscript be produced independently by two transcribers.  The products are then compared to each other and differences are reviewed by a manager and reconciled to produce a final transcription.  The VMR CRE provides tools to facilitate this reconciliation work. Collation and Visualization Collation is a key component to find differences in text witnesses when producing a critical edition.  Collation facilities in the VMR CRE are performed by CollateX.  Collation and regularization of uninteresting differences is an iterative cycle in digital editing and the VMR CRE ties these two actions together with an intuitive visual interface.  Visualization of a collation, either during the editing process or for the reader, can be rendered as a variant graph, an alignment table, or as a traditional negative apparatus.    Web Services, Open Programmatic Access The VMR CRE Web Services API layer is primarily useful for exposing the functionality of the VMR CRE to other research projects wishing to access the functionality or contribute to the dataset through their own systems and tools.  The VMR CRE Web Services API generally uses noun/verb nomenclature organized by category.  This means that the last 2 segments of an API URL will consist first of the type of object the call will affect, and second, of the action to be performed on the object.  Any path before the final two segments are merely for organizational purposes.  This is different from a strict REST convention which confines the action to one of 6 HTTP verbs.  The VMR CRE places no semantic meaning on the HTTP verb.  Both GET and POST HTTP verbs are accepted as identical, relegating the verb, or semantic action to the final segment of the URL.  This allows easy testing and examples for every action directly within a web browser.  Parameters to a service request are passed as standard HTTP FORM POST parameters or as query string parameters.</td>\n",
       "      <td>0.334997</td>\n",
       "      <td>-0.265567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TEITOK</td>\n",
       "      <td>TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation, initially developed at the Centro de Linguística da Universidade de Lisboa, later at CELGA-ILTEC, and currently maintained at the ÚFAL institute of Charles University, Prague. The system has a modular design with numerous modules making serving a wide range of different corpus types.  Below are some examples of some of those, and the type of corpora TEITOK can deal with. More modules are added frequently, and it is possible to add custom modules as well.  Historical Corpora  For historical corpora, TEITOK provides the option to have an alignment between the transcription and the facsimile image, it provides the option to work with multiple orthographic realizations to combine several editions of a text into a single XML file, and it provides the option to create a searchable document map to see where in the world several phenomena are more frequent.  TEITOK is freely available for anybody who wishes to create richly annotated textual corpora, and runs on any LINUX based web server.    Features    Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment   Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus   Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments   Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations   Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map   Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression   Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the original project.</td>\n",
       "      <td>0.316536</td>\n",
       "      <td>-0.816235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            brand_name  \\\n",
       "0              Comparo   \n",
       "2             CollateX   \n",
       "16  Versioning Machine   \n",
       "9              VMR CRE   \n",
       "13              TEITOK   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text.  The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability.   Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD](https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud) database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital'](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) , further settings can be made in order to adapt the view to the respective needs or preferences of the user, for example the connecting lines between the versions or the internal variances can be hidden as required. In addition, thanks to the data stored in Comparo, the user can choose which of the documents he would like to compare with one another and in this way put together an individual setting based on versions relevant to his research interests.   Technical Details Comparo is a tool developed in the Java programming language, which is used as a Java archive (JAR) regardless of the platform. Fundamental for the application is the availability of an FuD database, which provides the text versions to be compared and is used at the same time to save the compiled comparison results.    \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The Versioning Machine is a framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI) Guidelines, and is P5 compatible. While the VM provides for features typically found in critical editions, such as annotation and introductory material, it also takes advantage of the opportunities afforded by electronic publication to allow for the comparison diplomatic versions of witnesses, and the ability to easily compare an image of the manuscript with a diplomatic version. VM 5.0 adds a number of new features, including the ability to resize and reorganize text panels, panning and zooming in the image viewer, and text-audio interlinking. The Versioning Machine’s underlying code has also been completely revised to support enhanced features. The Versioning Machine is a useful tool for textual editors, providing an environment that allows editors to immediately see the consequences of their editorial decisions. The platform also has applications in teaching, translation, and digital publication. The many uses of the Versioning Machine are illustrated in the new VM IN USE section. The Versioning Machine can be used locally on a Mac or a PC, or it can be mounted on the Internet for public access. The documentation provided with the software not only provides information about the use of the software, but builds upon the Critical Apparatus chapter of the TEI Guidelines to give further guidance to those who wish to use this method of encoding   \n",
       "9   The Virtual Manuscript Room Collaborative Research Environment (VMR CRE) brings community and a toolbox of powerful research components to support all stages of research and production of a digital edition. Beginning with the popular open-source portal, Liferay, the VMR CRE integrates 30 DH components to naturally support: Cataloging witnesses; managing and displaying images; producing well-formed TEI transcriptions using a web-based WYSIWYM editor and storing those transcriptions to a versioned transcription repository; community volunteer task assignment and project management; automatic realtime collation of witnesses; regularization and apparatus editing; online publishing of the final results-- as a traditional apparatus, or with interactive tools which let users choose different ways to visualize the data produced in the edition.    Overview A walk through the workflow at the Institut für neutestamentliche Textforschung (INTF), in their efforts to edit the Editio Critica Maior (ECM), provides opportunity to touch on many components available in the VMR CRE.  Work can be divided into 9 discrete stages, progressively: 1) witness cataloging; 2) witness selection; 3) image management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus.   Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page.  For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features.  These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF.  A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be.  Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility.  For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John.  A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present.  If a region box is captured, a search query can specify to show the region box clips in the result.  For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips.   Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham.  This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component.  The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes.  The content may then be obtained as EpiDoc influenced TEI. The VMR CRE saves content in a versioned transcription repository backed by Git.  A user may have access to create and edit their own personal transcription, a project-wide transcription, or a site-wide (= published) transcription-- each having version history. The VMR CRE also includes a palaeography tool to assist a transcriber when encountering rare symbols, abbreviations, or ligatures.  If a portion of the unknown text can be identified, the researcher can enter one or more letters and will be presented with images of text instances elsewhere which include these letters, offering possibilities.  As more and more rare text items are tagged, the system grows more helpful. Quality assurance for the ECM requires that a transcription for a manuscript be produced independently by two transcribers.  The products are then compared to each other and differences are reviewed by a manager and reconciled to produce a final transcription.  The VMR CRE provides tools to facilitate this reconciliation work. Collation and Visualization Collation is a key component to find differences in text witnesses when producing a critical edition.  Collation facilities in the VMR CRE are performed by CollateX.  Collation and regularization of uninteresting differences is an iterative cycle in digital editing and the VMR CRE ties these two actions together with an intuitive visual interface.  Visualization of a collation, either during the editing process or for the reader, can be rendered as a variant graph, an alignment table, or as a traditional negative apparatus.    Web Services, Open Programmatic Access The VMR CRE Web Services API layer is primarily useful for exposing the functionality of the VMR CRE to other research projects wishing to access the functionality or contribute to the dataset through their own systems and tools.  The VMR CRE Web Services API generally uses noun/verb nomenclature organized by category.  This means that the last 2 segments of an API URL will consist first of the type of object the call will affect, and second, of the action to be performed on the object.  Any path before the final two segments are merely for organizational purposes.  This is different from a strict REST convention which confines the action to one of 6 HTTP verbs.  The VMR CRE places no semantic meaning on the HTTP verb.  Both GET and POST HTTP verbs are accepted as identical, relegating the verb, or semantic action to the final segment of the URL.  This allows easy testing and examples for every action directly within a web browser.  Parameters to a service request are passed as standard HTTP FORM POST parameters or as query string parameters.     \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation, initially developed at the Centro de Linguística da Universidade de Lisboa, later at CELGA-ILTEC, and currently maintained at the ÚFAL institute of Charles University, Prague. The system has a modular design with numerous modules making serving a wide range of different corpus types.  Below are some examples of some of those, and the type of corpora TEITOK can deal with. More modules are added frequently, and it is possible to add custom modules as well.  Historical Corpora  For historical corpora, TEITOK provides the option to have an alignment between the transcription and the facsimile image, it provides the option to work with multiple orthographic realizations to combine several editions of a text into a single XML file, and it provides the option to create a searchable document map to see where in the world several phenomena are more frequent.  TEITOK is freely available for anybody who wishes to create richly annotated textual corpora, and runs on any LINUX based web server.    Features    Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment   Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus   Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments   Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations   Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map   Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression   Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the original project.      \n",
       "\n",
       "    initial_similarity_score  rerank_score  \n",
       "0                   0.539826      2.767452  \n",
       "2                   0.493854      1.675979  \n",
       "16                  0.273050      0.559483  \n",
       "9                   0.334997     -0.265567  \n",
       "13                  0.316536     -0.816235  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort the results by similarity scores calculated using the sbert-representations in descending order\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# Select the top N (e.g., top 20) results for reranking\n",
    "n = 20\n",
    "top_n_results = result_df_sorted.head(n)\n",
    "\n",
    "# Prepare the input pairs (query, document description) for the reranking model\n",
    "model_inputs = [[query, description] for description in top_n_results[\"description_clean\"]]\n",
    "\n",
    "# Predict the relevance scores using the CrossEncoder model\n",
    "scores = cross_encoder_model.predict(model_inputs)\n",
    "\n",
    "# Create a DataFrame for reranked results\n",
    "reranked_results = pd.DataFrame({\n",
    "    \"brand_name\": top_n_results[\"brand_name\"].values,\n",
    "    \"description_clean\": top_n_results[\"description_clean\"].values,\n",
    "    \"initial_similarity_score\": top_n_results[\"similarity_score\"].values,\n",
    "    \"rerank_score\": scores\n",
    "})\n",
    "\n",
    "# Sort the results by rerank score in descending order\n",
    "reranked_results_sorted = reranked_results.sort_values(by='rerank_score', ascending=False)\n",
    "\n",
    "# Display the top reranked results\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(reranked_results_sorted[['brand_name', 'description_clean', 'initial_similarity_score', 'rerank_score']].head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

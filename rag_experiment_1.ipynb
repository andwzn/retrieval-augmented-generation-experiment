{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and create frequently used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from util.webscraper import WebScraper\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to load the FastText models\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"File was already downloaded.\")\n",
    "            return None\n",
    "        \n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"The file has been downloaded and saved as: {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred while downloading the file: {e}\")\n",
    "        \n",
    "def load_word_vectors(file_path: str):\n",
    "    \"\"\"Load word vectors from a file.\"\"\"\n",
    "    try:\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        print(\"Vectors loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the vectors: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.2/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the data from the API. As the API is not yet published, both the API-Url and the query to get information on edition-software need to be specified in your .env file. (consult the README for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api_url and query\n",
    "api_url = os.environ['API_URL']\n",
    "query = os.environ['QUERY']\n",
    "\n",
    "# get data from api\n",
    "api_response = requests.get(api_url + query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got the data from the API, we can load it into a dataframe to prepare it to be used as a knowledge base for rag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43 entries, 0 to 42\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   id                43 non-null     object\n",
      " 1   slug              43 non-null     object\n",
      " 2   brand_name        43 non-null     object\n",
      " 3   concept_doi       0 non-null      object\n",
      " 4   description       40 non-null     object\n",
      " 5   description_url   3 non-null      object\n",
      " 6   description_type  43 non-null     object\n",
      " 7   get_started_url   41 non-null     object\n",
      " 8   image_id          31 non-null     object\n",
      " 9   is_published      43 non-null     bool  \n",
      " 10  short_statement   43 non-null     object\n",
      " 11  created_at        43 non-null     object\n",
      " 12  updated_at        43 non-null     object\n",
      " 13  closed_source     43 non-null     bool  \n",
      "dtypes: bool(2), object(12)\n",
      "memory usage: 4.2+ KB\n"
     ]
    }
   ],
   "source": [
    "edition_software_info = json.loads(api_response.text)\n",
    "edition_software_info = pd.DataFrame(edition_software_info)\n",
    "edition_software_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief inspection allows us to formulate some initial tasks and questions for this experiment.\n",
    "\n",
    "- **Preprocessing:** As we can see, not a single entry contains a associated concept_doi. We might consider dropping the column.\n",
    "- **Impact of using short descriptions only:** Three entries are missing the in depth description. We can assume that rag won't be too useful for these entries. \n",
    "- **Impact of additional information:** Only three have a description-url. Down the road, we need to evaluate, if adding info from this source improves the performance of the rag-system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove Artefacts\n",
    "\n",
    "Both the `description` and `short_statement` columns seem to be of particular interest for the task at hand. To asses necessary preprocessing step, we'll need to take a closer look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>short_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>The Research Software Directory is a content management system that is tailored to research software.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                             short_statement  \n",
       "0                                                                                                                          Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.  \n",
       "1                                  Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.  \n",
       "2          CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.  \n",
       "3  LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.  \n",
       "4                                                                                                                                                                                                      The Research Software Directory is a content management system that is tailored to research software.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = edition_software_info[[\"description\", \"short_statement\"]]\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(descriptions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `description` column contains some formatting artefacts like `\\n` and markdown syntax like `**` and `#`. Let's clean them up.\n",
    "While we're at it, we can also remove double whitespaces etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autodone</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LaTeX</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Software Directory</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    brand_name  \\\n",
       "0                  Transkribus   \n",
       "1                     Autodone   \n",
       "2                     CollateX   \n",
       "3                        LaTeX   \n",
       "4  Research Software Directory   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)   \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = '\\\\n+'\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "pattern = r'[*#]+|\\s-+\\s|]]' #\\[\\]()<>\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description_clean\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"brand_name\", \"description\", \"description_clean\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fill nan values\n",
    "\n",
    "Before we continue preprocessing the data for later vectorization, we need to check for missing values and replace them with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j42m3p7n5jl51rggmnpn_6940000gn/T/ipykernel_67257/3857300838.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide additional context-information for the retrieval process, we'll scrape all webpages referenced in the software-description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get urls\n",
    "\n",
    "First, we isolate the urls from our description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "      <td>https://autodone.idh.uni-koeln.de/about,https://autodone.idh.uni-koeln.de/,https://autodone.idh.uni-koeln.de/usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Philology,http://collatex.net/,http://en.wikipedia.org/wiki/Sequence_alignment,http://en.wikipedia.org/wiki/Diff,http://en.wikipedia.org/wiki/Textual_criticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)    \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                           urls  \n",
       "0                                                                                                                                                                                           NaN  \n",
       "1                                                                            https://autodone.idh.uni-koeln.de/about,https://autodone.idh.uni-koeln.de/,https://autodone.idh.uni-koeln.de/usage  \n",
       "2  http://en.wikipedia.org/wiki/Philology,http://collatex.net/,http://en.wikipedia.org/wiki/Sequence_alignment,http://en.wikipedia.org/wiki/Diff,http://en.wikipedia.org/wiki/Textual_criticism  \n",
       "3                                                                                                                                                                                           NaN  \n",
       "4                                                                                                                                                                                           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "\n",
    "urls = edition_software_info[\"description\"].str.extractall(pattern)\n",
    "urls = urls.droplevel(1)\n",
    "urls_grouped = urls.groupby(urls.index).agg((lambda x: ','.join(set(x))))\n",
    "edition_software_info[\"urls\"] = urls_grouped\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"description_clean\", \"urls\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape Webpages\n",
    "\n",
    "Now we scrape the paragraphs from the webpages we found. \n",
    "The webscraper will take the list of urls associated with an entry and will save paragraphs from all webpages as a string in a column of our dataframe. \n",
    "\n",
    "**This might take some time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://autodone.idh.uni-koeln.de/about with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/ with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/usage with parameters tags = ['p']\n",
      "Scraping http://collatex.net/ with parameters tags = ['p']\n",
      "Scraping http://vbd.humnet.unipi.it/ with parameters tags = ['p']\n",
      "Scraping http://www.tei-c.org/ with parameters tags = ['p']\n",
      "Scraping https://sites.fastspring.com/stefanhagel/product/cte with parameters tags = ['p']\n",
      "Scraping https://www.oeaw.ac.at/oesterreichische-akademie-der-wissenschaften with parameters tags = ['p']\n",
      "Scraping https://cte.oeaw.ac.at/ with parameters tags = ['p']\n",
      "Scraping http://csel.at/ with parameters tags = ['p']\n",
      "Scraping https://opensource.org/licenses/EUPL-1.2 with parameters tags = ['p']\n",
      "Scraping https://phylipweb.github.io/phylip/general.html with parameters tags = ['p']\n",
      "Scraping https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html with parameters tags = ['p']\n",
      "HTTP error occurred for https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html: 404 Client Error: Not Found for url: https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html\n",
      "Scraping https://campus.dariah.eu/resource/posts/how-to-publish-your-dictionary-data-with-publex with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/publex with parameters tags = ['p']\n",
      "Scraping https://bitbucket.org/tcdh/publex-allentities/src/master/ with parameters tags = ['p']\n",
      "Scraping http://publex.uni-trier.de/dictionary-network with parameters tags = ['p']\n",
      "HTTP error occurred for http://publex.uni-trier.de/dictionary-network: 503 Server Error: Service Unavailable for url: http://publex.uni-trier.de/dictionary-network\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/komplett with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514 with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/transcribo with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://fud.uni-trier.de/community/referenzen/ with parameters tags = ['p']\n",
      "Scraping https://textgridrep.de/ with parameters tags = ['p']\n",
      "Scraping https://ride.i-d-e.de/issues/issue-11/omeka/ with parameters tags = ['p']\n",
      "Scraping https://viscoll.org with parameters tags = ['p']\n",
      "Scraping http://scta.lombardpress.org with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/print.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/check.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/download.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/witnesses.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/image.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/stats.php with parameters tags = ['p']\n"
     ]
    }
   ],
   "source": [
    "webscraper = WebScraper(tags = [\"p\"], exclude = [\"wikipedia\"])\n",
    "edition_software_info[\"webpages_text\"] = edition_software_info[\"urls\"].apply(lambda x: webscraper.scrape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>webpages_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://autodone.idh.uni-koeln.de/about,https:...</td>\n",
       "      <td>autodoneis a service for the automated, time-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://en.wikipedia.org/wiki/Philology,http://...</td>\n",
       "      <td>“In a language, in the system of language, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                urls  \\\n",
       "0                                                NaN   \n",
       "1  https://autodone.idh.uni-koeln.de/about,https:...   \n",
       "2  http://en.wikipedia.org/wiki/Philology,http://...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                       webpages_text  \n",
       "0                                                NaN  \n",
       "1  autodoneis a service for the automated, time-c...  \n",
       "2  “In a language, in the system of language, the...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info[[\"urls\", \"webpages_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is collected from the webpages, we can take a look at the average length of the texts received for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       13.000000\n",
       "mean     15502.692308\n",
       "std      17592.276342\n",
       "min        640.000000\n",
       "25%       2300.000000\n",
       "50%       6739.000000\n",
       "75%      23559.000000\n",
       "max      55063.000000\n",
       "Name: webpages_text, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = edition_software_info[\"webpages_text\"].apply(lambda x: len(x) if not pd.isna(x) else 0)\n",
    "length[length>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking only at entries, that we were able to collected webpage text for, we have an average character count of about 15.000 per entry. \n",
    "The standard deviation is quite large compared to the mean, indicating that there is a high degree of variability in character counts.\n",
    "\n",
    "The distribution is skewed towards entries with lower character counts, while some outliers with a high character counts pull the mean upwards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, 'data/edition_software_info.csv')\n",
    "edition_software_info.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can preprocess the newfound text using the function we defined earlier. Again, we have to replace missing values with empty strings.\n",
    "\n",
    "First, let us reimport the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, \"data/edition_software_info.csv\")\n",
    "edition_software_info = pd.read_csv(path)\n",
    "edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]] = edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove links\n",
    "\n",
    "First, we'll remove all links from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Erkennen, Transkribieren und Durchsuchen von...\n",
       "1    autodone is a service for the automated, time-...\n",
       "2    [CollateX]() is a software to  1. read  multip...\n",
       "Name: description_preprocessed, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_clean\"].str.replace(pattern, '', regex=True)\n",
    "edition_software_info[\"description_preprocessed\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove Stopwords and Punctuation\n",
    "\n",
    "For the later vectorisation of the texts, we remove both german and english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(stopwords: List[str], text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by converting it to lowercase, removing punctuation, and filtering out stopwords.\n",
    "    Args:\n",
    "        stopwords (List[str]): A list of stopwords to be filtered out from the text.\n",
    "        text (str): The input text to be preprocessed.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    \n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    erkennen transkribieren durchsuchen historisch...\n",
       "1    autodone service automated timecontrolled publ...\n",
       "2    collatex software 1 read multiple ≥ 2 versions...\n",
       "3    mathematiker donald e knuth entwickelte ende s...\n",
       "4                                                     \n",
       "Name: description_preprocessed, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get stopwords\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "stopwords_combined = stopwords_german.union(stopwords_english)\n",
    "\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: preprocess(stopwords_combined, x))\n",
    "edition_software_info[\"description_preprocessed\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     \n",
       "1    autodoneis service automated timecontrolled pu...\n",
       "2    “in language system language differences”– jac...\n",
       "3                                                     \n",
       "4                                                     \n",
       "Name: webpages_text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info[\"webpages_text_preprocessed\"] = edition_software_info[\"webpages_text\"].apply(lambda x: preprocess(stopwords_combined, x) if not pd.isna(x) else \"\")\n",
    "edition_software_info[\"webpages_text_preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lemmatize\n",
    "\n",
    "Finally, we can lemmatize our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef lemmatize_english_text(text):\\n    doc = nlp_en(text)\\n    return \\' \\'.join([token.lemma_ for token in doc])\\n\\ndef lemmatize_german_text(text):\\n    doc = nlp_de(text)\\n    return \\' \\'.join([token.lemma_ for token in doc])\\n\\nedition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_english_text(x))\\nedition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_german_text(x))\\nedition_software_info[\"description_preprocessed\"].head()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def lemmatize_english_text(text):\n",
    "    doc = nlp_en(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def lemmatize_german_text(text):\n",
    "    doc = nlp_de(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_english_text(x))\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_german_text(x))\n",
    "edition_software_info[\"description_preprocessed\"].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add Language Information\n",
    "\n",
    "As the entries in our repository are in both english and german, we add information on the texts language to the dataset.\n",
    "\n",
    "To do so, we'll use a fasttext-model for language identification, which can be found [here](https://fasttext.cc/docs/en/language-identification.html).\n",
    "\n",
    "As the model was trained on UTF-8 data, it expects UTF-8 as input. This sould be the case, as pandas `read_csv`-function imports text in UTF-8 by default.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found.\n"
     ]
    }
   ],
   "source": [
    "# download the fasttext model\n",
    "\n",
    "fasttext_path = os.path.join(os.getcwd(), 'models/lid.176.bin')\n",
    "\n",
    "if os.path.isfile(fasttext_path):\n",
    "    print(\"Model found.\")\n",
    "    language_detection_model = fasttext.load_model(fasttext_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_path)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    language_detection_model = fasttext.load_model(fasttext_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model to identify the languages of our entries. This might take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>description_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte E...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EVT (Edition Visualization Technology) is a so...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Der Classical Text Editor (CTE) wird auf Initi...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In vielen Editionsprojekten wird die Datenbank...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td># Benutzerfreundliches Arbeiten  Als zentrale...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description description_lang\n",
       "0  # Erkennen, Transkribieren und Durchsuchen von...               de\n",
       "1  autodone is a service for the automated, time-...               en\n",
       "2  [CollateX](http://collatex.net/) is a software...               en\n",
       "3  Der Mathematiker Donald E. Knuth entwickelte E...               de\n",
       "4                                                                 NaN\n",
       "5                                                                 NaN\n",
       "6  EVT (Edition Visualization Technology) is a so...               en\n",
       "7  Der Classical Text Editor (CTE) wird auf Initi...               de\n",
       "8  In vielen Editionsprojekten wird die Datenbank...               de\n",
       "9   # Benutzerfreundliches Arbeiten  Als zentrale...               de"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fasttext_model = fasttext.load_model(fasttext_path)\n",
    "\n",
    "def identify_language(text):\n",
    "    lang_detected = fasttext_model.predict(text)\n",
    "    return lang_detected[0][0]\n",
    "\n",
    "# clean the webpage text, as the model expect text without newlines\n",
    "edition_software_info.loc[:,\"description\"] = edition_software_info[\"description\"].str.replace(\"\\n\",\" \")\n",
    "\n",
    "# detect message-  and webpage-languages. If the column contains empty text, the language is set to nan\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description\"].apply(lambda x: identify_language(x) if not x == '' else np.nan)\n",
    "\n",
    "# clean the output\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description_lang\"].str.replace(\"__label__\",\"\")\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description_lang\"].str.replace(\"__label__\",\"\")\n",
    "\n",
    "# print the new columns\n",
    "edition_software_info[[\"description\",\"description_lang\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 1: TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with a simple TF-IDF vectorization.\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)** is a weighting scheme that weights the cells of a term-document matrix by their potential to be discriminatory.\n",
    "\n",
    "To do so, we first calculate the **term frequency (TF)**. The term frequency represents the number of instances of a given word $t$ in a document $d$.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Count of } t \\text{ in } d}{\\text{Total number of words in } d}\n",
    "$$\n",
    "\n",
    "This term frequency is then multiplied by the **inverse document frequency (IDF)**. The IDF is calculated by counting all documents that contain a term $t$ (the document frequency $\\text{df}(t)$). Then, we divide the total number of documents $N$ in the corpus by $\\text{df}(t)$.\n",
    "\n",
    "This inverse frequency is chosen over the regular frequency to **downweight** terms that appear in many documents, since these terms are less likely to be useful for distinguishing between documents.\n",
    "\n",
    "Usually, we also take the logarithm of the IDF to smooth out the very large values that can occur when a term appears in only a few documents. This ensures that rare terms are not excessively weighted.\n",
    "\n",
    "$$\n",
    "\\text{df}(t) = \\text{Document frequency of a term } t\n",
    "$$\n",
    "$$\n",
    "N = \\text{Number of documents}\n",
    "$$\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{\\text{df}(t)}\\right)\n",
    "$$\n",
    "\n",
    "Finally, we calculate the **TF-IDF** by multiplying the term frequency $\\text{TF}(t, d)$ with the inverse document frequency $\\text{IDF}(t)$.\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "The resulting value can be interpreted as a measure of the importance of the term in a document relative to the entire corpus. Terms that are frequent in a document but rare across the corpus will have higher TF-IDF scores, indicating their importance.\n",
    "\n",
    "\n",
    "**N-grams:**\n",
    "\n",
    "To capture not just the importance of single words but also some of the **context** in which they are used, we can apply TF-IDF to **n-grams**. N-grams are contiguous sequences of $n$ words that appear together in a text. The size of the sequence, $n$, is a hyperparameter that can be adjusted depending on the specific task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Fit TF-IDF Vectorizer\n",
    "First, we fit the vectorizer on the preprocessed descriptions. \n",
    "This way, the vectorizer can transform text into numerical feature vectors based on the learned vocabulary and its distribution over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12</th>\n",
       "      <th>12 languages</th>\n",
       "      <th>12 languages comprehensive</th>\n",
       "      <th>12 languages comprehensive righttoleft</th>\n",
       "      <th>1514</th>\n",
       "      <th>1514 digital</th>\n",
       "      <th>1514 digital marburg</th>\n",
       "      <th>1514 digital marburg büchner</th>\n",
       "      <th>19042024</th>\n",
       "      <th>19042024 official</th>\n",
       "      <th>...</th>\n",
       "      <th>überführt neben klassischen</th>\n",
       "      <th>überführt neben klassischen pdfformat</th>\n",
       "      <th>überschaubaren</th>\n",
       "      <th>überschaubaren teil</th>\n",
       "      <th>überschaubaren teil funktionen</th>\n",
       "      <th>überschaubaren teil funktionen aufgrund</th>\n",
       "      <th>überwiegende</th>\n",
       "      <th>überwiegende teil</th>\n",
       "      <th>überwiegende teil funktionen</th>\n",
       "      <th>überwiegende teil funktionen editionen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    12  12 languages  12 languages comprehensive  \\\n",
       "0  0.0           0.0                         0.0   \n",
       "1  0.0           0.0                         0.0   \n",
       "2  0.0           0.0                         0.0   \n",
       "3  0.0           0.0                         0.0   \n",
       "4  0.0           0.0                         0.0   \n",
       "\n",
       "   12 languages comprehensive righttoleft  1514  1514 digital  \\\n",
       "0                                     0.0   0.0           0.0   \n",
       "1                                     0.0   0.0           0.0   \n",
       "2                                     0.0   0.0           0.0   \n",
       "3                                     0.0   0.0           0.0   \n",
       "4                                     0.0   0.0           0.0   \n",
       "\n",
       "   1514 digital marburg  1514 digital marburg büchner  19042024  \\\n",
       "0                   0.0                           0.0  0.000000   \n",
       "1                   0.0                           0.0  0.064201   \n",
       "2                   0.0                           0.0  0.000000   \n",
       "3                   0.0                           0.0  0.000000   \n",
       "4                   0.0                           0.0  0.000000   \n",
       "\n",
       "   19042024 official  ...  überführt neben klassischen  \\\n",
       "0           0.000000  ...                          0.0   \n",
       "1           0.064201  ...                          0.0   \n",
       "2           0.000000  ...                          0.0   \n",
       "3           0.000000  ...                          0.0   \n",
       "4           0.000000  ...                          0.0   \n",
       "\n",
       "   überführt neben klassischen pdfformat  überschaubaren  überschaubaren teil  \\\n",
       "0                                    0.0             0.0                  0.0   \n",
       "1                                    0.0             0.0                  0.0   \n",
       "2                                    0.0             0.0                  0.0   \n",
       "3                                    0.0             0.0                  0.0   \n",
       "4                                    0.0             0.0                  0.0   \n",
       "\n",
       "   überschaubaren teil funktionen  überschaubaren teil funktionen aufgrund  \\\n",
       "0                             0.0                                      0.0   \n",
       "1                             0.0                                      0.0   \n",
       "2                             0.0                                      0.0   \n",
       "3                             0.0                                      0.0   \n",
       "4                             0.0                                      0.0   \n",
       "\n",
       "   überwiegende  überwiegende teil  überwiegende teil funktionen  \\\n",
       "0           0.0                0.0                           0.0   \n",
       "1           0.0                0.0                           0.0   \n",
       "2           0.0                0.0                           0.0   \n",
       "3           0.0                0.0                           0.0   \n",
       "4           0.0                0.0                           0.0   \n",
       "\n",
       "   überwiegende teil funktionen editionen  \n",
       "0                                     0.0  \n",
       "1                                     0.0  \n",
       "2                                     0.0  \n",
       "3                                     0.0  \n",
       "4                                     0.0  \n",
       "\n",
       "[5 rows x 18228 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(edition_software_info[\"description_preprocessed\"])\n",
    "\n",
    "# display the resulting matrix\n",
    "tfidf_matrix_beautify = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_matrix_beautify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Inspect the tf-idf representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in this dataframe is a unique word, while each row is a document. The cells denote the number of occurances of a word in a document, weighted by the words potential to be distinctive.\n",
    "\n",
    "Let's take a look at the tf-idf filtered words for each description (You can find them in the column \"tf_idf_filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>tf_idf_filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>[(dokumenten, 0.24192478472674683), (durchsuchen, 0.12096239236337342), (erkennen, 0.12096239236337342), (erkennung, 0.12096239236337342), (transkribieren, 0.12096239236337342)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "      <td>[(twitter, 0.1926029459944442), (autodone, 0.1284019639962961), (usage, 0.11567598600740456), (service, 0.10664676093079592)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>[(differences, 0.12889590979174362), (computational, 0.10345985709520088), (similarities, 0.10345985709520088), (similarities differences, 0.10345985709520088), (tokens, 0.10345985709520088)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>[(jahre, 0.1928958299837409)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)    \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                             tf_idf_filtered_words  \n",
       "0                [(dokumenten, 0.24192478472674683), (durchsuchen, 0.12096239236337342), (erkennen, 0.12096239236337342), (erkennung, 0.12096239236337342), (transkribieren, 0.12096239236337342)]  \n",
       "1                                                                    [(twitter, 0.1926029459944442), (autodone, 0.1284019639962961), (usage, 0.11567598600740456), (service, 0.10664676093079592)]  \n",
       "2  [(differences, 0.12889590979174362), (computational, 0.10345985709520088), (similarities, 0.10345985709520088), (similarities differences, 0.10345985709520088), (tokens, 0.10345985709520088)]  \n",
       "3                                                                                                                                                                    [(jahre, 0.1928958299837409)]  \n",
       "4                                                                                                                                                                                               []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the threshold for TF-IDF scores\n",
    "threshold = 0.1\n",
    "\n",
    "# Filter words with TF-IDF scores greater than the threshold for each document\n",
    "def filter_words_by_threshold(row, threshold):\n",
    "    filtered_words = [(word, score) for word, score in zip(tfidf_matrix_beautify.columns, row) if score > threshold]\n",
    "    return sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Apply the function to each row of the TF-IDF DataFrame\n",
    "filtered_words = tfidf_matrix_beautify.apply(lambda row: filter_words_by_threshold(row, threshold), axis=1)\n",
    "\n",
    "# Create a dataframe to display the filtered words\n",
    "filtered_words_df = pd.DataFrame(filtered_words, columns=[\"tf_idf_filtered_words\"])\n",
    "tfidf_display = pd.concat([edition_software_info[\"description_clean\"], filtered_words_df], axis=1)\n",
    "    \n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(tfidf_display[[\"description_clean\", \"tf_idf_filtered_words\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test tfidf-representation\n",
    "\n",
    "This cell will return the most relevant documents from our dataset based on a comparison of their tf-idf representations and a query. The query can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)</td>\n",
       "      <td>0.091537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Scripto is an open-source tool that permits registered users to view digital files and transcribe them with an easy-to-use toolbar, rendering that text searchable. The tool includes a versioning history and editorial controls to make public contributions more manageable, and supports the transcription of a wide range of file types (both images and documents). There are two versions of Scripto, each of which works with a different version of Omeka. Scripto for Omeka Classic creates a single transcription project for the content of your Omeka Classic site. Scripto for Omeka S enables the creation of multiple projects built from shared items in your Omeka S installation.</td>\n",
       "      <td>0.066523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>The Virtual Manuscript Room Collaborative Research Environment (VMR CRE) brings community and a toolbox of powerful research components to support all stages of research and production of a digital edition. Beginning with the popular open-source portal, Liferay, the VMR CRE integrates 30 DH components to naturally support: Cataloging witnesses; managing and displaying images; producing well-formed TEI transcriptions using a web-based WYSIWYM editor and storing those transcriptions to a versioned transcription repository; community volunteer task assignment and project management; automatic realtime collation of witnesses; regularization and apparatus editing; online publishing of the final results-- as a traditional apparatus, or with interactive tools which let users choose different ways to visualize the data produced in the edition.    Overview A walk through the workflow at the Institut für neutestamentliche Textforschung (INTF), in their efforts to edit the Editio Critica Maior (ECM), provides opportunity to touch on many components available in the VMR CRE.  Work can be divided into 9 discrete stages, progressively: 1) witness cataloging; 2) witness selection; 3) image management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus.   Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page.  For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features.  These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF.  A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be.  Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility.  For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John.  A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present.  If a region box is captured, a search query can specify to show the region box clips in the result.  For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips.   Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham.  This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component.  The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes.  The content may then be obtained as EpiDoc influenced TEI. The VMR CRE saves content in a versioned transcription repository backed by Git.  A user may have access to create and edit their own personal transcription, a project-wide transcription, or a site-wide (= published) transcription-- each having version history. The VMR CRE also includes a palaeography tool to assist a transcriber when encountering rare symbols, abbreviations, or ligatures.  If a portion of the unknown text can be identified, the researcher can enter one or more letters and will be presented with images of text instances elsewhere which include these letters, offering possibilities.  As more and more rare text items are tagged, the system grows more helpful. Quality assurance for the ECM requires that a transcription for a manuscript be produced independently by two transcribers.  The products are then compared to each other and differences are reviewed by a manager and reconciled to produce a final transcription.  The VMR CRE provides tools to facilitate this reconciliation work. Collation and Visualization Collation is a key component to find differences in text witnesses when producing a critical edition.  Collation facilities in the VMR CRE are performed by CollateX.  Collation and regularization of uninteresting differences is an iterative cycle in digital editing and the VMR CRE ties these two actions together with an intuitive visual interface.  Visualization of a collation, either during the editing process or for the reader, can be rendered as a variant graph, an alignment table, or as a traditional negative apparatus.    Web Services, Open Programmatic Access The VMR CRE Web Services API layer is primarily useful for exposing the functionality of the VMR CRE to other research projects wishing to access the functionality or contribute to the dataset through their own systems and tools.  The VMR CRE Web Services API generally uses noun/verb nomenclature organized by category.  This means that the last 2 segments of an API URL will consist first of the type of object the call will affect, and second, of the action to be performed on the object.  Any path before the final two segments are merely for organizational purposes.  This is different from a strict REST convention which confines the action to one of 6 HTTP verbs.  The VMR CRE places no semantic meaning on the HTTP verb.  Both GET and POST HTTP verbs are accepted as identical, relegating the verb, or semantic action to the final segment of the URL.  This allows easy testing and examples for every action directly within a web browser.  Parameters to a service request are passed as standard HTTP FORM POST parameters or as query string parameters.</td>\n",
       "      <td>0.065983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          description_clean  \\\n",
       "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)    \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Scripto is an open-source tool that permits registered users to view digital files and transcribe them with an easy-to-use toolbar, rendering that text searchable. The tool includes a versioning history and editorial controls to make public contributions more manageable, and supports the transcription of a wide range of file types (both images and documents). There are two versions of Scripto, each of which works with a different version of Omeka. Scripto for Omeka Classic creates a single transcription project for the content of your Omeka Classic site. Scripto for Omeka S enables the creation of multiple projects built from shared items in your Omeka S installation.    \n",
       "42  The Virtual Manuscript Room Collaborative Research Environment (VMR CRE) brings community and a toolbox of powerful research components to support all stages of research and production of a digital edition. Beginning with the popular open-source portal, Liferay, the VMR CRE integrates 30 DH components to naturally support: Cataloging witnesses; managing and displaying images; producing well-formed TEI transcriptions using a web-based WYSIWYM editor and storing those transcriptions to a versioned transcription repository; community volunteer task assignment and project management; automatic realtime collation of witnesses; regularization and apparatus editing; online publishing of the final results-- as a traditional apparatus, or with interactive tools which let users choose different ways to visualize the data produced in the edition.    Overview A walk through the workflow at the Institut für neutestamentliche Textforschung (INTF), in their efforts to edit the Editio Critica Maior (ECM), provides opportunity to touch on many components available in the VMR CRE.  Work can be divided into 9 discrete stages, progressively: 1) witness cataloging; 2) witness selection; 3) image management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus.   Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page.  For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features.  These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF.  A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be.  Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility.  For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John.  A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present.  If a region box is captured, a search query can specify to show the region box clips in the result.  For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips.   Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham.  This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component.  The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes.  The content may then be obtained as EpiDoc influenced TEI. The VMR CRE saves content in a versioned transcription repository backed by Git.  A user may have access to create and edit their own personal transcription, a project-wide transcription, or a site-wide (= published) transcription-- each having version history. The VMR CRE also includes a palaeography tool to assist a transcriber when encountering rare symbols, abbreviations, or ligatures.  If a portion of the unknown text can be identified, the researcher can enter one or more letters and will be presented with images of text instances elsewhere which include these letters, offering possibilities.  As more and more rare text items are tagged, the system grows more helpful. Quality assurance for the ECM requires that a transcription for a manuscript be produced independently by two transcribers.  The products are then compared to each other and differences are reviewed by a manager and reconciled to produce a final transcription.  The VMR CRE provides tools to facilitate this reconciliation work. Collation and Visualization Collation is a key component to find differences in text witnesses when producing a critical edition.  Collation facilities in the VMR CRE are performed by CollateX.  Collation and regularization of uninteresting differences is an iterative cycle in digital editing and the VMR CRE ties these two actions together with an intuitive visual interface.  Visualization of a collation, either during the editing process or for the reader, can be rendered as a variant graph, an alignment table, or as a traditional negative apparatus.    Web Services, Open Programmatic Access The VMR CRE Web Services API layer is primarily useful for exposing the functionality of the VMR CRE to other research projects wishing to access the functionality or contribute to the dataset through their own systems and tools.  The VMR CRE Web Services API generally uses noun/verb nomenclature organized by category.  This means that the last 2 segments of an API URL will consist first of the type of object the call will affect, and second, of the action to be performed on the object.  Any path before the final two segments are merely for organizational purposes.  This is different from a strict REST convention which confines the action to one of 6 HTTP verbs.  The VMR CRE places no semantic meaning on the HTTP verb.  Both GET and POST HTTP verbs are accepted as identical, relegating the verb, or semantic action to the final segment of the URL.  This allows easy testing and examples for every action directly within a web browser.  Parameters to a service request are passed as standard HTTP FORM POST parameters or as query string parameters.     \n",
       "\n",
       "    similarity_score  \n",
       "22          0.091537  \n",
       "38          0.066523  \n",
       "42          0.065983  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I am looking for a transcription tool'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# Transform the query to TF-IDF space\n",
    "query_tfidf = tfidf_vectorizer.transform([query]) \n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities[0]\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['description_clean', 'similarity_score']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 2: Aggregated Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create document representations by aggregating the word2vec embeddings of each word in a description. \n",
    "\n",
    "Word2vec encodes the meaning of the words by capturing their semantic relationships based on the context in which they appear. By aggregating the word2vec embeddings of each word in a description, we can create a document representation that retains the semantic information and provides a more nuanced understanding of the content.\n",
    "\n",
    "From a computational perspective, these representations are shorter and denser than tf-idf representations, making them more suitable for computations such as similarity measures, clustering, or classification tasks. The dense nature of word2vec embeddings allows for efficient storage and faster processing compared to sparse representations like tf-idf. Additionally, because word2vec captures the meaning and context of words, it can provide more meaningful insights into the relationships between different documents or terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "path = os.path.join(current_path, \"models/word2vec-google-news-300.bin\")\n",
    "\n",
    "# Load the model if it is already in our project. If not, download it.\n",
    "if os.path.isfile(path):\n",
    "    print(\"Model found. Loading...\")\n",
    "    word2vec_model = KeyedVectors.load(path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    word2vec_model.save(path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #TODO: Add some preprocessing\\ndef preprocess_word2vec(text: str) -> str:\\n    pass\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #TODO: Add some preprocessing\n",
    "def preprocess_word2vec(text: str) -> str:\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the document representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word2vec_vector(words, model):\n",
    "    words = words.split()\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in model]\n",
    "    \n",
    "    if not valid_words:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the vectors of the valid words to create a document representation\n",
    "    vectors = [model[word] for word in valid_words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply the function to create aggregated vectors\n",
    "word2vec = edition_software_info['description_preprocessed'].apply(lambda x: get_word2vec_vector(x, word2vec_model))\n",
    "\n",
    "# Convert the Series of 1D arrays to a 2D numpy array (to calculate the cosine similarity later on)\n",
    "word2vec_array = np.array(word2vec.tolist())\n",
    "len(word2vec_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test Word2Vec Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Digital Mappa</td>\n",
       "      <td>The premise of DM is simple: if you have a collection of digital images and texts, then you should be able to develop a project where you can identify specific moments on these images and texts, annotate them as much as you want, link them together, generate searchable content, collaborate with your friends, and publish your work online for others to see and share. DM was developed on the Heroku cloud-server platform, where installation and administration is straightforward, after which developing individual projects requires no specific IT expertise.    Features   Highlights Mark up your image and text documents with highlights you can then annotate and link together. Identify discrete moments on images and texts with highlight tools including dots, lines, circles, rectangles, other polygons, text tags, and multiple color options.   Annotations Develop your projects and publications with an unlimited number of annotations on individual highlights and entire images and texts. Highlights and entire documents can host an unlimited number of annotations, and annotations themselves can host additional annotation layers.   Links Once you've marked up your text and image documents with highlights and annotations, you can create links between individual highlights and entire documents, and your links are always bi-directional, so you and other viewers can travel back and forth between highlights.</td>\n",
       "      <td>0.544934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>IIIF Universal Viewer</td>\n",
       "      <td>The Universal Viewer is a popular IIIF viewer particularly prevalent in National Cultural Heritage Institutions. It is a plugin for Omeka CMS that adds the IIIF specifications in order to serve images like a simple image server, similar to a basic IIP Image, and the UniversalViewer, a unified online player for any file. It can display books, images, maps, audio, movies, pdf, 3D, and anything else as long as the appropriate extension is installed. Rotation, zoom, inside search, etc. may be managed too. Dynamic lists of records may be used, for example for browse pages. The Universal Viewer supports the IXIF media extension too, so manifests can be served for any type of file. For non-images files, it is recommended to use a specific viewer or the Universal Viewer, a widget that can display books, images, maps, audio, movies, pdf, 3D, and anything else as long as the appropriate extension is installed. The Universal Viewer was firstly developed by Digirati for the Wellcome Library of the British Library and the National Library of Wales, then open sourced.</td>\n",
       "      <td>0.528341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Mirador</td>\n",
       "      <td>Mirador is a configurable, extensible, and easy-to-integrate image viewer, which enables image annotation and comparison of images from repositories dispersed around the world. Mirador has been optimized to display resources from repositories that support the International Image Interoperability Framework (IIIF) API's. Mirador provides several workspaces for comparing image-based resources, suitable for use in both cultural heritage and research settings. It allows you to: Load images from any IIIF-compliant repositories through manifests Annotate images Rotate and manipulate images Download selected region or entire image Compare images side by side   FEATURES   CLASSIC IMAGE VIEWER FEATURES Compare images Zoom and pan Book, gallery, and scroll views   RICH METADATA DISPLAY Item and canvas level metadata Table of contents Multiple sequences   FLEXIBLE CONFIGURATION Mosaic workspace for snap-to-grid window display Elastic workspace for free window arrangement Easily import and export your workspace state for later reuse   ANNOTATIONS Annotation creation available via plugin Supports viewing both OpenAnnotation and W3C Web Annotation standards   IIIF API SUPPORT v2 and v3 Image and Presentation Authentication Search Image Level 0 Audio and Video support through Presentation v3 IIIF Collection viewing and navigation   MULTILINGUAL SUPPORT Available in 12 languages Comprehensive right-to-left support   CUSTOMIZABLE OUT OF THE BOX Rich set of out-of-the-box configurations Themable Light / Dark mode   EXTENSIBLE Available as a single JS file or customizable build Plugin framework enables additional customization   OPEN SOURCE, COMMUNITY-DRIVEN SOFTWARE Built with an accessibility focus Friendly developer community Continuous integration and testing</td>\n",
       "      <td>0.500089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Monasterium - Collaborative Archive</td>\n",
       "      <td>MOM-CA is a collaborative database system which allows for the research and editing of medieval and early modern charters.  By and large you can find the charters organized according to archival \"fonds\"; charters that originate from different sources are subsumed under \"Collections\".   General users can:  browse  search  Registered users can:  bookmark documents save and edit documents (description, transcription) with EditMOM3 create indices manipulate images (annotate images, create collections of image fragments, manipulate image fragments) create own charter collections  Special users can:  review user changes (moderators only) manage metadata on archives and collections (metadata manager only) import new data (metadata manager only) translate system messages (translators only) manage users (user manager only) manage static texts (like help, introduction etc., html-authors only)</td>\n",
       "      <td>0.490232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Transcribo</td>\n",
       "      <td>[Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)</td>\n",
       "      <td>0.467653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             brand_name  \\\n",
       "25                        Digital Mappa   \n",
       "20                IIIF Universal Viewer   \n",
       "36                              Mirador   \n",
       "40  Monasterium - Collaborative Archive   \n",
       "22                           Transcribo   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             description_clean  \\\n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The premise of DM is simple: if you have a collection of digital images and texts, then you should be able to develop a project where you can identify specific moments on these images and texts, annotate them as much as you want, link them together, generate searchable content, collaborate with your friends, and publish your work online for others to see and share. DM was developed on the Heroku cloud-server platform, where installation and administration is straightforward, after which developing individual projects requires no specific IT expertise.    Features   Highlights Mark up your image and text documents with highlights you can then annotate and link together. Identify discrete moments on images and texts with highlight tools including dots, lines, circles, rectangles, other polygons, text tags, and multiple color options.   Annotations Develop your projects and publications with an unlimited number of annotations on individual highlights and entire images and texts. Highlights and entire documents can host an unlimited number of annotations, and annotations themselves can host additional annotation layers.   Links Once you've marked up your text and image documents with highlights and annotations, you can create links between individual highlights and entire documents, and your links are always bi-directional, so you and other viewers can travel back and forth between highlights.    \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The Universal Viewer is a popular IIIF viewer particularly prevalent in National Cultural Heritage Institutions. It is a plugin for Omeka CMS that adds the IIIF specifications in order to serve images like a simple image server, similar to a basic IIP Image, and the UniversalViewer, a unified online player for any file. It can display books, images, maps, audio, movies, pdf, 3D, and anything else as long as the appropriate extension is installed. Rotation, zoom, inside search, etc. may be managed too. Dynamic lists of records may be used, for example for browse pages. The Universal Viewer supports the IXIF media extension too, so manifests can be served for any type of file. For non-images files, it is recommended to use a specific viewer or the Universal Viewer, a widget that can display books, images, maps, audio, movies, pdf, 3D, and anything else as long as the appropriate extension is installed. The Universal Viewer was firstly developed by Digirati for the Wellcome Library of the British Library and the National Library of Wales, then open sourced.   \n",
       "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Mirador is a configurable, extensible, and easy-to-integrate image viewer, which enables image annotation and comparison of images from repositories dispersed around the world. Mirador has been optimized to display resources from repositories that support the International Image Interoperability Framework (IIIF) API's. Mirador provides several workspaces for comparing image-based resources, suitable for use in both cultural heritage and research settings. It allows you to: Load images from any IIIF-compliant repositories through manifests Annotate images Rotate and manipulate images Download selected region or entire image Compare images side by side   FEATURES   CLASSIC IMAGE VIEWER FEATURES Compare images Zoom and pan Book, gallery, and scroll views   RICH METADATA DISPLAY Item and canvas level metadata Table of contents Multiple sequences   FLEXIBLE CONFIGURATION Mosaic workspace for snap-to-grid window display Elastic workspace for free window arrangement Easily import and export your workspace state for later reuse   ANNOTATIONS Annotation creation available via plugin Supports viewing both OpenAnnotation and W3C Web Annotation standards   IIIF API SUPPORT v2 and v3 Image and Presentation Authentication Search Image Level 0 Audio and Video support through Presentation v3 IIIF Collection viewing and navigation   MULTILINGUAL SUPPORT Available in 12 languages Comprehensive right-to-left support   CUSTOMIZABLE OUT OF THE BOX Rich set of out-of-the-box configurations Themable Light / Dark mode   EXTENSIBLE Available as a single JS file or customizable build Plugin framework enables additional customization   OPEN SOURCE, COMMUNITY-DRIVEN SOFTWARE Built with an accessibility focus Friendly developer community Continuous integration and testing    \n",
       "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            MOM-CA is a collaborative database system which allows for the research and editing of medieval and early modern charters.  By and large you can find the charters organized according to archival \"fonds\"; charters that originate from different sources are subsumed under \"Collections\".   General users can:  browse  search  Registered users can:  bookmark documents save and edit documents (description, transcription) with EditMOM3 create indices manipulate images (annotate images, create collections of image fragments, manipulate image fragments) create own charter collections  Special users can:  review user changes (moderators only) manage metadata on archives and collections (metadata manager only) import new data (metadata manager only) translate system messages (translators only) manage users (user manager only) manage static texts (like help, introduction etc., html-authors only)    \n",
       "22  [Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)    \n",
       "\n",
       "    similarity_score  \n",
       "25          0.544934  \n",
       "20          0.528341  \n",
       "36          0.500089  \n",
       "40          0.490232  \n",
       "22          0.467653  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I am working with images'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# get vector representation of the query using word2vec\n",
    "query_word2vec = get_word2vec_vector(query, word2vec_model)\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_word2vec = query_word2vec.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_word2vec, word2vec_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"brand_name\",\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 3: FastText\n",
    "\n",
    "One downside of pretrained Word2Vec representations is their inability to handle words not contained in their vocabulary. \n",
    "\n",
    "FastText overcomes this issue by representing words not only as embeddings but also as collections of embedded character n-grams. This approach allows FastText to generate meaningful word vectors for previously unseen words, which is particularly useful when dealing with highly specialized terminologies. In our dataset, which contains such specialized language, FastText may therefore offer more reliable performance compared to traditional Word2Vec models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the models\n",
    "\n",
    "FastText provides pre-aligned word vectors, meaning that word vectors for different languages (like German and English) have already been mapped into a common vector space. This allows words with similar meanings across different languages to have similar vector representations, which is crucial when working with multilingual datasets.\n",
    "\n",
    "Since our dataset contains both German and English texts, we need to download the pre-aligned FastText models for these two languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_file_path: str, extract_to: str) -> None:\n",
    "    \"\"\"Unzip a file to a target directory.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Unzipped {zip_file_path} to {extract_to}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error while unzipping the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Check if the english model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future. \n",
    "# This might take a while\n",
    "\n",
    "current_path = os.getcwd()\n",
    "models_dir = os.path.join(current_path, \"models\")\n",
    "fasttext_eng_zip_path = os.path.join(models_dir, \"wiki.en.zip\")\n",
    "fasttext_eng_path_vec = os.path.join(models_dir, \"wiki.en.vec\")\n",
    "fasttext_eng_path_bin = os.path.join(models_dir, \"wiki.en.bin\")\n",
    "\n",
    "if os.path.isfile(fasttext_eng_path_bin):\n",
    "    print(\"Model found.\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model(fasttext_eng_path_bin) #load the full model, including subword information.\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\" #IDEA: Download the bin model, as it contains subword info. Use this subwordinfo to handle all unknown words.\n",
    "    # download the models\n",
    "    download_file(url, fasttext_eng_zip_path)\n",
    "    \n",
    "    print(\"Unzipping the file...\")\n",
    "    unzip_file(fasttext_eng_zip_path, models_dir)    \n",
    "\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model.load(fasttext_eng_path_bin)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_eng.save(fasttext_eng_path_bin)\n",
    "\n",
    "    \n",
    "if aligned_vectors_eng is None:\n",
    "    raise ValueError(\"The FastText model not loaded properly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n"
     ]
    }
   ],
   "source": [
    "# Check if the german model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future.\n",
    "# This might take a while\n",
    "\n",
    "fasttext_de_path_bin = os.path.join(current_path, \"models/wiki_de_align.bin\")\n",
    "fasttext_de_path_vec = os.path.join(current_path, \"models/wiki_de_align.vec\")\n",
    "\n",
    "if os.path.isfile(fasttext_de_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_de = KeyedVectors.load(fasttext_de_path_bin)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_de_path_vec)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_de = load_word_vectors(fasttext_de_path_vec)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_de.save(fasttext_de_path_bin)\n",
    "    \n",
    "if aligned_vectors_de is None:\n",
    "    raise ValueError(\"The FastText model or vectors were not loaded properly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we loaded both models, let's check if they are properly aligned. \n",
    "\n",
    "To do so, we'll pick an english word, get its english vector representation and return the most similar word vector in the german vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German word vector closest to skyscraper: [('hochhaus', 0.5159210562705994), ('wolkenkratzer', 0.4891827404499054), ('bürohochhaus', 0.4611120820045471), ('hochhausturm', 0.43771031498908997), ('wolkenkratzern', 0.43421125411987305), ('hochhäuser', 0.4312896728515625), ('bürohochhäuser', 0.4262181520462036), ('wolkenkratzers', 0.4224645495414734), ('hochhausbau', 0.4211919605731964), ('„wolkenkratzer', 0.42107880115509033)]\n"
     ]
    }
   ],
   "source": [
    "# get the vector-representation of a random english word\n",
    "word_english = 'skyscraper'\n",
    "word_vector_in_english = aligned_vectors_eng.wv[word_english]\n",
    "\n",
    "print(f\"German word vector closest to {word_english}:\", aligned_vectors_de.most_similar(positive=[word_vector_in_english]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Document Representations\n",
    "\n",
    "Now that we confirmed, that the vector spaces are properly aligned, we can create the document representations. \n",
    "As the pre-aligned german model does not contain subword-infomation, we'll use the subword-information contained in the english model to embedd unknown words in both languages.\n",
    "\n",
    "We'll use the preprocessed descriptions we created earlier.\n",
    "\n",
    "Additionaly, we'll print the words we created new wod vectors using the english subword information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Vector based on Subword Information for: mitttels\n",
      "Created Vector based on Subword Information for: texterkennungsmodellen\n",
      "Created Vector based on Subword Information for: kigestützte\n",
      "Created Vector based on Subword Information for: layoutanalyse\n",
      "Created Vector based on Subword Information for: strukturerkennung\n",
      "Created Vector based on Subword Information for: transkriptionseditor\n",
      "Created Vector based on Subword Information for: kigestützten\n",
      "Created Vector based on Subword Information for: kimodelle\n",
      "Created Vector based on Subword Information for: readsearch\n",
      "Created Vector based on Subword Information for: transkribusinhalte\n",
      "Created Vector based on Subword Information for: erkennungsmodelle\n",
      "Created Vector based on Subword Information for: gdpr\n",
      "Created Vector based on Subword Information for: texsystem\n",
      "Created Vector based on Subword Information for: 2e\n",
      "Created Vector based on Subword Information for: 1997\n",
      "Created Vector based on Subword Information for: opensourcelizenz\n",
      "Created Vector based on Subword Information for: eupl12\n",
      "Created Vector based on Subword Information for: pdfformat\n",
      "Created Vector based on Subword Information for: xmlpublikation\n",
      "Created Vector based on Subword Information for: id0features\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: teikonformen\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: teielementen\n",
      "Created Vector based on Subword Information for: xsltanweisungen\n",
      "Created Vector based on Subword Information for: teidaten\n",
      "Created Vector based on Subword Information for: publishing“ansatz\n",
      "Created Vector based on Subword Information for: pdfformat\n",
      "Created Vector based on Subword Information for: xslfo\n",
      "Created Vector based on Subword Information for: 8\n",
      "Created Vector based on Subword Information for: ausgabemodulen\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: codeansicht\n",
      "Created Vector based on Subword Information for: wordartigen\n",
      "Created Vector based on Subword Information for: »autorenansicht«\n",
      "Created Vector based on Subword Information for: sachanmerkungen\n",
      "Created Vector based on Subword Information for: teiauszeichnung\n",
      "Created Vector based on Subword Information for: teikonformen\n",
      "Created Vector based on Subword Information for: xmldatenbank\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: xmldokumente\n",
      "Created Vector based on Subword Information for: ediarumdb\n",
      "Created Vector based on Subword Information for: eingabewerkzeug\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: websiteentwicklung\n",
      "Created Vector based on Subword Information for: ediarumweb\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: insttitutionen\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: neugermanistische\n",
      "Created Vector based on Subword Information for: ediarum\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_fasttext_vector(row, aligned_vectors_de=None, aligned_vectors_eng=None):\n",
    "    \"\"\"\n",
    "    Calculates the FastText vector representation for a given row.\n",
    "    Parameters:\n",
    "    - row: A row of data.\n",
    "    - aligned_vectors_de: Aligned FastText vectors for the German language. Default is None.\n",
    "    - aligned_vectors_eng: Aligned FastText vectors for the English language. Default is None.\n",
    "    Note:\n",
    "    - If the language is not specified or not supported (only \"en\" and \"de\" are supported), it returns a zero vector.\n",
    "    - If a word in the row's description is not found in the aligned vectors, it tries to create a vector based on english subword information.\n",
    "    - If no vectors are found, it returns a zero vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # default size to avoid errors if vectors are None\n",
    "    vector_size = aligned_vectors_de.vector_size if aligned_vectors_de else 300\n",
    "    \n",
    "    # check if language is valid\n",
    "    lang = row.get(\"description_lang\")\n",
    "    if pd.isna(lang) or lang not in [\"en\", \"de\"]:\n",
    "        return np.zeros(vector_size) #Maybe rather use none?\n",
    "    \n",
    "    words = row.get(\"description_preprocessed\", \"\").split()\n",
    "    vectors = []\n",
    "\n",
    "    # process based on language\n",
    "    if lang == \"de\" and aligned_vectors_de:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_de[word])\n",
    "            except KeyError:\n",
    "                print(f\"Created Vector based on Subword Information for: {word}\")                \n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "                #vectors.append(np.zeros(vector_size))\n",
    "                \n",
    "    elif lang == \"en\" and aligned_vectors_eng:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "            except KeyError:\n",
    "                print(f\"Missing Vector for: {word}\")\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros\n",
    "\n",
    "\n",
    "# Apply the function to create aggregated vectors\n",
    "fasttext = edition_software_info.apply(lambda x: get_fasttext_vector(x, aligned_vectors_de, aligned_vectors_eng), axis=1)\n",
    "\n",
    "# Convert the Series of 1D arrays to a 2D numpy array (to calculate the cosine similarity later on)\n",
    "fasttext_array = np.array(word2vec.tolist())\n",
    "len(fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test the FastText Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LaTeX</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>0.099804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>0.090453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TEI Publisher</td>\n",
       "      <td>In vielen Editionsprojekten wird die Datenbanklösung [eXist-db]() eingesetzt. Die darin enthaltenen TEI-konformen Daten sollen i.d.R. zu einem bestimmten Zeitpunkt oder auch fortlaufend aufbereitet und in gedruckter oder digitaler Form ausgegeben werden. Für diese Transformation bietet der TEI Publisher einen komfortable und gleichzeitig mächtigen Weg. Der kann direkt über den eXist-db Marktplatz installiert werden. Um die gewünschte Ausgabe zu erhalten, wird durch Einstellungen mittels grafischer Benutzeroberfläche allen TEI-Elementen das gewünschte Aussehen zugeordnet. Es muss also nicht auf XSLT-Anweisungen zurückgegriffen werden. Die TEI-Daten können gemäß „Single Source Publishing“-Ansatz in verschiedene Ausgabeformate überführt werden. Neben dem klassischen PDF-Format bzw. ePUB und HTML können auch wichtige Zwischenstufen wie XSL-FO oder [LaTeX]() erzeugt werden. Seit Version 8 beherrscht der TEI Publisher außerdem die Ausgabe nach Print CSS.  Die so erstellen Dateien sind auf verschiedenen Geräten und Plattformen nutzbar und bringen für verschiedene Endgeräte jeweils passgenaue Funktionen mit.  Für die Entwicklung von Ausgabemodulen in Projekten, die auf eXist-db aufbauen, erlaubt der TEI Publisher eine schlanke und effiziente Umsetzung, die automatisch von den zahlreichen Verbesserungen aus der Community profitiert.</td>\n",
       "      <td>0.064059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ediarum</td>\n",
       "      <td>Benutzerfreundliches Arbeiten Als zentrale Softwarekomponente der Arbeitsumgebung wird Oxygen XML Author eingesetzt. Die Bearbeiter arbeiten im Oxygen XML Author nicht in einer Codeansicht, sondern in einer benutzerfreundlichen, Word-artigen »Autorenansicht«, die über Cascading Stylesheets (CSS) gestaltet wird. Dem Bearbeiter stehen dabei mehrere Ansichten zur Auswahl, so dass per Mausklick die für den Arbeitsschritt geeigneteste ausgewählt werden kann. Außerdem kann der Endanwender über eine eigene Werkzeugleiste per Knopfdruck Auszeichnungen vornehmen. So können z.B. in Manuskripten Streichungen markiert oder Sachanmerkungen eingegeben werden. Auch Personen- oder Ortsnamen können mit der entsprechenden TEI-Auszeichnung versehen und gleichzeitig über eine komfortable Auswahlliste mit dem jeweiligen Eintrag im zentralen Personen- bzw. Ortsregister verknüpft werden. Der gesamte Text kann dadurch einfach und schnell mit TEI-konformen XML ausgezeichnet werden.   Kollaboratives Arbeiten Die digitale Arbeitsumgebung nutzt die freie XML-Datenbank existdb als zentrales Repositorium für die XML-Dokumente. Die Datenbank ist auf einem Server installiert und online zugänglich. Dadurch können alle Projektmitarbeiter auf ein und denselben Datenbestand zugreifen und zusammenarbeiten. Um die Einrichtung und Konfiguration zu vereinfachen, wurde das Modul ediarum.DB entwickelt.    Website Neben dem eigentlichen Eingabewerkzeug in Oxygen XML Author, wird für die Forschungsvorhaben auch jeweils eine Website auf Basis von eXist, XQuery und XSLT erstellt. In ihr kann von den Wissenschaftlern der aktuelle Datenbestand leicht durchblättert bzw. durchsucht werden. Die Website kann je nach Bedarf nur den Bearbeitern zugänglich oder der gesamten Öffentlichkeit zugänglich gemacht werden. Beispiele für mit ediarum erstellte digitale Editionen finden Sie in der Rubrik Referenzen. Um den Prozess der Website-Entwicklung zu vereinfachen ist derzeit das Modul ediarum.WEB in Entwicklung.   Anpassung an projektspezifische Anforderungen Ziel von ediarum ist es, einen Kern an Funktionen bereitzustellen, der über verschiedene Projekte (und auch Insttitutionen hinweg) entwickelt und eingesetzt werden kann. Dadurch wird der Aufwand für alle Editionsvorhaben, die ediarum einsetzen, verringert. Der ganz überwiegende Teil der Funktionen kann auch für Editionen gleichen Typs (z.B. neugermanistische Editionen) tatsächlich projektübergreifend entwickelt werden. Allerdings gibt es i.d.R. immer einen wenn auch sehr überschaubaren Teil an Funktionen, die aufgrund des spezifischen Editions- oder Forschungskonzept, ergänzt werden müssen. Das ist in ediarum ohne Weiteres möglich. Dadurch kann zwischen notwendiger Standardisierung und ebenso notwendiger Orientierung an Forschungsfragen eine Brücke geschlagen werden.</td>\n",
       "      <td>0.056260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Classical Text Editor (CTE)</td>\n",
       "      <td>Der Classical Text Editor (CTE) wird auf Initiative der [Österreichischen Akademie der Wissenschaften](https://www.oeaw.ac.at/oesterreichische-akademie-der-wissenschaften) und des Editionsprojekts [Corpus Scriptorum Ecclesiasticorum Latinorum (CSEL)](http://csel.at/) seit 1997 entwickelt. Für die Nutzung des CTE muss eine [Lizenz erworben](https://sites.fastspring.com/stefanhagel/product/cte) werden, da das Projekt keine öffentliche Förderung erhält. Perspektivisch wird eine Veröffentlichung gemäß der Opensource-Lizenz [EUPL-1.2](https://opensource.org/licenses/EUPL-1.2) angestrebt. Der CTE schließt für den Bereich der kritischen Ausgaben die Lücke zwischen klassischen Text verarbeitungs programmen (Open Office Writer, Microsoft Word u.a.) auf der einen und Text satz programmen (TeX, TUSTEP Satz) auf der anderen Seite. Autor:innen können mit Hilfe einer graphischen Oberfläche Publikationen erstellen und gleichzeitig die gerade für kritische Ausgaben nötigen typographischen Anforderungen umsetzen. Neben umfangreichen Funktionen im Bereich kritischer Editionen (verschiedene Layouts, beliebig viele Apparate, Varianten etc.) gehören daher auch die Unterstützung von Unicode, komplexen Skripten sowie die Einbindung verschiedener Referenzsysteme zum Funktionsumfang des CTE. Das Ergebnis kann sowohl für die Drucklegung im PDF-Format als auch für eine elektronische Fassung als HTML- oder XML-Publikation exportiert werden. Umgekehrt können Texte in verschiedenen Formaten importiert werden.     Eine detaillierte Auflistung aller Feature des CTE findet man unter https://cte.oeaw.ac.at/?id0=features.</td>\n",
       "      <td>0.053857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    brand_name  \\\n",
       "3                        LaTeX   \n",
       "0                  Transkribus   \n",
       "8                TEI Publisher   \n",
       "9                      ediarum   \n",
       "7  Classical Text Editor (CTE)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   description_clean  \\\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In vielen Editionsprojekten wird die Datenbanklösung [eXist-db]() eingesetzt. Die darin enthaltenen TEI-konformen Daten sollen i.d.R. zu einem bestimmten Zeitpunkt oder auch fortlaufend aufbereitet und in gedruckter oder digitaler Form ausgegeben werden. Für diese Transformation bietet der TEI Publisher einen komfortable und gleichzeitig mächtigen Weg. Der kann direkt über den eXist-db Marktplatz installiert werden. Um die gewünschte Ausgabe zu erhalten, wird durch Einstellungen mittels grafischer Benutzeroberfläche allen TEI-Elementen das gewünschte Aussehen zugeordnet. Es muss also nicht auf XSLT-Anweisungen zurückgegriffen werden. Die TEI-Daten können gemäß „Single Source Publishing“-Ansatz in verschiedene Ausgabeformate überführt werden. Neben dem klassischen PDF-Format bzw. ePUB und HTML können auch wichtige Zwischenstufen wie XSL-FO oder [LaTeX]() erzeugt werden. Seit Version 8 beherrscht der TEI Publisher außerdem die Ausgabe nach Print CSS.  Die so erstellen Dateien sind auf verschiedenen Geräten und Plattformen nutzbar und bringen für verschiedene Endgeräte jeweils passgenaue Funktionen mit.  Für die Entwicklung von Ausgabemodulen in Projekten, die auf eXist-db aufbauen, erlaubt der TEI Publisher eine schlanke und effiziente Umsetzung, die automatisch von den zahlreichen Verbesserungen aus der Community profitiert.   \n",
       "9     Benutzerfreundliches Arbeiten Als zentrale Softwarekomponente der Arbeitsumgebung wird Oxygen XML Author eingesetzt. Die Bearbeiter arbeiten im Oxygen XML Author nicht in einer Codeansicht, sondern in einer benutzerfreundlichen, Word-artigen »Autorenansicht«, die über Cascading Stylesheets (CSS) gestaltet wird. Dem Bearbeiter stehen dabei mehrere Ansichten zur Auswahl, so dass per Mausklick die für den Arbeitsschritt geeigneteste ausgewählt werden kann. Außerdem kann der Endanwender über eine eigene Werkzeugleiste per Knopfdruck Auszeichnungen vornehmen. So können z.B. in Manuskripten Streichungen markiert oder Sachanmerkungen eingegeben werden. Auch Personen- oder Ortsnamen können mit der entsprechenden TEI-Auszeichnung versehen und gleichzeitig über eine komfortable Auswahlliste mit dem jeweiligen Eintrag im zentralen Personen- bzw. Ortsregister verknüpft werden. Der gesamte Text kann dadurch einfach und schnell mit TEI-konformen XML ausgezeichnet werden.   Kollaboratives Arbeiten Die digitale Arbeitsumgebung nutzt die freie XML-Datenbank existdb als zentrales Repositorium für die XML-Dokumente. Die Datenbank ist auf einem Server installiert und online zugänglich. Dadurch können alle Projektmitarbeiter auf ein und denselben Datenbestand zugreifen und zusammenarbeiten. Um die Einrichtung und Konfiguration zu vereinfachen, wurde das Modul ediarum.DB entwickelt.    Website Neben dem eigentlichen Eingabewerkzeug in Oxygen XML Author, wird für die Forschungsvorhaben auch jeweils eine Website auf Basis von eXist, XQuery und XSLT erstellt. In ihr kann von den Wissenschaftlern der aktuelle Datenbestand leicht durchblättert bzw. durchsucht werden. Die Website kann je nach Bedarf nur den Bearbeitern zugänglich oder der gesamten Öffentlichkeit zugänglich gemacht werden. Beispiele für mit ediarum erstellte digitale Editionen finden Sie in der Rubrik Referenzen. Um den Prozess der Website-Entwicklung zu vereinfachen ist derzeit das Modul ediarum.WEB in Entwicklung.   Anpassung an projektspezifische Anforderungen Ziel von ediarum ist es, einen Kern an Funktionen bereitzustellen, der über verschiedene Projekte (und auch Insttitutionen hinweg) entwickelt und eingesetzt werden kann. Dadurch wird der Aufwand für alle Editionsvorhaben, die ediarum einsetzen, verringert. Der ganz überwiegende Teil der Funktionen kann auch für Editionen gleichen Typs (z.B. neugermanistische Editionen) tatsächlich projektübergreifend entwickelt werden. Allerdings gibt es i.d.R. immer einen wenn auch sehr überschaubaren Teil an Funktionen, die aufgrund des spezifischen Editions- oder Forschungskonzept, ergänzt werden müssen. Das ist in ediarum ohne Weiteres möglich. Dadurch kann zwischen notwendiger Standardisierung und ebenso notwendiger Orientierung an Forschungsfragen eine Brücke geschlagen werden.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Der Classical Text Editor (CTE) wird auf Initiative der [Österreichischen Akademie der Wissenschaften](https://www.oeaw.ac.at/oesterreichische-akademie-der-wissenschaften) und des Editionsprojekts [Corpus Scriptorum Ecclesiasticorum Latinorum (CSEL)](http://csel.at/) seit 1997 entwickelt. Für die Nutzung des CTE muss eine [Lizenz erworben](https://sites.fastspring.com/stefanhagel/product/cte) werden, da das Projekt keine öffentliche Förderung erhält. Perspektivisch wird eine Veröffentlichung gemäß der Opensource-Lizenz [EUPL-1.2](https://opensource.org/licenses/EUPL-1.2) angestrebt. Der CTE schließt für den Bereich der kritischen Ausgaben die Lücke zwischen klassischen Text verarbeitungs programmen (Open Office Writer, Microsoft Word u.a.) auf der einen und Text satz programmen (TeX, TUSTEP Satz) auf der anderen Seite. Autor:innen können mit Hilfe einer graphischen Oberfläche Publikationen erstellen und gleichzeitig die gerade für kritische Ausgaben nötigen typographischen Anforderungen umsetzen. Neben umfangreichen Funktionen im Bereich kritischer Editionen (verschiedene Layouts, beliebig viele Apparate, Varianten etc.) gehören daher auch die Unterstützung von Unicode, komplexen Skripten sowie die Einbindung verschiedener Referenzsysteme zum Funktionsumfang des CTE. Das Ergebnis kann sowohl für die Drucklegung im PDF-Format als auch für eine elektronische Fassung als HTML- oder XML-Publikation exportiert werden. Umgekehrt können Texte in verschiedenen Formaten importiert werden.     Eine detaillierte Auflistung aller Feature des CTE findet man unter https://cte.oeaw.ac.at/?id0=features.   \n",
       "\n",
       "   similarity_score  \n",
       "3          0.099804  \n",
       "0          0.090453  \n",
       "8          0.064059  \n",
       "9          0.056260  \n",
       "7          0.053857  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I need to work with images in a browser'\n",
    "\n",
    "# check the queries language, to decide which model to use.\n",
    "query_lang = identify_language(query).replace(\"__label__\",\"\") if not query == '' else np.nan\n",
    "\n",
    "# preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "\n",
    "# create a series that can be used as an input in the \n",
    "query_information = pd.Series({\n",
    "    \"description_lang\": query_lang,\n",
    "    \"description_preprocessed\": query\n",
    "})\n",
    "\n",
    "# get vector representation of the query using fasttext\n",
    "query_fasttext= get_fasttext_vector(query_information, aligned_vectors_de, aligned_vectors_eng)\n",
    "\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_fasttext = query_fasttext.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_fasttext, fasttext_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"brand_name\",\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

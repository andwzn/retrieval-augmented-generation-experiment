{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from util.webscraper import WebScraper\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.2/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the data from the API. As the API is not yet published, both the API-Url and the query to get information on edition-software need to be specified in your .env file. (consult the README for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api_url and query\n",
    "api_url = os.environ['API_URL']\n",
    "query = os.environ['QUERY']\n",
    "\n",
    "# get data from api\n",
    "api_response = requests.get(api_url + query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got the data from the API, we can load it into a dataframe to prepare it to be used as a knowledge base for rag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31 entries, 0 to 30\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   id                31 non-null     object\n",
      " 1   slug              31 non-null     object\n",
      " 2   brand_name        31 non-null     object\n",
      " 3   concept_doi       0 non-null      object\n",
      " 4   description       28 non-null     object\n",
      " 5   description_url   3 non-null      object\n",
      " 6   description_type  31 non-null     object\n",
      " 7   get_started_url   30 non-null     object\n",
      " 8   image_id          23 non-null     object\n",
      " 9   is_published      31 non-null     bool  \n",
      " 10  short_statement   31 non-null     object\n",
      " 11  created_at        31 non-null     object\n",
      " 12  updated_at        31 non-null     object\n",
      " 13  closed_source     31 non-null     bool  \n",
      "dtypes: bool(2), object(12)\n",
      "memory usage: 3.1+ KB\n"
     ]
    }
   ],
   "source": [
    "edition_software_info = json.loads(api_response.text)\n",
    "edition_software_info = pd.DataFrame(edition_software_info)\n",
    "edition_software_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief inspection allows us to formulate some initial tasks and questions for this experiment.\n",
    "\n",
    "- **Preprocessing:** As we can see, not a single entry contains a associated concept_doi. We might consider dropping the column.\n",
    "- **Impact of using short descriptions only:** Three entries are missing the in depth description. We can assume that rag won't be too useful for these entries. \n",
    "- **Impact of additional information:** Only three have a description-url. Down the road, we need to evaluate, if adding info from this source improves the performance of the rag-system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove Artefacts\n",
    "\n",
    "Both the `description` and `short_statement` columns seem to be of particular interest for the task at hand. To asses necessary preprocessing step, we'll need to take a closer look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>short_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>The Research Software Directory is a content management system that is tailored to research software.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                             short_statement  \n",
       "0                                                                                                                          Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.  \n",
       "1                                  Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.  \n",
       "2          CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.  \n",
       "3  LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.  \n",
       "4                                                                                                                                                                                                      The Research Software Directory is a content management system that is tailored to research software.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = edition_software_info[[\"description\", \"short_statement\"]]\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(descriptions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `description` column contains some formatting artefacts like `\\n` and markdown syntax like `**` and `#`. Let's clean them up.\n",
    "While we're at it, we can also remove double whitespaces etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autodone</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LaTeX</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Software Directory</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    brand_name  \\\n",
       "0                  Transkribus   \n",
       "1                     Autodone   \n",
       "2                     CollateX   \n",
       "3                        LaTeX   \n",
       "4  Research Software Directory   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)   \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = '\\\\n+'\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "pattern = r'[*#]+|\\s-+\\s|]]' #\\[\\]()<>\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description_clean\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"brand_name\", \"description\", \"description_clean\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fill nan values\n",
    "\n",
    "Before we continue preprocessing the data for later vectorization, we need to check for missing values and replace them with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j42m3p7n5jl51rggmnpn_6940000gn/T/ipykernel_31398/3857300838.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide additional context-information for the retrieval process, we'll scrape all webpages referenced in the software-description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get urls\n",
    "\n",
    "First, we isolate the urls from our description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "      <td>https://autodone.idh.uni-koeln.de/usage,https://autodone.idh.uni-koeln.de/,https://autodone.idh.uni-koeln.de/about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Philology,http://en.wikipedia.org/wiki/Textual_criticism,http://en.wikipedia.org/wiki/Sequence_alignment,http://collatex.net/,http://en.wikipedia.org/wiki/Diff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)    \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                           urls  \n",
       "0                                                                                                                                                                                           NaN  \n",
       "1                                                                            https://autodone.idh.uni-koeln.de/usage,https://autodone.idh.uni-koeln.de/,https://autodone.idh.uni-koeln.de/about  \n",
       "2  http://en.wikipedia.org/wiki/Philology,http://en.wikipedia.org/wiki/Textual_criticism,http://en.wikipedia.org/wiki/Sequence_alignment,http://collatex.net/,http://en.wikipedia.org/wiki/Diff  \n",
       "3                                                                                                                                                                                           NaN  \n",
       "4                                                                                                                                                                                           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "\n",
    "urls = edition_software_info[\"description\"].str.extractall(pattern)\n",
    "urls = urls.droplevel(1)\n",
    "urls_grouped = urls.groupby(urls.index).agg((lambda x: ','.join(set(x))))\n",
    "edition_software_info[\"urls\"] = urls_grouped\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"description_clean\", \"urls\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape Webpages\n",
    "\n",
    "Now we scrape the paragraphs from the webpages we found. \n",
    "The webscraper will take the list of urls associated with an entry and will save paragraphs from all webpages as a string in a column of our dataframe. \n",
    "\n",
    "**This might take some time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://autodone.idh.uni-koeln.de/usage with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/ with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/about with parameters tags = ['p']\n",
      "Scraping http://collatex.net/ with parameters tags = ['p']\n",
      "Scraping http://www.tei-c.org/ with parameters tags = ['p']\n",
      "Scraping http://vbd.humnet.unipi.it/ with parameters tags = ['p']\n",
      "Request exception occurred for http://vbd.humnet.unipi.it/: HTTPConnectionPool(host='vbd.humnet.unipi.it', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x146ead640>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Scraping https://sites.fastspring.com/stefanhagel/product/cte with parameters tags = ['p']\n",
      "Scraping https://www.oeaw.ac.at/oesterreichische-akademie-der-wissenschaften with parameters tags = ['p']\n",
      "Scraping https://opensource.org/licenses/EUPL-1.2 with parameters tags = ['p']\n",
      "Scraping http://csel.at/ with parameters tags = ['p']\n",
      "Scraping https://cte.oeaw.ac.at/ with parameters tags = ['p']\n",
      "Scraping https://phylipweb.github.io/phylip/general.html with parameters tags = ['p']\n",
      "Scraping https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html with parameters tags = ['p']\n",
      "HTTP error occurred for https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html: 404 Client Error: Not Found for url: https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html\n",
      "Scraping http://publex.uni-trier.de/dictionary-network with parameters tags = ['p']\n",
      "HTTP error occurred for http://publex.uni-trier.de/dictionary-network: 503 Server Error: Service Unavailable for url: http://publex.uni-trier.de/dictionary-network\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/publex with parameters tags = ['p']\n",
      "Scraping https://bitbucket.org/tcdh/publex-allentities/src/master/ with parameters tags = ['p']\n",
      "Scraping https://campus.dariah.eu/resource/posts/how-to-publish-your-dictionary-data-with-publex with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/komplett with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/transcribo with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514 with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud with parameters tags = ['p']\n",
      "Scraping https://fud.uni-trier.de/community/referenzen/ with parameters tags = ['p']\n",
      "Scraping https://textgridrep.de/ with parameters tags = ['p']\n",
      "Scraping https://ride.i-d-e.de/issues/issue-11/omeka/ with parameters tags = ['p']\n",
      "Scraping https://viscoll.org with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/download.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/check.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/witnesses.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/image.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/stats.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/print.php with parameters tags = ['p']\n"
     ]
    }
   ],
   "source": [
    "webscraper = WebScraper(tags = [\"p\"], exclude = [\"wikipedia\"])\n",
    "edition_software_info[\"webpages_text\"] = edition_software_info[\"urls\"].apply(lambda x: webscraper.scrape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>webpages_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://autodone.idh.uni-koeln.de/usage,https:...</td>\n",
       "      <td>On this page you will find instructions on how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://en.wikipedia.org/wiki/Philology,http://...</td>\n",
       "      <td>“In a language, in the system of language, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                urls  \\\n",
       "0                                                NaN   \n",
       "1  https://autodone.idh.uni-koeln.de/usage,https:...   \n",
       "2  http://en.wikipedia.org/wiki/Philology,http://...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                       webpages_text  \n",
       "0                                                NaN  \n",
       "1  On this page you will find instructions on how...  \n",
       "2  “In a language, in the system of language, the...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info[[\"urls\", \"webpages_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is collected from the webpages, we can take a look at the average length of the texts received for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       13.000000\n",
       "mean     14854.923077\n",
       "std      17207.648297\n",
       "min        640.000000\n",
       "25%       1919.000000\n",
       "50%       6739.000000\n",
       "75%      23517.000000\n",
       "max      55063.000000\n",
       "Name: webpages_text, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = edition_software_info[\"webpages_text\"].apply(lambda x: len(x) if not pd.isna(x) else 0)\n",
    "length[length>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking only at entries, that we were able to collected webpage text for, we have an average character count of about 15.000 per entry. \n",
    "The standard deviation is quite large compared to the mean, indicating that there is a high degree of variability in character counts.\n",
    "\n",
    "The distribution is skewed towards entries with lower character counts, while some outliers with a high character counts pull the mean upwards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, 'data/edition_software_info.csv')\n",
    "edition_software_info.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can preprocess the newfound text using the function we defined earlier. Again, we have to replace missing values with empty strings.\n",
    "\n",
    "First, let us reimport the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, \"data/edition_software_info.csv\")\n",
    "edition_software_info = pd.read_csv(path)\n",
    "edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]] = edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove links\n",
    "\n",
    "First, we'll remove all links from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Erkennen, Transkribieren und Durchsuchen von...\n",
       "1    autodone is a service for the automated, time-...\n",
       "2    [CollateX]() is a software to  1. read  multip...\n",
       "Name: description_preprocessed, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_clean\"].str.replace(pattern, '', regex=True)\n",
    "edition_software_info[\"description_preprocessed\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove Stopwords and Punctuation\n",
    "\n",
    "For the later vectorisation of the texts, we remove both german and english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(stopwords: List[str], text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    \n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    erkennen transkribieren durchsuchen historisch...\n",
       "1    autodone service automated timecontrolled publ...\n",
       "2    collatex software 1 read multiple ≥ 2 versions...\n",
       "3    mathematiker donald e knuth entwickelte ende s...\n",
       "4                                                     \n",
       "Name: description_preprocessed, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get stopwords\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "stopwords_combined = stopwords_german.union(stopwords_english)\n",
    "\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: preprocess(stopwords_combined, x))\n",
    "edition_software_info[\"description_preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     \n",
       "1    page find instructions use autodone based scre...\n",
       "2    “in language system language differences”– jac...\n",
       "3                                                     \n",
       "4                                                     \n",
       "Name: webpages_text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info[\"webpages_text_preprocessed\"] = edition_software_info[\"webpages_text\"].apply(lambda x: preprocess(stopwords_combined, x) if not pd.isna(x) else \"\")\n",
    "edition_software_info[\"webpages_text_preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 1: TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with a simple tf-idf vectorisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Fit TF-IDF Vectorizer\n",
    "First, we fit the vectorizer on the preprocessed descriptions. \n",
    "This way, the vectorizer can transform text into numerical feature vectors based on the learned vocabulary and its distribution over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1514</th>\n",
       "      <th>1514 digital</th>\n",
       "      <th>19042024</th>\n",
       "      <th>19042024 official</th>\n",
       "      <th>1905</th>\n",
       "      <th>1905 1931</th>\n",
       "      <th>1931</th>\n",
       "      <th>1931 used</th>\n",
       "      <th>1986</th>\n",
       "      <th>1986 authors</th>\n",
       "      <th>...</th>\n",
       "      <th>österreichischen</th>\n",
       "      <th>österreichischen akademie</th>\n",
       "      <th>úfal</th>\n",
       "      <th>úfal institute</th>\n",
       "      <th>überführt</th>\n",
       "      <th>überführt neben</th>\n",
       "      <th>überschaubaren</th>\n",
       "      <th>überschaubaren teil</th>\n",
       "      <th>überwiegende</th>\n",
       "      <th>überwiegende teil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089367</td>\n",
       "      <td>0.089367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5493 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1514  1514 digital  19042024  19042024 official  1905  1905 1931  1931  \\\n",
       "0   0.0           0.0  0.000000           0.000000   0.0        0.0   0.0   \n",
       "1   0.0           0.0  0.089367           0.089367   0.0        0.0   0.0   \n",
       "2   0.0           0.0  0.000000           0.000000   0.0        0.0   0.0   \n",
       "3   0.0           0.0  0.000000           0.000000   0.0        0.0   0.0   \n",
       "4   0.0           0.0  0.000000           0.000000   0.0        0.0   0.0   \n",
       "\n",
       "   1931 used  1986  1986 authors  ...  österreichischen  \\\n",
       "0        0.0   0.0           0.0  ...               0.0   \n",
       "1        0.0   0.0           0.0  ...               0.0   \n",
       "2        0.0   0.0           0.0  ...               0.0   \n",
       "3        0.0   0.0           0.0  ...               0.0   \n",
       "4        0.0   0.0           0.0  ...               0.0   \n",
       "\n",
       "   österreichischen akademie  úfal  úfal institute  überführt  \\\n",
       "0                        0.0   0.0             0.0        0.0   \n",
       "1                        0.0   0.0             0.0        0.0   \n",
       "2                        0.0   0.0             0.0        0.0   \n",
       "3                        0.0   0.0             0.0        0.0   \n",
       "4                        0.0   0.0             0.0        0.0   \n",
       "\n",
       "   überführt neben  überschaubaren  überschaubaren teil  überwiegende  \\\n",
       "0              0.0             0.0                  0.0           0.0   \n",
       "1              0.0             0.0                  0.0           0.0   \n",
       "2              0.0             0.0                  0.0           0.0   \n",
       "3              0.0             0.0                  0.0           0.0   \n",
       "4              0.0             0.0                  0.0           0.0   \n",
       "\n",
       "   überwiegende teil  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  \n",
       "\n",
       "[5 rows x 5493 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(edition_software_info[\"description_preprocessed\"])\n",
    "\n",
    "# display the resulting matrix\n",
    "tfidf_matrix_beautify = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_matrix_beautify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Inspect the tf-idf representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in this dataframe is a unique word, while each row is a document. The cells denote the number of occurances of a word in a document, weighted by the words potential to be distinctive.\n",
    "\n",
    "Let's take a look at the tf-idf filtered words for each description (You can find them in the column \"tf_idf_filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>tf_idf_filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>[(dokumenten, 0.33021410932225603)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "      <td>[(twitter, 0.26810009116835964)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>[(differences, 0.2150540011707874)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>[(jahre, 0.26585686887909327)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)    \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                 tf_idf_filtered_words  \n",
       "0  [(dokumenten, 0.33021410932225603)]  \n",
       "1     [(twitter, 0.26810009116835964)]  \n",
       "2  [(differences, 0.2150540011707874)]  \n",
       "3       [(jahre, 0.26585686887909327)]  \n",
       "4                                   []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the threshold for TF-IDF scores\n",
    "threshold = 0.2\n",
    "\n",
    "# Filter words with TF-IDF scores greater than the threshold for each document\n",
    "def filter_words_by_threshold(row, threshold):\n",
    "    filtered_words = [(word, score) for word, score in zip(tfidf_matrix_beautify.columns, row) if score > threshold]\n",
    "    return sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Apply the function to each row of the TF-IDF DataFrame\n",
    "filtered_words = tfidf_matrix_beautify.apply(lambda row: filter_words_by_threshold(row, threshold), axis=1)\n",
    "\n",
    "# Create a dataframe to display the filtered words\n",
    "filtered_words_df = pd.DataFrame(filtered_words, columns=[\"tf_idf_filtered_words\"])\n",
    "tfidf_display = pd.concat([edition_software_info[\"description_clean\"], filtered_words_df], axis=1)\n",
    "    \n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(tfidf_display[[\"description_clean\", \"tf_idf_filtered_words\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test tfidf-representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)</td>\n",
       "      <td>0.145407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Omeka Classic is a web publishing platform designed for sharing digital collections and creating media-rich online exhibits. It has been adapted for scholarly digital editions, integrating tools for various steps of the edition process, such as transcription, encoding, and publishing. These functionalities are facilitated by extensions (or plugins), maintained either by the Omeka team (official plugins) or by its community (unofficial plugins). A detailed review of Omeka for creating digital edition can be found [here](https://ride.i-d-e.de/issues/issue-11/omeka/).   Plugins to work with TEI  MLA TEI TEI Annotations TEI Display TEI Editions Transcript.12    Plugins for transcription Scripto   Plugins for publishing digital editions TEI Display  TEI Edition</td>\n",
       "      <td>0.068280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation, initially developed at the Centro de Linguística da Universidade de Lisboa, later at CELGA-ILTEC, and currently maintained at the ÚFAL institute of Charles University, Prague. The system has a modular design with numerous modules making serving a wide range of different corpus types.  Below are some examples of some of those, and the type of corpora TEITOK can deal with. More modules are added frequently, and it is possible to add custom modules as well.  Historical Corpora  For historical corpora, TEITOK provides the option to have an alignment between the transcription and the facsimile image, it provides the option to work with multiple orthographic realizations to combine several editions of a text into a single XML file, and it provides the option to create a searchable document map to see where in the world several phenomena are more frequent.  TEITOK is freely available for anybody who wishes to create richly annotated textual corpora, and runs on any LINUX based web server.    Features    Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment   Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus   Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments   Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations   Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map   Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression   Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the original project.</td>\n",
       "      <td>0.053668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             description_clean  \\\n",
       "22  [Transcribo](https://tcdh.uni-trier.de/en/projekt/transcribo) is an editing tool developed by the Trier Center for Digital Humanities as part of the project “Arthur Schnitzler: Digital Historical-Critical Edition”. The digital tool can support users in transcribing texts in various fields. In addition to the actual transcription, the texts can also be annotated and text-genetic facts can be marked up. Transcribo offers the possibility to transcribe texts productively and in a time-saving way, using the same intuitive approach as you are used to from manual transcription. The tool offers all the subtleties needed for a differentiated transcription and supports the working process both efficiently and easily comprehensible.   Transcribe almost like with sheet and pencil Transcribing is done in a graphical editor directly on the facsimile, simulating the procedure of analog transcribing with sheet and pencil. This intuitive approach is an advantage especially when transcribing difficult to decipher manuscripts or heavily revised typescript pages and additionally reduces the learning time for the tool, since no markup language (e.g. TEI/XML) has to be learned. Another advantage of this method is that by working on the facsimile, a positional link between the transcribed text and the image is automatically stored. This positional data can be used in the context of the electronic presentation to visualize the link between text and facsimile for the user in a sustainable way, e.g. in the form of cross-fades or mouse-over effects.   Can be used both locally and with FuD  Transcribo can be used both locally and in conjunction with our FuD system.  In local operation, facsimiles located on the terminal device can be opened and edited. All processed information is stored locally in the form of XML files. When FuD is used in addition, the data is stored in FuD's own relational database, which enables collaborative work. In addition, some functionalities, such as the recording of cross-page phenomena, the correction tool or the storage of text states, are only available in this operating mode. The many years of development work accompanying the project have paid off.  Thus, adaptations and ideas from numerous humanities projects at TCDH have been incorporated into the work and have resulted in a very differentiated and practically tested tool.    Some of the most important features of Transcribo 1.  Fine-grained markup of microgenetic facts  (at document, page, sentence/line, word, and graph levels) 2.  Markup of complex interrelated phenomena/change operations  via relations (page and cross-page) 3.  Correction function  for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4.  OCR functionality  for automatic reading of typescripts or prints (Tesseract) 5.  Navigation perspective:  interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6.  Structural perspective:  overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7.  Module for defining and labeling text states or text layers.    Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems.   Projects using Transcribo [Arthur Schnitzler Digital](https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital) [The Augsburg Master Builder’s Ledgers](https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers) [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)](https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514) [Digital Marburg Büchner Edition](https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition) [Johann Caspar Lavater](https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater) [Kurt Schwitters' Intermedia Networks of the Avant-garde](https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde) [Stefan Heym: “Ahasver”](https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver) [Digitalization of the Plock Bible, Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible) [Old High German Dictionary](https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary)    \n",
       "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Omeka Classic is a web publishing platform designed for sharing digital collections and creating media-rich online exhibits. It has been adapted for scholarly digital editions, integrating tools for various steps of the edition process, such as transcription, encoding, and publishing. These functionalities are facilitated by extensions (or plugins), maintained either by the Omeka team (official plugins) or by its community (unofficial plugins). A detailed review of Omeka for creating digital edition can be found [here](https://ride.i-d-e.de/issues/issue-11/omeka/).   Plugins to work with TEI  MLA TEI TEI Annotations TEI Display TEI Editions Transcript.12    Plugins for transcription Scripto   Plugins for publishing digital editions TEI Display  TEI Edition     \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation, initially developed at the Centro de Linguística da Universidade de Lisboa, later at CELGA-ILTEC, and currently maintained at the ÚFAL institute of Charles University, Prague. The system has a modular design with numerous modules making serving a wide range of different corpus types.  Below are some examples of some of those, and the type of corpora TEITOK can deal with. More modules are added frequently, and it is possible to add custom modules as well.  Historical Corpora  For historical corpora, TEITOK provides the option to have an alignment between the transcription and the facsimile image, it provides the option to work with multiple orthographic realizations to combine several editions of a text into a single XML file, and it provides the option to create a searchable document map to see where in the world several phenomena are more frequent.  TEITOK is freely available for anybody who wishes to create richly annotated textual corpora, and runs on any LINUX based web server.    Features    Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment   Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus   Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments   Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations   Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map   Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression   Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the original project.      \n",
       "\n",
       "    similarity_score  \n",
       "22          0.145407  \n",
       "27          0.068280  \n",
       "29          0.053668  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = 'I am looking for a transcription tool'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# Transform the query to TF-IDF space\n",
    "query_tfidf = tfidf_vectorizer.transform([query]) \n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities[0]\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['description_clean', 'similarity_score']].head(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

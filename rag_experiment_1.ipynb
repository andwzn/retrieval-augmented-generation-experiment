{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and create frequently used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from util.webscraper import WebScraper\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to load the FastText models\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"File was already downloaded.\")\n",
    "            return None\n",
    "        \n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"The file has been downloaded and saved as: {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred while downloading the file: {e}\")\n",
    "        \n",
    "def load_word_vectors(file_path: str):\n",
    "    \"\"\"Load word vectors from a file.\"\"\"\n",
    "    try:\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        print(\"Vectors loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the vectors: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.2/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the data from the API. As the API is not yet published, both the API-Url and the query to get information on edition-software need to be specified in your .env file. (consult the README for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api_url and query\n",
    "api_url = os.environ['API_URL']\n",
    "query = os.environ['QUERY']\n",
    "\n",
    "# get data from api\n",
    "api_response = requests.get(api_url + query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got the data from the API, we can load it into a dataframe to prepare it to be used as a knowledge base for rag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_software_info = json.loads(api_response.text)\n",
    "edition_software_info = pd.DataFrame(edition_software_info)\n",
    "edition_software_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief inspection allows us to formulate some initial tasks and questions for this experiment.\n",
    "\n",
    "- **Preprocessing:** As we can see, not a single entry contains a associated concept_doi. We might consider dropping the column.\n",
    "- **Impact of using short descriptions only:** Three entries are missing the in depth description. We can assume that rag won't be too useful for these entries. \n",
    "- **Impact of additional information:** Only three have a description-url. Down the road, we need to evaluate, if adding info from this source improves the performance of the rag-system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove Artefacts\n",
    "\n",
    "Both the `description` and `short_statement` columns seem to be of particular interest for the task at hand. To asses necessary preprocessing step, we'll need to take a closer look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = edition_software_info[[\"description\", \"short_statement\"]]\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(descriptions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `description` column contains some formatting artefacts like `\\n` and markdown syntax like `**` and `#`. Let's clean them up.\n",
    "While we're at it, we can also remove double whitespaces etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '\\\\n+'\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "pattern = r'[*#]+|\\s-+\\s|]]' #\\[\\]()<>\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description_clean\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"brand_name\", \"description\", \"description_clean\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fill nan values\n",
    "\n",
    "Before we continue preprocessing the data for later vectorization, we need to check for missing values and replace them with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide additional context-information for the retrieval process, we'll scrape all webpages referenced in the software-description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get urls\n",
    "\n",
    "First, we isolate the urls from our description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "\n",
    "urls = edition_software_info[\"description\"].str.extractall(pattern)\n",
    "urls = urls.droplevel(1)\n",
    "urls_grouped = urls.groupby(urls.index).agg((lambda x: ','.join(set(x))))\n",
    "edition_software_info[\"urls\"] = urls_grouped\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"description_clean\", \"urls\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape Webpages\n",
    "\n",
    "Now we scrape the paragraphs from the webpages we found. \n",
    "The webscraper will take the list of urls associated with an entry and will save paragraphs from all webpages as a string in a column of our dataframe. \n",
    "\n",
    "**This might take some time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscraper = WebScraper(tags = [\"p\"], exclude = [\"wikipedia\"])\n",
    "edition_software_info[\"webpages_text\"] = edition_software_info[\"urls\"].apply(lambda x: webscraper.scrape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_software_info[[\"urls\", \"webpages_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is collected from the webpages, we can take a look at the average length of the texts received for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = edition_software_info[\"webpages_text\"].apply(lambda x: len(x) if not pd.isna(x) else 0)\n",
    "length[length>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking only at entries, that we were able to collected webpage text for, we have an average character count of about 15.000 per entry. \n",
    "The standard deviation is quite large compared to the mean, indicating that there is a high degree of variability in character counts.\n",
    "\n",
    "The distribution is skewed towards entries with lower character counts, while some outliers with a high character counts pull the mean upwards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, 'data/edition_software_info.csv')\n",
    "edition_software_info.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can preprocess the newfound text using the function we defined earlier. Again, we have to replace missing values with empty strings.\n",
    "\n",
    "First, let us reimport the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, \"data/edition_software_info.csv\")\n",
    "edition_software_info = pd.read_csv(path)\n",
    "edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]] = edition_software_info[[\"description\", \"description_clean\", \"webpages_text\", \"urls\"]].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove links\n",
    "\n",
    "First, we'll remove all links from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_clean\"].str.replace(pattern, '', regex=True)\n",
    "edition_software_info[\"description_preprocessed\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove Stopwords and Punctuation\n",
    "\n",
    "For the later vectorisation of the texts, we remove both german and english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(stopwords: List[str], text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    \n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stopwords\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "stopwords_combined = stopwords_german.union(stopwords_english)\n",
    "\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: preprocess(stopwords_combined, x))\n",
    "edition_software_info[\"description_preprocessed\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_software_info[\"webpages_text_preprocessed\"] = edition_software_info[\"webpages_text\"].apply(lambda x: preprocess(stopwords_combined, x) if not pd.isna(x) else \"\")\n",
    "edition_software_info[\"webpages_text_preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lemmatize\n",
    "\n",
    "Finally, we can lemmatize our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def lemmatize_english_text(text):\n",
    "    doc = nlp_en(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def lemmatize_german_text(text):\n",
    "    doc = nlp_de(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_english_text(x))\n",
    "edition_software_info[\"description_preprocessed\"] = edition_software_info[\"description_preprocessed\"].apply(lambda x: lemmatize_german_text(x))\n",
    "edition_software_info[\"description_preprocessed\"].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add Language Information\n",
    "\n",
    "As the entries in our repository are in both english and german, we add information on the texts language to the dataset.\n",
    "\n",
    "To do so, we'll use a fasttext-model for language identification, which can be found [here](https://fasttext.cc/docs/en/language-identification.html).\n",
    "\n",
    "As the model was trained on UTF-8 data, it expects UTF-8 as input. This sould be the case, as pandas `read_csv`-function imports text in UTF-8 by default.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the fasttext model\n",
    "\n",
    "fasttext_path = os.path.join(os.getcwd(), 'models/lid.176.bin')\n",
    "\n",
    "if os.path.isfile(fasttext_path):\n",
    "    print(\"Model found.\")\n",
    "    language_detection_model = fasttext.load_model(fasttext_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_path)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    language_detection_model = fasttext.load_model(fasttext_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model to identify the languages of our entries. This might take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fasttext_model = fasttext.load_model(fasttext_path)\n",
    "\n",
    "def identify_language(text):\n",
    "    lang_detected = fasttext_model.predict(text)\n",
    "    return lang_detected[0][0]\n",
    "\n",
    "# clean the webpage text, as the model expect text without newlines\n",
    "edition_software_info.loc[:,\"description\"] = edition_software_info[\"description\"].str.replace(\"\\n\",\" \")\n",
    "\n",
    "# detect message-  and webpage-languages. If the column contains empty text, the language is set to nan\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description\"].apply(lambda x: identify_language(x) if not x == '' else np.nan)\n",
    "\n",
    "# clean the output\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description_lang\"].str.replace(\"__label__\",\"\")\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description_lang\"].str.replace(\"__label__\",\"\")\n",
    "\n",
    "# print the new columns\n",
    "edition_software_info[[\"description\",\"description_lang\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 1: TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with a simple TF-IDF vectorization.\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)** is a weighting scheme that weights the cells of a term-document matrix by their potential to be discriminatory.\n",
    "\n",
    "To do so, we first calculate the **term frequency (TF)**. The term frequency represents the number of instances of a given word $t$ in a document $d$.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Count of } t \\text{ in } d}{\\text{Total number of words in } d}\n",
    "$$\n",
    "\n",
    "This term frequency is then multiplied by the **inverse document frequency (IDF)**. The IDF is calculated by counting all documents that contain a term $t$ (the document frequency $\\text{df}(t)$). Then, we divide the total number of documents $N$ in the corpus by $\\text{df}(t)$.\n",
    "\n",
    "This inverse frequency is chosen over the regular frequency to **downweight** terms that appear in many documents, since these terms are less likely to be useful for distinguishing between documents.\n",
    "\n",
    "Usually, we also take the logarithm of the IDF to smooth out the very large values that can occur when a term appears in only a few documents. This ensures that rare terms are not excessively weighted.\n",
    "\n",
    "$$\n",
    "\\text{df}(t) = \\text{Document frequency of a term } t\n",
    "$$\n",
    "$$\n",
    "N = \\text{Number of documents}\n",
    "$$\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{\\text{df}(t)}\\right)\n",
    "$$\n",
    "\n",
    "Finally, we calculate the **TF-IDF** by multiplying the term frequency $\\text{TF}(t, d)$ with the inverse document frequency $\\text{IDF}(t)$.\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "The resulting value can be interpreted as a measure of the importance of the term in a document relative to the entire corpus. Terms that are frequent in a document but rare across the corpus will have higher TF-IDF scores, indicating their importance.\n",
    "\n",
    "\n",
    "**N-grams:**\n",
    "\n",
    "To capture not just the importance of single words but also some of the **context** in which they are used, we can apply TF-IDF to **n-grams**. N-grams are contiguous sequences of $n$ words that appear together in a text. The size of the sequence, $n$, is a hyperparameter that can be adjusted depending on the specific task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Fit TF-IDF Vectorizer\n",
    "First, we fit the vectorizer on the preprocessed descriptions. \n",
    "This way, the vectorizer can transform text into numerical feature vectors based on the learned vocabulary and its distribution over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(edition_software_info[\"description_preprocessed\"])\n",
    "\n",
    "# display the resulting matrix\n",
    "tfidf_matrix_beautify = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_matrix_beautify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Inspect the tf-idf representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in this dataframe is a unique word, while each row is a document. The cells denote the number of occurances of a word in a document, weighted by the words potential to be distinctive.\n",
    "\n",
    "Let's take a look at the tf-idf filtered words for each description (You can find them in the column \"tf_idf_filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold for TF-IDF scores\n",
    "threshold = 0.1\n",
    "\n",
    "# Filter words with TF-IDF scores greater than the threshold for each document\n",
    "def filter_words_by_threshold(row, threshold):\n",
    "    filtered_words = [(word, score) for word, score in zip(tfidf_matrix_beautify.columns, row) if score > threshold]\n",
    "    return sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Apply the function to each row of the TF-IDF DataFrame\n",
    "filtered_words = tfidf_matrix_beautify.apply(lambda row: filter_words_by_threshold(row, threshold), axis=1)\n",
    "\n",
    "# Create a dataframe to display the filtered words\n",
    "filtered_words_df = pd.DataFrame(filtered_words, columns=[\"tf_idf_filtered_words\"])\n",
    "tfidf_display = pd.concat([edition_software_info[\"description_clean\"], filtered_words_df], axis=1)\n",
    "    \n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(tfidf_display[[\"description_clean\", \"tf_idf_filtered_words\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test tfidf-representation\n",
    "\n",
    "This cell will return the most relevant documents from our dataset based on a comparison of their tf-idf representations and a query. The query can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I am looking for a transcription tool'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# Transform the query to TF-IDF space\n",
    "query_tfidf = tfidf_vectorizer.transform([query]) \n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities[0]\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['description_clean', 'similarity_score']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 2: Aggregated Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create document representations by aggregating the word2vec embeddings of each word in a description. \n",
    "\n",
    "Word2vec encodes the meaning of the words by capturing their semantic relationships based on the context in which they appear. By aggregating the word2vec embeddings of each word in a description, we can create a document representation that retains the semantic information and provides a more nuanced understanding of the content.\n",
    "\n",
    "From a computational perspective, these representations are shorter and denser than tf-idf representations, making them more suitable for computations such as similarity measures, clustering, or classification tasks. The dense nature of word2vec embeddings allows for efficient storage and faster processing compared to sparse representations like tf-idf. Additionally, because word2vec captures the meaning and context of words, it can provide more meaningful insights into the relationships between different documents or terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "path = os.path.join(current_path, \"models/word2vec-google-news-300.bin\")\n",
    "\n",
    "# Load the model if it is already in our project. If not, download it.\n",
    "if os.path.isfile(path):\n",
    "    print(\"Model found. Loading...\")\n",
    "    word2vec_model = KeyedVectors.load(path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    word2vec_model.save(path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #TODO: Add some preprocessing\n",
    "def preprocess_word2vec(text: str) -> str:\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the document representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_vector(words, model):\n",
    "    words = words.split()\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in model]\n",
    "    \n",
    "    if not valid_words:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the vectors of the valid words to create a document representation\n",
    "    vectors = [model[word] for word in valid_words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply the function to create aggregated vectors\n",
    "word2vec = edition_software_info['description_preprocessed'].apply(lambda x: get_word2vec_vector(x, word2vec_model))\n",
    "\n",
    "# Convert the Series of 1D arrays to a 2D numpy array (to calculate the cosine similarity later on)\n",
    "word2vec_array = np.array(word2vec.tolist())\n",
    "len(word2vec_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test Word2Vec Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I am working with images'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# get vector representation of the query using word2vec\n",
    "query_word2vec = get_word2vec_vector(query, word2vec_model)\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_word2vec = query_word2vec.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_word2vec, word2vec_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info[[\"brand_name\",\"description_clean\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 3: FastText\n",
    "\n",
    "One downside of pretrained Word2Vec representations is their inability to handle words not contained in their vocabulary. \n",
    "\n",
    "FastText overcomes this issue by representing words not only as embeddings but also as collections of embedded character n-grams. This approach allows FastText to generate meaningful word vectors for previously unseen words, which is particularly useful when dealing with highly specialized terminologies. In our dataset, which contains such specialized language, FastText may therefore offer more reliable performance compared to traditional Word2Vec models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the models\n",
    "\n",
    "FastText provides pre-aligned word vectors, meaning that word vectors for different languages (like German and English) have already been mapped into a common vector space. This allows words with similar meanings across different languages to have similar vector representations, which is crucial when working with multilingual datasets.\n",
    "\n",
    "Since our dataset contains both German and English texts, we need to download the pre-aligned FastText models for these two languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_file_path: str, extract_to: str) -> None:\n",
    "    \"\"\"Unzip a file to a target directory.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Unzipped {zip_file_path} to {extract_to}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error while unzipping the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Check if the english model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future. \n",
    "# This might take a while\n",
    "\n",
    "current_path = os.getcwd()\n",
    "models_dir = os.path.join(current_path, \"models\")\n",
    "fasttext_eng_zip_path = os.path.join(models_dir, \"wiki.en.zip\")\n",
    "fasttext_eng_path_vec = os.path.join(models_dir, \"wiki.en.vec\")\n",
    "fasttext_eng_path_bin = os.path.join(models_dir, \"wiki.en.bin\")\n",
    "\n",
    "if os.path.isfile(fasttext_eng_path_bin):\n",
    "    print(\"Model found.\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model(fasttext_eng_path_bin) #load the full model, including subword information.\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\" #IDEA: Download the bin model, as it contains subword info. Use this subwordinfo to handle all unknown words.\n",
    "    # download the models\n",
    "    download_file(url, fasttext_eng_zip_path)\n",
    "    \n",
    "    print(\"Unzipping the file...\")\n",
    "    unzip_file(fasttext_eng_zip_path, models_dir)    \n",
    "\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model.load(fasttext_eng_path_bin)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_eng.save(fasttext_eng_path_bin)\n",
    "\n",
    "    \n",
    "if aligned_vectors_eng is None:\n",
    "    raise ValueError(\"The FastText model or vectors were not loaded properly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the german model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future.\n",
    "# This might take a while\n",
    "\n",
    "fasttext_de_path_bin = os.path.join(current_path, \"models/wiki_de_align.bin\")\n",
    "fasttext_de_path_vec = os.path.join(current_path, \"models/wiki_de_align.vec\")\n",
    "\n",
    "if os.path.isfile(fasttext_de_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_de = KeyedVectors.load(fasttext_de_path_bin)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_de_path_vec)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_de = load_word_vectors(fasttext_de_path_vec)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_de.save(fasttext_de_path_bin)\n",
    "    \n",
    "if aligned_vectors_de is None:\n",
    "    raise ValueError(\"The FastText model or vectors were not loaded properly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we loaded both models, let's check if they are properly aligned. \n",
    "\n",
    "To do so, we'll pick an english word, get its vector representation and return the most similar word vector in the german representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector-representation of a random english word\n",
    "word_english = 'skyscraper'\n",
    "word_vector_in_english = aligned_vectors_eng.wv[word_english]\n",
    "\n",
    "print(f\"German word vector closest to {word_english}:\", aligned_vectors_de.most_similar(positive=[word_vector_in_english]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_vector(row, aligned_vectors_de=None, aligned_vectors_eng=None):\n",
    "    \n",
    "    # default size to avoid errors if vectors are None\n",
    "    vector_size = aligned_vectors_de.vector_size if aligned_vectors_de else 300\n",
    "    \n",
    "    # check if language is valid\n",
    "    lang = row.get(\"description_lang\")\n",
    "    if pd.isna(lang) or lang not in [\"en\", \"de\"]:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    words = row.get(\"description\", \"\").split()\n",
    "    vectors = []\n",
    "\n",
    "    # process based on language\n",
    "    if lang == \"de\" and aligned_vectors_de:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_de[word])\n",
    "            except KeyError:\n",
    "                print(f\"Created Vector based on Subword Information for: {word}\")                \n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "                #vectors.append(np.zeros(vector_size))\n",
    "                \n",
    "    elif lang == \"en\" and aligned_vectors_eng:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "            except KeyError:\n",
    "                print(f\"Missing Vector for: {word}\")\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros\n",
    "\n",
    "\n",
    "# Apply the function to create aggregated vectors\n",
    "fasttext = edition_software_info.apply(lambda x: get_fasttext_vector(x, aligned_vectors_de, aligned_vectors_eng), axis=1)\n",
    "\n",
    "# Convert the Series of 1D arrays to a 2D numpy array (to calculate the cosine similarity later on)\n",
    "fasttext_array = np.array(word2vec.tolist())\n",
    "len(fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

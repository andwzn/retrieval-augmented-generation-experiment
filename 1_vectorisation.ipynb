{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and create frequently used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewinzen/opt/anaconda3/envs/retrieval_augmented_generation/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from util.webscraper import WebScraper\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "import sentence_transformers\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to load the \n",
    "# \n",
    "# models\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"File was already downloaded.\")\n",
    "            return None\n",
    "        \n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"The file has been downloaded and saved as: {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred while downloading the file: {e}\")\n",
    "        \n",
    "def load_word_vectors(file_path: str):\n",
    "    \"\"\"Load word vectors from a file.\"\"\"\n",
    "    try:\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        print(\"Vectors loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the vectors: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the data from the API. As the API is not yet published, both the API-Url and the query to get information on edition-software need to be specified in your .env file. (consult the README for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get api_url and query\n",
    "api_url = os.environ['API_URL']\n",
    "query = os.environ['QUERY']\n",
    "\n",
    "# get data from api\n",
    "api_response = requests.get(api_url + query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got the data from the API, we can load it into a dataframe to prepare it to be used as a knowledge base for rag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43 entries, 0 to 42\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   id                43 non-null     object\n",
      " 1   slug              43 non-null     object\n",
      " 2   brand_name        43 non-null     object\n",
      " 3   concept_doi       0 non-null      object\n",
      " 4   description       40 non-null     object\n",
      " 5   description_url   3 non-null      object\n",
      " 6   description_type  43 non-null     object\n",
      " 7   get_started_url   41 non-null     object\n",
      " 8   image_id          31 non-null     object\n",
      " 9   is_published      43 non-null     bool  \n",
      " 10  short_statement   43 non-null     object\n",
      " 11  created_at        43 non-null     object\n",
      " 12  updated_at        43 non-null     object\n",
      " 13  closed_source     43 non-null     bool  \n",
      "dtypes: bool(2), object(12)\n",
      "memory usage: 4.2+ KB\n"
     ]
    }
   ],
   "source": [
    "edition_software_info = json.loads(api_response.text)\n",
    "edition_software_info = pd.DataFrame(edition_software_info)\n",
    "edition_software_info.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief inspection allows us to formulate some initial tasks and questions for this experiment.\n",
    "\n",
    "- **Preprocessing:** As we can see, not a single entry contains a associated concept_doi. We might consider dropping the column.\n",
    "- **Impact of using short descriptions only:** Three entries are missing the in depth description. We can assume that rag won't be too useful for these entries. \n",
    "- **Impact of additional information:** Only three have a description-url. Down the road, we need to evaluate, if adding info from this source improves the performance of the rag-system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove Artefacts\n",
    "\n",
    "Both the `description` and `short_statement` columns seem to be of particular interest for the task at hand. To asses necessary preprocessing step, we'll need to take a closer look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>short_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>The Research Software Directory is a content management system that is tailored to research software.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                             short_statement  \n",
       "0                                                                                                                          Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.  \n",
       "1                                  Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.  \n",
       "2          CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.  \n",
       "3  LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.  \n",
       "4                                                                                                                                                                                                      The Research Software Directory is a content management system that is tailored to research software.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptions = edition_software_info[[\"description\", \"short_statement\"]]\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(descriptions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `description` column contains some formatting artefacts like `\\n` and markdown syntax like `**` and `#`. Let's clean them up.\n",
    "While we're at it, we can also remove double whitespaces etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autodone</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n</td>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LaTeX</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Software Directory</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    brand_name  \\\n",
       "0                  Transkribus   \n",
       "1                     Autodone   \n",
       "2                     CollateX   \n",
       "3                        LaTeX   \n",
       "4  Research Software Directory   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  description  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.\\n\\nSpecial features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future.\\n\\nautodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage.\\n\\n(quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)\\n\\n--- \\n## Official Site:\\n[https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)\\n\\n---\\n## Usage Instructions\\n[https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)\\n   \n",
       "2  [CollateX](http://collatex.net/) is a software to\\n\\n 1. read **multiple (≥ 2) versions of a text**, splitting each version into parts (tokens) to be compared,\\n 1. **identify similarities of and differences between the versions** (including moved/transposed segments) by aligning tokens, and\\n 1. output the alignment results in a **variety of formats for further processing**, for instance\\n 1. to support **the production of a critical apparatus** or the stemmatical analysis of a text's genesis.\\n\\nIt resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results.\\n\\nAs such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable.\\n\\nPlease go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache.\\n\\nMit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)   \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = '\\\\n+'\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "pattern = r'[*#]+|\\s-+\\s|]]' #\\[\\]()<>\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description_clean\"].str.replace(pattern, ' ', regex=True)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"brand_name\", \"description\", \"description_clean\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fill nan values\n",
    "\n",
    "Before we continue preprocessing the data for later vectorization, we need to check for missing values and replace them with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j42m3p7n5jl51rggmnpn_6940000gn/T/ipykernel_4225/3857300838.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "edition_software_info[\"description_clean\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide additional context-information for the retrieval process, we'll scrape all webpages referenced in the software-description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get urls\n",
    "\n",
    "First, we isolate the urls from our description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_clean</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)</td>\n",
       "      <td>https://autodone.idh.uni-koeln.de/usage,https://autodone.idh.uni-koeln.de/about,https://autodone.idh.uni-koeln.de/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;http://collatex.net/&gt; for further information.</td>\n",
       "      <td>http://collatex.net/,http://en.wikipedia.org/wiki/Philology,http://en.wikipedia.org/wiki/Sequence_alignment,http://en.wikipedia.org/wiki/Textual_criticism,http://en.wikipedia.org/wiki/Diff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: https://autodone.idh.uni-koeln.de/about, 19.04.2024)    Official Site: [https://autodone.idh.uni-koeln.de/](https://autodone.idh.uni-koeln.de/)   Usage Instructions [https://autodone.idh.uni-koeln.de/usage](https://autodone.idh.uni-koeln.de/usage)    \n",
       "2  [CollateX](http://collatex.net/) is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff](http://en.wikipedia.org/wiki/Diff)) or tools for [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology](http://en.wikipedia.org/wiki/Philology) or – more specifically – the field of [Textual Criticism](http://en.wikipedia.org/wiki/Textual_criticism) where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <http://collatex.net/> for further information.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                           urls  \n",
       "0                                                                                                                                                                                           NaN  \n",
       "1                                                                            https://autodone.idh.uni-koeln.de/usage,https://autodone.idh.uni-koeln.de/about,https://autodone.idh.uni-koeln.de/  \n",
       "2  http://collatex.net/,http://en.wikipedia.org/wiki/Philology,http://en.wikipedia.org/wiki/Sequence_alignment,http://en.wikipedia.org/wiki/Textual_criticism,http://en.wikipedia.org/wiki/Diff  \n",
       "3                                                                                                                                                                                           NaN  \n",
       "4                                                                                                                                                                                           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "\n",
    "urls = edition_software_info[\"description\"].str.extractall(pattern)\n",
    "urls = urls.droplevel(1)\n",
    "urls_grouped = urls.groupby(urls.index).agg((lambda x: ','.join(set(x))))\n",
    "edition_software_info[\"urls\"] = urls_grouped\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info[[\"description_clean\", \"urls\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape Webpages\n",
    "\n",
    "Now we scrape the paragraphs from the webpages we found. \n",
    "The webscraper will take the list of urls associated with an entry and will save paragraphs from all webpages as a string in a column of our dataframe. \n",
    "\n",
    "**This might take some time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://autodone.idh.uni-koeln.de/usage with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/about with parameters tags = ['p']\n",
      "Scraping https://autodone.idh.uni-koeln.de/ with parameters tags = ['p']\n",
      "Scraping http://collatex.net/ with parameters tags = ['p']\n",
      "Scraping http://www.tei-c.org/ with parameters tags = ['p']\n",
      "Scraping http://vbd.humnet.unipi.it/ with parameters tags = ['p']\n",
      "Scraping https://www.oeaw.ac.at/oesterreichische-akademie-der-wissenschaften with parameters tags = ['p']\n",
      "Scraping https://cte.oeaw.ac.at/ with parameters tags = ['p']\n",
      "Scraping https://opensource.org/licenses/EUPL-1.2 with parameters tags = ['p']\n",
      "Scraping https://sites.fastspring.com/stefanhagel/product/cte with parameters tags = ['p']\n",
      "Scraping http://csel.at/ with parameters tags = ['p']\n",
      "Scraping https://phylipweb.github.io/phylip/general.html with parameters tags = ['p']\n",
      "Scraping https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html with parameters tags = ['p']\n",
      "HTTP error occurred for https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html: 404 Client Error: Not Found for url: https://www.sglp.uzh.ch/static/MLS/stemmatology/PAUP_229150101.html\n",
      "Scraping http://publex.uni-trier.de/dictionary-network with parameters tags = ['p']\n",
      "HTTP error occurred for http://publex.uni-trier.de/dictionary-network: 503 Server Error: Service Unavailable for url: http://publex.uni-trier.de/dictionary-network\n",
      "Scraping https://campus.dariah.eu/resource/posts/how-to-publish-your-dictionary-data-with-publex with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/komplett with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/publex with parameters tags = ['p']\n",
      "Scraping https://bitbucket.org/tcdh/publex-allentities/src/master/ with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/kurt-schwitters-intermedia-networks-avant-garde with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitale-edition-and-analysis-medulla-gestorum-treverensium-johann-enen-1514 with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/transcribo with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digitalization-plock-bible with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/stefan-heym-ahasver with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/augsburg-master-builders-ledgers with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/digital-marburg-buchner-edition with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/johann-caspar-lavater with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/old-high-german-dictionary with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/virtual-research-environment-humanities-and-social-sciences-fud with parameters tags = ['p']\n",
      "Scraping https://tcdh.uni-trier.de/en/projekt/arthur-schnitzler-digital with parameters tags = ['p']\n",
      "Scraping https://fud.uni-trier.de/community/referenzen/ with parameters tags = ['p']\n",
      "Scraping https://textgridrep.de/ with parameters tags = ['p']\n",
      "Scraping https://ride.i-d-e.de/issues/issue-11/omeka/ with parameters tags = ['p']\n",
      "Scraping https://viscoll.org with parameters tags = ['p']\n",
      "Scraping http://scta.lombardpress.org with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/download.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/witnesses.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/print.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/check.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/image.php with parameters tags = ['p']\n",
      "Scraping http://teicat.huma-num.fr/stats.php with parameters tags = ['p']\n"
     ]
    }
   ],
   "source": [
    "webscraper = WebScraper(tags = [\"p\"], exclude = [\"wikipedia\"])\n",
    "edition_software_info[\"webpages_text\"] = edition_software_info[\"urls\"].apply(lambda x: webscraper.scrape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>webpages_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://autodone.idh.uni-koeln.de/usage,https:...</td>\n",
       "      <td>On this page you will find instructions on how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://collatex.net/,http://en.wikipedia.org/w...</td>\n",
       "      <td>“In a language, in the system of language, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                urls  \\\n",
       "0                                                NaN   \n",
       "1  https://autodone.idh.uni-koeln.de/usage,https:...   \n",
       "2  http://collatex.net/,http://en.wikipedia.org/w...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                       webpages_text  \n",
       "0                                                NaN  \n",
       "1  On this page you will find instructions on how...  \n",
       "2  “In a language, in the system of language, the...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info[[\"urls\", \"webpages_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is collected from the webpages, we can take a look at the average length of the texts received for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       13.000000\n",
       "mean     15479.153846\n",
       "std      17608.288786\n",
       "min       1114.000000\n",
       "25%       2300.000000\n",
       "50%       6739.000000\n",
       "75%      23657.000000\n",
       "max      55063.000000\n",
       "Name: webpages_text, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = edition_software_info[\"webpages_text\"].apply(lambda x: len(x) if not pd.isna(x) else 0)\n",
    "length[length>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking only at entries, that we were able to collected webpage text for, we have an average character count of about 15.000 per entry. \n",
    "The standard deviation is quite large compared to the mean, indicating that there is a high degree of variability in character counts.\n",
    "\n",
    "The distribution is skewed towards entries with lower character counts, while some outliers with a high character counts pull the mean upwards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine some information to increase the information density in our description. We'll start off by simply appending the `short statement` to the front of the description. \n",
    "In later steps we might add other information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_software_info[\"description_clean\"] = edition_software_info[\"short_statement\"] + edition_software_info[\"description_clean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can prepare our data for the information retrieval process. We'll focus on 'descriptions' for now. However, this process could be easily expanded to the webpage text we collected earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove links\n",
    "\n",
    "First, we'll remove all links from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Transkribus ist eine umfassende Plattform für ...\n",
       "1    Autodone is a service for the automated, time-...\n",
       "2    CollateX is a software to (a.) read multiple v...\n",
       "Name: description_clean, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"((?:https?:\\/\\/|w{3}.)[\\w\\d%/.-]+)\"\n",
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description_clean\"].str.replace(pattern, '', regex=True)\n",
    "edition_software_info[\"description_clean\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Replace missing descriptions\n",
    "\n",
    "To allow for smoother text processing, we'll replace all NaN values in the relevant text columns with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN remaining: 0\n"
     ]
    }
   ],
   "source": [
    "edition_software_info[\"description_clean\"] = edition_software_info[\"description_clean\"].fillna('')\n",
    "null = edition_software_info[\"description_clean\"].isna().sum()\n",
    "print(f\"NaN remaining: {null}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add Language Information\n",
    "\n",
    "As the entries in our repository are in both english and german, we add information on the texts language to the dataset.\n",
    "\n",
    "To do so, we'll use a fasttext-model for language identification, which can be found [here](https://fasttext.cc/docs/en/language-identification.html).\n",
    "\n",
    "As the model was trained on UTF-8 data, it expects UTF-8 as input. This sould be the case, as pandas `read_csv`-function imports text in UTF-8 by default.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found.\n"
     ]
    }
   ],
   "source": [
    "# download the fasttext model\n",
    "\n",
    "fasttext_path = os.path.join(os.getcwd(), 'models/lid.176.bin')\n",
    "\n",
    "if os.path.isfile(fasttext_path):\n",
    "    print(\"Model found.\")\n",
    "    language_detection_model = fasttext.load_model(fasttext_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_path)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    language_detection_model = fasttext.load_model(fasttext_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model to identify the languages of our entries. This might take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>description_lang</th>\n",
       "      <th>short_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von...</td>\n",
       "      <td>de</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>autodone is a service for the automated, time-...</td>\n",
       "      <td>en</td>\n",
       "      <td>Autodone is a service for the automated, time-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CollateX](http://collatex.net/) is a software...</td>\n",
       "      <td>en</td>\n",
       "      <td>CollateX is a software to (a.) read multiple v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Mathematiker Donald E. Knuth entwickelte E...</td>\n",
       "      <td>de</td>\n",
       "      <td>LaTeX (gesprochen “Lah-tech” oder “Lay-tech”),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>The Research Software Directory is a content m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>de</td>\n",
       "      <td>Tesseract ist eine Software zur Texterkennung....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EVT (Edition Visualization Technology) is a so...</td>\n",
       "      <td>en</td>\n",
       "      <td>A light-weight, open source tool specifically ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Der Classical Text Editor (CTE) wird auf Initi...</td>\n",
       "      <td>de</td>\n",
       "      <td>Der CTE ist ein Spezialwerkzeug für die Erstel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In vielen Editionsprojekten wird die Datenbank...</td>\n",
       "      <td>de</td>\n",
       "      <td>Der TEI Publisher ist eine eXist-db Applikatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n# Benutzerfreundliches Arbeiten\\n\\nAls zentr...</td>\n",
       "      <td>de</td>\n",
       "      <td>ediarum is a digital working environment consi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description description_lang  \\\n",
       "0  # Erkennen, Transkribieren und Durchsuchen von...               de   \n",
       "1  autodone is a service for the automated, time-...               en   \n",
       "2  [CollateX](http://collatex.net/) is a software...               en   \n",
       "3  Der Mathematiker Donald E. Knuth entwickelte E...               de   \n",
       "4                                               None               en   \n",
       "5                                               None               de   \n",
       "6  EVT (Edition Visualization Technology) is a so...               en   \n",
       "7  Der Classical Text Editor (CTE) wird auf Initi...               de   \n",
       "8  In vielen Editionsprojekten wird die Datenbank...               de   \n",
       "9  \\n# Benutzerfreundliches Arbeiten\\n\\nAls zentr...               de   \n",
       "\n",
       "                                     short_statement  \n",
       "0  Transkribus ist eine umfassende Plattform für ...  \n",
       "1  Autodone is a service for the automated, time-...  \n",
       "2  CollateX is a software to (a.) read multiple v...  \n",
       "3  LaTeX (gesprochen “Lah-tech” oder “Lay-tech”),...  \n",
       "4  The Research Software Directory is a content m...  \n",
       "5  Tesseract ist eine Software zur Texterkennung....  \n",
       "6  A light-weight, open source tool specifically ...  \n",
       "7  Der CTE ist ein Spezialwerkzeug für die Erstel...  \n",
       "8  Der TEI Publisher ist eine eXist-db Applikatio...  \n",
       "9  ediarum is a digital working environment consi...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fasttext_model = fasttext.load_model(fasttext_path)\n",
    "\n",
    "def identify_language(text):\n",
    "    lang_detected = fasttext_model.predict(text)\n",
    "    return lang_detected[0][0]\n",
    "\n",
    "# clean the webpage text, as the model expect text without newlines\n",
    "edition_software_info.loc[:,\"description_clean\"] = edition_software_info[\"description_clean\"].str.replace(\"\\n\",\" \")\n",
    "\n",
    "# detect message-languages. If the column contains empty text, the language is set to nan\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description_clean\"].apply(lambda x: identify_language(x) if not x == '' else np.nan)\n",
    "\n",
    "# clean the output\n",
    "edition_software_info.loc[:,\"description_lang\"] = edition_software_info[\"description_lang\"].str.replace(\"__label__\",\"\")\n",
    "\n",
    "# print the new columns\n",
    "edition_software_info[[\"description\",\"description_lang\", \"short_statement\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Chunking\n",
    "\n",
    "Finally, we chunk longer entries into smaller paragraphs. We'lle leave some overlap between chunks, as to retain some context between chunks.\n",
    "\n",
    "We start off with chunk sizes of 108 token and an overlap of 20, as 128 was identified to be a effective chunk size for RAG in this [blogpost](https://www.mattambrogi.com/posts/chunk-size-matters/) by Matt Ambrogi.\n",
    "\n",
    "**Both the chunk and overlap size are hyperparamters and should be fine tuned for the task at hand.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.  Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autodone</td>\n",
       "      <td>Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: , 19.04.2024)    Official Site: []()   Usage Instructions []()</td>\n",
       "      <td>Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: , 19.04.2024) Official Site: []() Usage Instructions []()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;&gt; for further information.</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to 1. read multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared, 1. identify similarities of and differences between the versions (including moved/transposed segments) by aligning tokens, and 1. output the alignment results in a variety of formats for further processing , for instance 1. to support the production of a critical apparatus or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;&gt; for further information.</td>\n",
       "      <td>[Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;&gt; for further information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LaTeX</td>\n",
       "      <td>LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Research Software Directory</td>\n",
       "      <td>The Research Software Directory is a content management system that is tailored to research software.</td>\n",
       "      <td>The Research Software Directory is a content management system that is tailored to research software.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tesseract OCR</td>\n",
       "      <td>Tesseract ist eine Software zur Texterkennung. Mehr als 100 Sprachen und Sprachvarianten werden unterstützt, zudem verschiedene Schriften / Schriftsysteme:  lateinische Antiqua, Fraktur, Devanagari (indische Schrift), chinesische, arabische, griechische, hebräische, kyrillische Schrift.</td>\n",
       "      <td>Tesseract ist eine Software zur Texterkennung. Mehr als 100 Sprachen und Sprachvarianten werden unterstützt, zudem verschiedene Schriften / Schriftsysteme: lateinische Antiqua, Fraktur, Devanagari (indische Schrift), chinesische, arabische, griechische, hebräische, kyrillische Schrift.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EVT</td>\n",
       "      <td>A light-weight, open source tool specifically designed to create digital editions from XML-encoded texts, freeing the scholar from the burden of web programming and enabling the final user to browse, explore and study digital editions by means of a user-friendly interface.EVT (Edition Visualization Technology) is a software for creating and browsing digital editions of manuscripts based on text encoded according to the [TEI XML]() schemas and Guidelines. This tool was born as part of the [Digital Vercelli Book]() project in order to allow the creation of a digital edition of the Vercelli Book, a parchment codex of the late tenth century, now preserved in the Archivio e Biblioteca Capitolare of Vercelli and regarded as one of the four most important manuscripts of the Anglo-Saxon period as regards the transmission of poetic texts in the Old English language. To ensure that it will be working on all the most recent web browsers, and for as long as possible on the World Wide Web itself, EVT is built on open and standard web technologies such as HTML, CSS and JavaScript. Specific features, such as the magnifying lens, are entrusted to jQuery plug-ins, again chosen among the open source and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble or turn out to be not completely up to the task can be replaced easily.</td>\n",
       "      <td>A light-weight, open source tool specifically designed to create digital editions from XML-encoded texts, freeing the scholar from the burden of web programming and enabling the final user to browse, explore and study digital editions by means of a user-friendly interface.EVT (Edition Visualization Technology) is a software for creating and browsing digital editions of manuscripts based on text encoded according to the [TEI XML]() schemas and Guidelines. This tool was born as part of the [Digital Vercelli Book]() project in order to allow the creation of a digital edition of the Vercelli Book, a parchment codex of the late tenth century, now preserved in the Archivio e Biblioteca Capitolare of Vercelli and regarded as one of the four most important manuscripts of the Anglo-Saxon period as regards the transmission of poetic texts in the Old English language. To ensure that it will be working on all the most recent web browsers, and for as long as possible on the World Wide Web itself, EVT is built on open and standard web technologies such as HTML, CSS and JavaScript. Specific features, such as the magnifying lens, are entrusted to jQuery plug-ins, again chosen among the open source and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EVT</td>\n",
       "      <td>A light-weight, open source tool specifically designed to create digital editions from XML-encoded texts, freeing the scholar from the burden of web programming and enabling the final user to browse, explore and study digital editions by means of a user-friendly interface.EVT (Edition Visualization Technology) is a software for creating and browsing digital editions of manuscripts based on text encoded according to the [TEI XML]() schemas and Guidelines. This tool was born as part of the [Digital Vercelli Book]() project in order to allow the creation of a digital edition of the Vercelli Book, a parchment codex of the late tenth century, now preserved in the Archivio e Biblioteca Capitolare of Vercelli and regarded as one of the four most important manuscripts of the Anglo-Saxon period as regards the transmission of poetic texts in the Old English language. To ensure that it will be working on all the most recent web browsers, and for as long as possible on the World Wide Web itself, EVT is built on open and standard web technologies such as HTML, CSS and JavaScript. Specific features, such as the magnifying lens, are entrusted to jQuery plug-ins, again chosen among the open source and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble or turn out to be not completely up to the task can be replaced easily.</td>\n",
       "      <td>and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble or turn out to be not completely up to the task can be replaced easily.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Classical Text Editor (CTE)</td>\n",
       "      <td>Der CTE ist ein Spezialwerkzeug für die Erstellung einer kritischen Ausgabe bzw. eines Texts mit Kommentar oder Übersetzung. Er dient der einfachen Erzeugung einer druckfertigen PDF- oder elektronischen Ausgabe.Der Classical Text Editor (CTE) wird auf Initiative der [Österreichischen Akademie der Wissenschaften]() und des Editionsprojekts [Corpus Scriptorum Ecclesiasticorum Latinorum (CSEL)]() seit 1997 entwickelt. Für die Nutzung des CTE muss eine [Lizenz erworben]() werden, da das Projekt keine öffentliche Förderung erhält. Perspektivisch wird eine Veröffentlichung gemäß der Opensource-Lizenz [EUPL-1.2]() angestrebt. Der CTE schließt für den Bereich der kritischen Ausgaben die Lücke zwischen klassischen Text verarbeitungs programmen (Open Office Writer, Microsoft Word u.a.) auf der einen und Text satz programmen (TeX, TUSTEP Satz) auf der anderen Seite. Autor:innen können mit Hilfe einer graphischen Oberfläche Publikationen erstellen und gleichzeitig die gerade für kritische Ausgaben nötigen typographischen Anforderungen umsetzen. Neben umfangreichen Funktionen im Bereich kritischer Editionen (verschiedene Layouts, beliebig viele Apparate, Varianten etc.) gehören daher auch die Unterstützung von Unicode, komplexen Skripten sowie die Einbindung verschiedener Referenzsysteme zum Funktionsumfang des CTE. Das Ergebnis kann sowohl für die Drucklegung im PDF-Format als auch für eine elektronische Fassung als HTML- oder XML-Publikation exportiert werden. Umgekehrt können Texte in verschiedenen Formaten importiert werden.     Eine detaillierte Auflistung aller Feature des CTE findet man unter ?id0=features.</td>\n",
       "      <td>Der CTE ist ein Spezialwerkzeug für die Erstellung einer kritischen Ausgabe bzw. eines Texts mit Kommentar oder Übersetzung. Er dient der einfachen Erzeugung einer druckfertigen PDF- oder elektronischen Ausgabe.Der Classical Text Editor (CTE) wird auf Initiative der [Österreichischen Akademie der Wissenschaften]() und des Editionsprojekts [Corpus Scriptorum Ecclesiasticorum Latinorum (CSEL)]() seit 1997 entwickelt. Für die Nutzung des CTE muss eine [Lizenz erworben]() werden, da das Projekt keine öffentliche Förderung erhält. Perspektivisch wird eine Veröffentlichung gemäß der Opensource-Lizenz [EUPL-1.2]() angestrebt. Der CTE schließt für den Bereich der kritischen Ausgaben die Lücke zwischen klassischen Text verarbeitungs programmen (Open Office Writer, Microsoft Word u.a.) auf der einen und Text satz programmen (TeX, TUSTEP Satz) auf der anderen Seite. Autor:innen können mit Hilfe einer graphischen Oberfläche Publikationen erstellen und gleichzeitig die gerade für kritische Ausgaben nötigen typographischen Anforderungen umsetzen. Neben umfangreichen Funktionen im Bereich kritischer Editionen (verschiedene Layouts, beliebig viele Apparate, Varianten etc.) gehören daher auch die Unterstützung von Unicode, komplexen Skripten sowie die Einbindung verschiedener Referenzsysteme zum Funktionsumfang des CTE. Das Ergebnis kann sowohl für die Drucklegung im PDF-Format als auch für eine elektronische Fassung als HTML- oder XML-Publikation exportiert werden. Umgekehrt können Texte in verschiedenen Formaten importiert werden. Eine detaillierte Auflistung aller Feature des CTE findet man unter ?id0=features.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    brand_name  \\\n",
       "0                  Transkribus   \n",
       "1                     Autodone   \n",
       "2                     CollateX   \n",
       "3                     CollateX   \n",
       "4                        LaTeX   \n",
       "5  Research Software Directory   \n",
       "6                Tesseract OCR   \n",
       "7                          EVT   \n",
       "8                          EVT   \n",
       "9  Classical Text Editor (CTE)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              description_clean  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten.  Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: , 19.04.2024)    Official Site: []()   Usage Instructions []()    \n",
       "2                                                                                      CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <> for further information.   \n",
       "3                                                                                      CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to  1. read  multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared,  1.  identify similarities of and differences between the versions  (including moved/transposed segments) by aligning tokens, and  1. output the alignment results in a  variety of formats for further processing , for instance  1. to support  the production of a critical apparatus  or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <> for further information.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The Research Software Directory is a content management system that is tailored to research software.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Tesseract ist eine Software zur Texterkennung. Mehr als 100 Sprachen und Sprachvarianten werden unterstützt, zudem verschiedene Schriften / Schriftsysteme:  lateinische Antiqua, Fraktur, Devanagari (indische Schrift), chinesische, arabische, griechische, hebräische, kyrillische Schrift.   \n",
       "7                                                                                                                                                                             A light-weight, open source tool specifically designed to create digital editions from XML-encoded texts, freeing the scholar from the burden of web programming and enabling the final user to browse, explore and study digital editions by means of a user-friendly interface.EVT (Edition Visualization Technology) is a software for creating and browsing digital editions of manuscripts based on text encoded according to the [TEI XML]() schemas and Guidelines. This tool was born as part of the [Digital Vercelli Book]() project in order to allow the creation of a digital edition of the Vercelli Book, a parchment codex of the late tenth century, now preserved in the Archivio e Biblioteca Capitolare of Vercelli and regarded as one of the four most important manuscripts of the Anglo-Saxon period as regards the transmission of poetic texts in the Old English language. To ensure that it will be working on all the most recent web browsers, and for as long as possible on the World Wide Web itself, EVT is built on open and standard web technologies such as HTML, CSS and JavaScript. Specific features, such as the magnifying lens, are entrusted to jQuery plug-ins, again chosen among the open source and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble or turn out to be not completely up to the task can be replaced easily.   \n",
       "8                                                                                                                                                                             A light-weight, open source tool specifically designed to create digital editions from XML-encoded texts, freeing the scholar from the burden of web programming and enabling the final user to browse, explore and study digital editions by means of a user-friendly interface.EVT (Edition Visualization Technology) is a software for creating and browsing digital editions of manuscripts based on text encoded according to the [TEI XML]() schemas and Guidelines. This tool was born as part of the [Digital Vercelli Book]() project in order to allow the creation of a digital edition of the Vercelli Book, a parchment codex of the late tenth century, now preserved in the Archivio e Biblioteca Capitolare of Vercelli and regarded as one of the four most important manuscripts of the Anglo-Saxon period as regards the transmission of poetic texts in the Old English language. To ensure that it will be working on all the most recent web browsers, and for as long as possible on the World Wide Web itself, EVT is built on open and standard web technologies such as HTML, CSS and JavaScript. Specific features, such as the magnifying lens, are entrusted to jQuery plug-ins, again chosen among the open source and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble or turn out to be not completely up to the task can be replaced easily.   \n",
       "9  Der CTE ist ein Spezialwerkzeug für die Erstellung einer kritischen Ausgabe bzw. eines Texts mit Kommentar oder Übersetzung. Er dient der einfachen Erzeugung einer druckfertigen PDF- oder elektronischen Ausgabe.Der Classical Text Editor (CTE) wird auf Initiative der [Österreichischen Akademie der Wissenschaften]() und des Editionsprojekts [Corpus Scriptorum Ecclesiasticorum Latinorum (CSEL)]() seit 1997 entwickelt. Für die Nutzung des CTE muss eine [Lizenz erworben]() werden, da das Projekt keine öffentliche Förderung erhält. Perspektivisch wird eine Veröffentlichung gemäß der Opensource-Lizenz [EUPL-1.2]() angestrebt. Der CTE schließt für den Bereich der kritischen Ausgaben die Lücke zwischen klassischen Text verarbeitungs programmen (Open Office Writer, Microsoft Word u.a.) auf der einen und Text satz programmen (TeX, TUSTEP Satz) auf der anderen Seite. Autor:innen können mit Hilfe einer graphischen Oberfläche Publikationen erstellen und gleichzeitig die gerade für kritische Ausgaben nötigen typographischen Anforderungen umsetzen. Neben umfangreichen Funktionen im Bereich kritischer Editionen (verschiedene Layouts, beliebig viele Apparate, Varianten etc.) gehören daher auch die Unterstützung von Unicode, komplexen Skripten sowie die Einbindung verschiedener Referenzsysteme zum Funktionsumfang des CTE. Das Ergebnis kann sowohl für die Drucklegung im PDF-Format als auch für eine elektronische Fassung als HTML- oder XML-Publikation exportiert werden. Umgekehrt können Texte in verschiedenen Formaten importiert werden.     Eine detaillierte Auflistung aller Feature des CTE findet man unter ?id0=features.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   description_clean_chunks  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: , 19.04.2024) Official Site: []() Usage Instructions []()  \n",
       "2                                                                                                                                                                 CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to 1. read multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared, 1. identify similarities of and differences between the versions (including moved/transposed segments) by aligning tokens, and 1. output the alignment results in a variety of formats for further processing , for instance 1. to support the production of a critical apparatus or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <> for further information.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The Research Software Directory is a content management system that is tailored to research software.  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Tesseract ist eine Software zur Texterkennung. Mehr als 100 Sprachen und Sprachvarianten werden unterstützt, zudem verschiedene Schriften / Schriftsysteme: lateinische Antiqua, Fraktur, Devanagari (indische Schrift), chinesische, arabische, griechische, hebräische, kyrillische Schrift.  \n",
       "7                                                                                                                                                                                                                                                 A light-weight, open source tool specifically designed to create digital editions from XML-encoded texts, freeing the scholar from the burden of web programming and enabling the final user to browse, explore and study digital editions by means of a user-friendly interface.EVT (Edition Visualization Technology) is a software for creating and browsing digital editions of manuscripts based on text encoded according to the [TEI XML]() schemas and Guidelines. This tool was born as part of the [Digital Vercelli Book]() project in order to allow the creation of a digital edition of the Vercelli Book, a parchment codex of the late tenth century, now preserved in the Archivio e Biblioteca Capitolare of Vercelli and regarded as one of the four most important manuscripts of the Anglo-Saxon period as regards the transmission of poetic texts in the Old English language. To ensure that it will be working on all the most recent web browsers, and for as long as possible on the World Wide Web itself, EVT is built on open and standard web technologies such as HTML, CSS and JavaScript. Specific features, such as the magnifying lens, are entrusted to jQuery plug-ins, again chosen among the open source and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          and best supported ones to reduce the risk of future incompatibilities. The general architecture of the software, in any case, is modular, so that any component which may cause trouble or turn out to be not completely up to the task can be replaced easily.  \n",
       "9  Der CTE ist ein Spezialwerkzeug für die Erstellung einer kritischen Ausgabe bzw. eines Texts mit Kommentar oder Übersetzung. Er dient der einfachen Erzeugung einer druckfertigen PDF- oder elektronischen Ausgabe.Der Classical Text Editor (CTE) wird auf Initiative der [Österreichischen Akademie der Wissenschaften]() und des Editionsprojekts [Corpus Scriptorum Ecclesiasticorum Latinorum (CSEL)]() seit 1997 entwickelt. Für die Nutzung des CTE muss eine [Lizenz erworben]() werden, da das Projekt keine öffentliche Förderung erhält. Perspektivisch wird eine Veröffentlichung gemäß der Opensource-Lizenz [EUPL-1.2]() angestrebt. Der CTE schließt für den Bereich der kritischen Ausgaben die Lücke zwischen klassischen Text verarbeitungs programmen (Open Office Writer, Microsoft Word u.a.) auf der einen und Text satz programmen (TeX, TUSTEP Satz) auf der anderen Seite. Autor:innen können mit Hilfe einer graphischen Oberfläche Publikationen erstellen und gleichzeitig die gerade für kritische Ausgaben nötigen typographischen Anforderungen umsetzen. Neben umfangreichen Funktionen im Bereich kritischer Editionen (verschiedene Layouts, beliebig viele Apparate, Varianten etc.) gehören daher auch die Unterstützung von Unicode, komplexen Skripten sowie die Einbindung verschiedener Referenzsysteme zum Funktionsumfang des CTE. Das Ergebnis kann sowohl für die Drucklegung im PDF-Format als auch für eine elektronische Fassung als HTML- oder XML-Publikation exportiert werden. Umgekehrt können Texte in verschiedenen Formaten importiert werden. Eine detaillierte Auflistung aller Feature des CTE findet man unter ?id0=features.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chunk(text, chunk_size=226, overlap_size=30):\n",
    "    \"\"\"\n",
    "    Splits the given text into chunks with overlap.\n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        chunk_size (int, optional): The size of each chunk. Defaults to 108.\n",
    "        overlap_size (int, optional): The size of overlap between chunks. Defaults to 20.\n",
    "    Returns:\n",
    "        list: A list of chunks with overlap.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap_size):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        if i + chunk_size >= len(words):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "# Apply the chunking function and explode the chunks, to give every chunk its own row while retaining all other data\n",
    "edition_software_info_chunked = edition_software_info.copy()\n",
    "edition_software_info_chunked['description_clean_chunks'] = edition_software_info_chunked['description_clean'].apply(lambda x: chunk(x))\n",
    "edition_software_info_chunked = edition_software_info_chunked.explode('description_clean_chunks') \n",
    "\n",
    "# reindex the dataframe\n",
    "edition_software_info_chunked = edition_software_info_chunked.reset_index(drop=True)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(edition_software_info_chunked[[\"brand_name\", \"description_clean\", \"description_clean_chunks\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Removing punctiation, stopwords and upper case letters.\n",
    "\n",
    "As some vectorization methods need additional preprocessing steps, we'll create a new column for description with their punctiation, stopwords and upper case letters removed. As we have both english and german texts, we'll need to account for stopwords in both languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(stopwords: List[str], text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by converting it to lowercase, removing punctuation, and filtering out stopwords.\n",
    "    Args:\n",
    "        stopwords (List[str]): A list of stopwords to be filtered out from the text.\n",
    "        text (str): The input text to be preprocessed.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    \n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "# get stopwords\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "stopwords_combined = stopwords_german.union(stopwords_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    transkribus umfassende plattform digitalisieru...\n",
       "1    autodone service automated timecontrolled publ...\n",
       "2    collatex software read multiple versions text ...\n",
       "3    philology – specifically – field textual criti...\n",
       "4    latex gesprochen “lahtech” “laytech” textsatzs...\n",
       "Name: description_preprocessed_chunks, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edition_software_info_chunked[\"description_preprocessed_chunks\"] = edition_software_info_chunked[\"description_clean_chunks\"].apply(lambda x: preprocess(stopwords_combined, x))\n",
    "edition_software_info_chunked[\"description_preprocessed_chunks\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to the vectorisations, we'll save the dataset to our drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "path = os.path.join(current_dir, 'data/edition_software_info_chunked.csv')\n",
    "edition_software_info_chunked.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 1: TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off with a simple TF-IDF vectorization.\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)** is a weighting scheme that weights the cells of a term-document matrix by their potential to be discriminatory.\n",
    "\n",
    "To do so, we first calculate the **term frequency (TF)**. The term frequency represents the number of instances of a given word $t$ in a document $d$.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Count of } t \\text{ in } d}{\\text{Total number of words in } d}\n",
    "$$\n",
    "\n",
    "This term frequency is then multiplied by the **inverse document frequency (IDF)**. The IDF is calculated by counting all documents that contain a term $t$ (the document frequency $\\text{df}(t)$). Then, we divide the total number of documents $N$ in the corpus by $\\text{df}(t)$.\n",
    "\n",
    "This inverse frequency is chosen over the regular frequency to **downweight** terms that appear in many documents, since these terms are less likely to be useful for distinguishing between documents.\n",
    "\n",
    "Usually, we also take the logarithm of the IDF to smooth out the very large values that can occur when a term appears in only a few documents. This ensures that rare terms are not excessively weighted.\n",
    "\n",
    "$$\n",
    "\\text{df}(t) = \\text{Document frequency of a term } t\n",
    "$$\n",
    "$$\n",
    "N = \\text{Number of documents}\n",
    "$$\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{\\text{df}(t)}\\right)\n",
    "$$\n",
    "\n",
    "Finally, we calculate the **TF-IDF** by multiplying the term frequency $\\text{TF}(t, d)$ with the inverse document frequency $\\text{IDF}(t)$.\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "The resulting value can be interpreted as a measure of the importance of the term in a document relative to the entire corpus. Terms that are frequent in a document but rare across the corpus will have higher TF-IDF scores, indicating their importance.\n",
    "\n",
    "\n",
    "**N-grams:**\n",
    "\n",
    "To capture not just the importance of single words but also some of the **context** in which they are used, we can apply TF-IDF to **n-grams**. N-grams are contiguous sequences of $n$ words that appear together in a text. The size of the sequence, $n$, is a hyperparameter that can be adjusted depending on the specific task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Fit TF-IDF Vectorizer\n",
    "First, we fit the vectorizer on the preprocessed descriptions. \n",
    "This way, the vectorizer can transform text into numerical feature vectors based on the learned vocabulary and its distribution over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>100 sprachen</th>\n",
       "      <th>100 sprachen sprachvarianten</th>\n",
       "      <th>100 sprachen sprachvarianten unterstützt</th>\n",
       "      <th>12</th>\n",
       "      <th>12 languages</th>\n",
       "      <th>12 languages comprehensive</th>\n",
       "      <th>12 languages comprehensive righttoleft</th>\n",
       "      <th>1514</th>\n",
       "      <th>1514 digital</th>\n",
       "      <th>...</th>\n",
       "      <th>überschaubaren teil funktionen</th>\n",
       "      <th>überschaubaren teil funktionen aufgrund</th>\n",
       "      <th>übersetzung</th>\n",
       "      <th>übersetzung dient</th>\n",
       "      <th>übersetzung dient einfachen</th>\n",
       "      <th>übersetzung dient einfachen erzeugung</th>\n",
       "      <th>überwiegende</th>\n",
       "      <th>überwiegende teil</th>\n",
       "      <th>überwiegende teil funktionen</th>\n",
       "      <th>überwiegende teil funktionen editionen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 19962 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    100  100 sprachen  100 sprachen sprachvarianten  \\\n",
       "0   0.0           0.0                           0.0   \n",
       "1   0.0           0.0                           0.0   \n",
       "2   0.0           0.0                           0.0   \n",
       "3   0.0           0.0                           0.0   \n",
       "4   0.0           0.0                           0.0   \n",
       "..  ...           ...                           ...   \n",
       "65  0.0           0.0                           0.0   \n",
       "66  0.0           0.0                           0.0   \n",
       "67  0.0           0.0                           0.0   \n",
       "68  0.0           0.0                           0.0   \n",
       "69  0.0           0.0                           0.0   \n",
       "\n",
       "    100 sprachen sprachvarianten unterstützt   12  12 languages  \\\n",
       "0                                        0.0  0.0           0.0   \n",
       "1                                        0.0  0.0           0.0   \n",
       "2                                        0.0  0.0           0.0   \n",
       "3                                        0.0  0.0           0.0   \n",
       "4                                        0.0  0.0           0.0   \n",
       "..                                       ...  ...           ...   \n",
       "65                                       0.0  0.0           0.0   \n",
       "66                                       0.0  0.0           0.0   \n",
       "67                                       0.0  0.0           0.0   \n",
       "68                                       0.0  0.0           0.0   \n",
       "69                                       0.0  0.0           0.0   \n",
       "\n",
       "    12 languages comprehensive  12 languages comprehensive righttoleft  1514  \\\n",
       "0                          0.0                                     0.0   0.0   \n",
       "1                          0.0                                     0.0   0.0   \n",
       "2                          0.0                                     0.0   0.0   \n",
       "3                          0.0                                     0.0   0.0   \n",
       "4                          0.0                                     0.0   0.0   \n",
       "..                         ...                                     ...   ...   \n",
       "65                         0.0                                     0.0   0.0   \n",
       "66                         0.0                                     0.0   0.0   \n",
       "67                         0.0                                     0.0   0.0   \n",
       "68                         0.0                                     0.0   0.0   \n",
       "69                         0.0                                     0.0   0.0   \n",
       "\n",
       "    1514 digital  ...  überschaubaren teil funktionen  \\\n",
       "0            0.0  ...                             0.0   \n",
       "1            0.0  ...                             0.0   \n",
       "2            0.0  ...                             0.0   \n",
       "3            0.0  ...                             0.0   \n",
       "4            0.0  ...                             0.0   \n",
       "..           ...  ...                             ...   \n",
       "65           0.0  ...                             0.0   \n",
       "66           0.0  ...                             0.0   \n",
       "67           0.0  ...                             0.0   \n",
       "68           0.0  ...                             0.0   \n",
       "69           0.0  ...                             0.0   \n",
       "\n",
       "    überschaubaren teil funktionen aufgrund  übersetzung  übersetzung dient  \\\n",
       "0                                       0.0          0.0                0.0   \n",
       "1                                       0.0          0.0                0.0   \n",
       "2                                       0.0          0.0                0.0   \n",
       "3                                       0.0          0.0                0.0   \n",
       "4                                       0.0          0.0                0.0   \n",
       "..                                      ...          ...                ...   \n",
       "65                                      0.0          0.0                0.0   \n",
       "66                                      0.0          0.0                0.0   \n",
       "67                                      0.0          0.0                0.0   \n",
       "68                                      0.0          0.0                0.0   \n",
       "69                                      0.0          0.0                0.0   \n",
       "\n",
       "    übersetzung dient einfachen  übersetzung dient einfachen erzeugung  \\\n",
       "0                           0.0                                    0.0   \n",
       "1                           0.0                                    0.0   \n",
       "2                           0.0                                    0.0   \n",
       "3                           0.0                                    0.0   \n",
       "4                           0.0                                    0.0   \n",
       "..                          ...                                    ...   \n",
       "65                          0.0                                    0.0   \n",
       "66                          0.0                                    0.0   \n",
       "67                          0.0                                    0.0   \n",
       "68                          0.0                                    0.0   \n",
       "69                          0.0                                    0.0   \n",
       "\n",
       "    überwiegende  überwiegende teil  überwiegende teil funktionen  \\\n",
       "0            0.0                0.0                           0.0   \n",
       "1            0.0                0.0                           0.0   \n",
       "2            0.0                0.0                           0.0   \n",
       "3            0.0                0.0                           0.0   \n",
       "4            0.0                0.0                           0.0   \n",
       "..           ...                ...                           ...   \n",
       "65           0.0                0.0                           0.0   \n",
       "66           0.0                0.0                           0.0   \n",
       "67           0.0                0.0                           0.0   \n",
       "68           0.0                0.0                           0.0   \n",
       "69           0.0                0.0                           0.0   \n",
       "\n",
       "    überwiegende teil funktionen editionen  \n",
       "0                                      0.0  \n",
       "1                                      0.0  \n",
       "2                                      0.0  \n",
       "3                                      0.0  \n",
       "4                                      0.0  \n",
       "..                                     ...  \n",
       "65                                     0.0  \n",
       "66                                     0.0  \n",
       "67                                     0.0  \n",
       "68                                     0.0  \n",
       "69                                     0.0  \n",
       "\n",
       "[70 rows x 19962 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(edition_software_info_chunked['description_preprocessed_chunks'])\n",
    "\n",
    "# display the resulting matrix\n",
    "tfidf_matrix_beautify = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_matrix_beautify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Inspect the tf-idf representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in this dataframe is a unique word, while each row is a document. The cells denote the number of occurances of a word in a document, weighted by the words potential to be distinctive.\n",
    "\n",
    "Let's take a look at the tf-idf filtered words for each description (You can find them in the column \"tf_idf_filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>tf_idf_filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>[(dokumenten, 0.2706568514645822), (durchsuchen, 0.16239411087874928)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autodone</td>\n",
       "      <td>Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: , 19.04.2024) Official Site: []() Usage Instructions []()</td>\n",
       "      <td>[(twitter, 0.14227410099289636), (service, 0.12069270558109332)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to 1. read multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared, 1. identify similarities of and differences between the versions (including moved/transposed segments) by aligning tokens, and 1. output the alignment results in a variety of formats for further processing , for instance 1. to support the production of a critical apparatus or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is</td>\n",
       "      <td>[(differences, 0.13884687211434793), (tokens, 0.1302538939544816)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>[Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to &lt;&gt; for further information.</td>\n",
       "      <td>[(computable, 0.13397024401843652), (computable please, 0.13397024401843652), (computable please go, 0.13397024401843652), (computable please go information, 0.13397024401843652), (computational means necessarily, 0.13397024401843652), (computational means necessarily computable, 0.13397024401843652), (go, 0.13397024401843652), (go information, 0.13397024401843652), (means necessarily, 0.13397024401843652), (means necessarily computable, 0.13397024401843652), (means necessarily computable please, 0.13397024401843652), (necessarily, 0.13397024401843652), (necessarily computable, 0.13397024401843652), (necessarily computable please, 0.13397024401843652), (necessarily computable please go, 0.13397024401843652), (please, 0.13397024401843652), (please go, 0.13397024401843652), (please go information, 0.13397024401843652), (supported computational means necessarily, 0.13397024401843652), (assessment, 0.12208275724850044), (assessment findings, 0.12208275724850044), (assessment findings based, 0.12208275724850044), (assessment findings based interpretation, 0.12208275724850044), (based interpretation, 0.12208275724850044), (based interpretation therefore, 0.12208275724850044), (based interpretation therefore supported, 0.12208275724850044), (computational means, 0.12208275724850044), (criticism, 0.12208275724850044), (criticism assessment, 0.12208275724850044), (criticism assessment findings, 0.12208275724850044), (criticism assessment findings based, 0.12208275724850044), (field, 0.12208275724850044), (field textual, 0.12208275724850044), (field textual criticism, 0.12208275724850044), (field textual criticism assessment, 0.12208275724850044), (findings, 0.12208275724850044), (findings based, 0.12208275724850044), (findings based interpretation, 0.12208275724850044), (findings based interpretation therefore, 0.12208275724850044), (interpretation, 0.12208275724850044), (interpretation therefore, 0.12208275724850044), (interpretation therefore supported, 0.12208275724850044), (interpretation therefore supported computational, 0.12208275724850044), (philology specifically, 0.12208275724850044), (philology specifically field, 0.12208275724850044), (philology specifically field textual, 0.12208275724850044), (specifically field, 0.12208275724850044), (specifically field textual, 0.12208275724850044), (specifically field textual criticism, 0.12208275724850044), (supported computational, 0.12208275724850044), (supported computational means, 0.12208275724850044), (textual criticism, 0.12208275724850044), (textual criticism assessment, 0.12208275724850044), (textual criticism assessment findings, 0.12208275724850044), (therefore supported, 0.12208275724850044), (therefore supported computational, 0.12208275724850044), (therefore supported computational means, 0.12208275724850044), (computational, 0.11364845115943978), (philology, 0.11364845115943978), (specifically, 0.11364845115943978)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LaTeX</td>\n",
       "      <td>LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket</td>\n",
       "      <td>[(jahre, 0.14427051575072258), (latex, 0.12238628647109732)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    brand_name  \\\n",
       "0  Transkribus   \n",
       "1     Autodone   \n",
       "2     CollateX   \n",
       "3     CollateX   \n",
       "4        LaTeX   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    description_clean_chunks  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                          Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users.autodone is a service for the automated, time-controlled publication of status updates on any Mastodon instance. The codebase is developed under a free license by the Department of Digital Humanities at the University of Cologne and is open to all interested users. Special features of the service include the ability to upload content in tabular format (tsv files) and the ability to publish posts as a thread. In addition to these basic functionalities, more features will be developed in the future. autodone replaces autoChirp, which offered the same functionality for Twitter before the Twitter API and Twitter itself was massively restricted regarding free and ethical usage. (quoted from: , 19.04.2024) Official Site: []() Usage Instructions []()   \n",
       "2  CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to 1. read multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared, 1. identify similarities of and differences between the versions (including moved/transposed segments) by aligning tokens, and 1. output the alignment results in a variety of formats for further processing , for instance 1. to support the production of a critical apparatus or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is not necessarily computable. Please go to <> for further information.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        LaTeX (gesprochen “Lah-tech” oder “Lay-tech”), ist eine Textsatz*sprache* und ein *Programm* für die Erstellung qualitativ hochwertiger Druckausgaben. Ursprünglich entwickelt für mathematischen Textsatz wird es heute für alle Arten von wissenschaftlichen Texten und auch darüber hinaus eingesetzt.Der Mathematiker Donald E. Knuth entwickelte Ende der Siebziger Jahre ein Textsatzprogramm, um seine Bücher schöner setzen zu können. Das so entstandene TeX-System verbreitete sich recht schnell, erforderte aber eine intensive Einarbeitung in die zugehörige Programmiersprache. Mit LaTeX 2e, dem Anfang der Neunziger Jahre entwickelten Makropaket   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           tf_idf_filtered_words  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [(dokumenten, 0.2706568514645822), (durchsuchen, 0.16239411087874928)]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [(twitter, 0.14227410099289636), (service, 0.12069270558109332)]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [(differences, 0.13884687211434793), (tokens, 0.1302538939544816)]  \n",
       "3  [(computable, 0.13397024401843652), (computable please, 0.13397024401843652), (computable please go, 0.13397024401843652), (computable please go information, 0.13397024401843652), (computational means necessarily, 0.13397024401843652), (computational means necessarily computable, 0.13397024401843652), (go, 0.13397024401843652), (go information, 0.13397024401843652), (means necessarily, 0.13397024401843652), (means necessarily computable, 0.13397024401843652), (means necessarily computable please, 0.13397024401843652), (necessarily, 0.13397024401843652), (necessarily computable, 0.13397024401843652), (necessarily computable please, 0.13397024401843652), (necessarily computable please go, 0.13397024401843652), (please, 0.13397024401843652), (please go, 0.13397024401843652), (please go information, 0.13397024401843652), (supported computational means necessarily, 0.13397024401843652), (assessment, 0.12208275724850044), (assessment findings, 0.12208275724850044), (assessment findings based, 0.12208275724850044), (assessment findings based interpretation, 0.12208275724850044), (based interpretation, 0.12208275724850044), (based interpretation therefore, 0.12208275724850044), (based interpretation therefore supported, 0.12208275724850044), (computational means, 0.12208275724850044), (criticism, 0.12208275724850044), (criticism assessment, 0.12208275724850044), (criticism assessment findings, 0.12208275724850044), (criticism assessment findings based, 0.12208275724850044), (field, 0.12208275724850044), (field textual, 0.12208275724850044), (field textual criticism, 0.12208275724850044), (field textual criticism assessment, 0.12208275724850044), (findings, 0.12208275724850044), (findings based, 0.12208275724850044), (findings based interpretation, 0.12208275724850044), (findings based interpretation therefore, 0.12208275724850044), (interpretation, 0.12208275724850044), (interpretation therefore, 0.12208275724850044), (interpretation therefore supported, 0.12208275724850044), (interpretation therefore supported computational, 0.12208275724850044), (philology specifically, 0.12208275724850044), (philology specifically field, 0.12208275724850044), (philology specifically field textual, 0.12208275724850044), (specifically field, 0.12208275724850044), (specifically field textual, 0.12208275724850044), (specifically field textual criticism, 0.12208275724850044), (supported computational, 0.12208275724850044), (supported computational means, 0.12208275724850044), (textual criticism, 0.12208275724850044), (textual criticism assessment, 0.12208275724850044), (textual criticism assessment findings, 0.12208275724850044), (therefore supported, 0.12208275724850044), (therefore supported computational, 0.12208275724850044), (therefore supported computational means, 0.12208275724850044), (computational, 0.11364845115943978), (philology, 0.11364845115943978), (specifically, 0.11364845115943978)]  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [(jahre, 0.14427051575072258), (latex, 0.12238628647109732)]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the threshold for TF-IDF scores\n",
    "threshold = 0.11\n",
    "\n",
    "# Filter words with TF-IDF scores greater than the threshold for each document\n",
    "def filter_words_by_threshold(row, threshold):\n",
    "    filtered_words = [(word, score) for word, score in zip(tfidf_matrix_beautify.columns, row) if score > threshold]\n",
    "    return sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Apply the function to each row of the TF-IDF DataFrame\n",
    "filtered_words = tfidf_matrix_beautify.apply(lambda row: filter_words_by_threshold(row, threshold), axis=1)\n",
    "\n",
    "# Create a dataframe to display the filtered words\n",
    "filtered_words_df = pd.DataFrame(filtered_words, columns=[\"tf_idf_filtered_words\"])\n",
    "tfidf_display = pd.concat([edition_software_info_chunked[[\"brand_name\",\"description_clean_chunks\"]], filtered_words_df], axis=1)\n",
    "    \n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(tfidf_display[[\"brand_name\", \"description_clean_chunks\", \"tf_idf_filtered_words\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test tfidf-representation\n",
    "\n",
    "This cell will return the most relevant documents from our dataset based on a comparison of their tf-idf representations and a query. The query can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>TEITOK</td>\n",
       "      <td>based web server. Features Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various</td>\n",
       "      <td>0.086888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>VMR CRE</td>\n",
       "      <td>management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus. Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page. For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features. These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF. A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be. Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility. For example, one might search for all manuscript folio sides from Egypt</td>\n",
       "      <td>0.064380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TEITOK</td>\n",
       "      <td>TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation. TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation, initially developed at the Centro de Linguística da Universidade de Lisboa, later at CELGA-ILTEC, and currently maintained at the ÚFAL institute of Charles University, Prague. The system has a modular design with numerous modules making serving a wide range of different corpus types. Below are some examples of some of those, and the type of corpora TEITOK can deal with. More modules are added frequently, and it is possible to add custom modules as well. Historical Corpora For historical corpora, TEITOK provides the option to have an alignment between the transcription and the facsimile image, it provides the option to work with multiple orthographic realizations to combine several editions of a text into a single XML file, and it provides the option to create a searchable document map to see where in the world several phenomena are more frequent. TEITOK is freely available for anybody who wishes to create richly annotated textual corpora, and runs on any LINUX based web server. Features Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep</td>\n",
       "      <td>0.058220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  \\\n",
       "48     TEITOK   \n",
       "65    VMR CRE   \n",
       "47     TEITOK   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            description_clean_chunks  \\\n",
       "48  based web server. Features Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various   \n",
       "65                                                                                                              management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus. Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page. For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features. These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF. A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be. Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility. For example, one might search for all manuscript folio sides from Egypt   \n",
       "47                                                   TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation. TEITOK is a web-based platform for viewing, creating, and editing corpora with both rich textual mark-up and linguistic annotation, initially developed at the Centro de Linguística da Universidade de Lisboa, later at CELGA-ILTEC, and currently maintained at the ÚFAL institute of Charles University, Prague. The system has a modular design with numerous modules making serving a wide range of different corpus types. Below are some examples of some of those, and the type of corpora TEITOK can deal with. More modules are added frequently, and it is possible to add custom modules as well. Historical Corpora For historical corpora, TEITOK provides the option to have an alignment between the transcription and the facsimile image, it provides the option to work with multiple orthographic realizations to combine several editions of a text into a single XML file, and it provides the option to create a searchable document map to see where in the world several phenomena are more frequent. TEITOK is freely available for anybody who wishes to create richly annotated textual corpora, and runs on any LINUX based web server. Features Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep   \n",
       "\n",
       "    similarity_score  \n",
       "48          0.086888  \n",
       "65          0.064380  \n",
       "47          0.058220  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I want to transcribe and annotate a manuscript'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# Transform the query to TF-IDF space\n",
    "query_tfidf = tfidf_vectorizer.transform([query]) \n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities[0]\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info_chunked[[\"brand_name\", \"description_clean_chunks\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[[\"brand_name\", 'description_clean_chunks', 'similarity_score']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Save vectorisations and the TFIDF-vectorizer\n",
    "\n",
    "Now we can save the vectorisations and the vectorizer to be later used in our RAG-Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"vectorisations/tfidf.npy\"\n",
    "np.save(path, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = os.path.join(os.getcwd(), \"models/tfidf_vectorizer.pickle\")\n",
    "pickle.dump(tfidf_vectorizer, open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 2: Aggregated Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create document representations by aggregating the word2vec embeddings of each word in a description. \n",
    "\n",
    "Word2vec encodes the meaning of the words by capturing their semantic relationships based on the context in which they appear. By aggregating the word2vec embeddings of each word in a description, we can create a document representation that retains the semantic information and provides a more nuanced understanding of the content.\n",
    "\n",
    "From a computational perspective, these representations are shorter and denser than tf-idf representations, making them more suitable for computations such as similarity measures, clustering, or classification tasks. The dense nature of word2vec embeddings allows for efficient storage and faster processing compared to sparse representations like tf-idf. Additionally, because word2vec captures the meaning and context of words, it can provide more meaningful insights into the relationships between different documents or terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "path = os.path.join(current_path, \"models/word2vec-google-news-300.bin\")\n",
    "\n",
    "# Load the model if it is already in our project. If not, download it.\n",
    "if os.path.isfile(path):\n",
    "    print(\"Model found. Loading...\")\n",
    "    word2vec_model = KeyedVectors.load(path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    word2vec_model.save(path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #TODO: Add some preprocessing\\ndef preprocess_word2vec(text: str) -> str:\\n    pass\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #TODO: Add some preprocessing\n",
    "def preprocess_word2vec(text: str) -> str:\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the document representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word2vec_vector(words, model):\n",
    "    words = words.split()\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in model]\n",
    "    \n",
    "    if not valid_words:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the vectors of the valid words to create a document representation\n",
    "    vectors = [model[word] for word in valid_words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply the function to create aggregated vectors\n",
    "word2vec = edition_software_info_chunked['description_preprocessed_chunks'].apply(lambda x: get_word2vec_vector(x, word2vec_model))\n",
    "\n",
    "# Convert the Series of 1D arrays to a 2D numpy array (to calculate the cosine similarity later on)\n",
    "word2vec_array = np.array(word2vec.tolist())\n",
    "len(word2vec_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test Word2Vec Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TEI Critical Apparatus Toolbox</td>\n",
       "      <td>A visualization and quality control tool for people preparing a natively digital TEI critical edition.The TEI Critical Apparatus Toolbox (TEI CAT) is a simple tool offering an easy visualization for TEI XML critical editions. It especially targets the needs of people working on natively-digital editions. Its main purpose is to provide editors with an easy way of visualizing their ongoing work before it is finalized, and also to perform some automatic quality checks on their encoding. Features The Toolbox lets you... [Check your encoding](): offers facilities to display your edition while it is still in the making, and check the consistency of your encoding [Display parallel versions](): choose the sigla of the witnesses, and the different versions of the text, following each chosen witness, will be displayed in parallel columns. [Print an edition]() of a TEI XML edition, with a TEI-to-LateX and PDF transformation [Annotate an image](): lets you easily trace zones on an image to prepare a documentary edition [Get statistics]() on the XML tags effectively used in different parts of your edition, and some word count. A [downloadable version]() is coming soon.</td>\n",
       "      <td>0.649751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Publex</td>\n",
       "      <td>online version. Upon import, Publex parses the data and captures all tags, attributes, and associated attribute values with which the resource has been tagged. For each of these elements and specific combinations, properties such as font style, font size, color, letterspacing, or text indent can now be specified. In addition, elements can be defined as search categories and added to the lemma list. Special characters can be included and displayed via the [KompLett]() font. For this purpose, the TCDH provides a file with all the [non-Unicode characters defined for the Trier dictionaries](). In parallel to the creation of the display rules, the user can look at the Dictionary Preview at any time to see how the defined styling rules are implemented and how the dictionary will look in the publication version. Publex provides different accesses to the dictionary: a lemma list and different search options. The items in the lemma list are sorted alphabetically and can be searched using a search box. Clicking on them displays the linked dictionary item on the screen. The general search can be used to search the dictionary contents in full text, the advanced search offers an AND-linked search over the full text and any number of information fields previously defined as search categories in the styling rules. Once published, the dictionary receives its own URI and also appears, along with</td>\n",
       "      <td>0.637895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>correspSearch</td>\n",
       "      <td>With correspSearch you can search within the metadata of diverse scholarly editions of letters. One can search according to the letter's sender, adressee, as well as place and date of the letter's creation. With correspSearch you can search through indexes of different letter collections (digital or print) by sender, addressee, location written, location sent, and date. To this purpose a website and a technical interface are provided. The web service collects and evaluates TEI-XML data in the ‘Correspondence Metadata Interchange’ format. The web service correspSearch is operated and developed according to the following principles: Reference System : The web service aims to help users with their research by offering a central location to search for letters, and by guiding them to the original publication. Academic Data : The web service is based on the data from letter-indexes of editions or repositories that are edited according to academic criteria. Conceptionaly Open : There is no focus on a particular time period or place. This allows for new kinds of research questions to be explored. Open Access : Data is only collected that is under a free license, and the data from the web service continues to be under a free license and is thus available for further use. Open Interfaces : correspSearch offers technical interfaces that are open and well documented. Other projects can easily query and</td>\n",
       "      <td>0.635463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>VMR CRE</td>\n",
       "      <td>tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility. For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John. A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present. If a region box is captured, a search query can specify to show the region box clips in the result. For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips. Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham. This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component. The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes. The content may then be obtained as EpiDoc influenced TEI.</td>\n",
       "      <td>0.632575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>TEITOK</td>\n",
       "      <td>therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the</td>\n",
       "      <td>0.623739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        brand_name  \\\n",
       "52  TEI Critical Apparatus Toolbox   \n",
       "27                          Publex   \n",
       "14                   correspSearch   \n",
       "66                         VMR CRE   \n",
       "49                          TEITOK   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 description_clean_chunks  \\\n",
       "52                                                                                                                                                                                                                                                 A visualization and quality control tool for people preparing a natively digital TEI critical edition.The TEI Critical Apparatus Toolbox (TEI CAT) is a simple tool offering an easy visualization for TEI XML critical editions. It especially targets the needs of people working on natively-digital editions. Its main purpose is to provide editors with an easy way of visualizing their ongoing work before it is finalized, and also to perform some automatic quality checks on their encoding. Features The Toolbox lets you... [Check your encoding](): offers facilities to display your edition while it is still in the making, and check the consistency of your encoding [Display parallel versions](): choose the sigla of the witnesses, and the different versions of the text, following each chosen witness, will be displayed in parallel columns. [Print an edition]() of a TEI XML edition, with a TEI-to-LateX and PDF transformation [Annotate an image](): lets you easily trace zones on an image to prepare a documentary edition [Get statistics]() on the XML tags effectively used in different parts of your edition, and some word count. A [downloadable version]() is coming soon.   \n",
       "27              online version. Upon import, Publex parses the data and captures all tags, attributes, and associated attribute values with which the resource has been tagged. For each of these elements and specific combinations, properties such as font style, font size, color, letterspacing, or text indent can now be specified. In addition, elements can be defined as search categories and added to the lemma list. Special characters can be included and displayed via the [KompLett]() font. For this purpose, the TCDH provides a file with all the [non-Unicode characters defined for the Trier dictionaries](). In parallel to the creation of the display rules, the user can look at the Dictionary Preview at any time to see how the defined styling rules are implemented and how the dictionary will look in the publication version. Publex provides different accesses to the dictionary: a lemma list and different search options. The items in the lemma list are sorted alphabetically and can be searched using a search box. Clicking on them displays the linked dictionary item on the screen. The general search can be used to search the dictionary contents in full text, the advanced search offers an AND-linked search over the full text and any number of information fields previously defined as search categories in the styling rules. Once published, the dictionary receives its own URI and also appears, along with   \n",
       "14  With correspSearch you can search within the metadata of diverse scholarly editions of letters. One can search according to the letter's sender, adressee, as well as place and date of the letter's creation. With correspSearch you can search through indexes of different letter collections (digital or print) by sender, addressee, location written, location sent, and date. To this purpose a website and a technical interface are provided. The web service collects and evaluates TEI-XML data in the ‘Correspondence Metadata Interchange’ format. The web service correspSearch is operated and developed according to the following principles: Reference System : The web service aims to help users with their research by offering a central location to search for letters, and by guiding them to the original publication. Academic Data : The web service is based on the data from letter-indexes of editions or repositories that are edited according to academic criteria. Conceptionaly Open : There is no focus on a particular time period or place. This allows for new kinds of research questions to be explored. Open Access : Data is only collected that is under a free license, and the data from the web service continues to be under a free license and is thus available for further use. Open Interfaces : correspSearch offers technical interfaces that are open and well documented. Other projects can easily query and   \n",
       "66                                                                    tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility. For example, one might search for all manuscript folio sides from Egypt which include Illuminations and any part of the Gospel of John. A Feature tagged to a manuscript page can also include a region box, marking the area on a folio image where the Feature is present. If a region box is captured, a search query can specify to show the region box clips in the result. For example, a paleographer might choose to capture a set of representative letters for each manuscript and then perform a search for all double column manuscripts with a height of at least 20cm between the II and V centuries, and to ask the query results to show the representative α (alpha) clips. Transcription and Reconciliation Transcription work in the VMR CRE is done using a What You See Is What You Mean (WYSIWYM) web-based editor originally developed by the University of Trier in collaboration with the INTF and ITSEE in Birmingham. This transcription editor has been developed as a plugin for the popular TinyMCE HTML editor component. The editor includes menus and dialogs to assist the researcher with composing a transcription, without asking the transcriber to learn special markup codes. The content may then be obtained as EpiDoc influenced TEI.   \n",
       "49                                                                                                                                  therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the   \n",
       "\n",
       "    similarity_score  \n",
       "52          0.649751  \n",
       "27          0.637895  \n",
       "14          0.635463  \n",
       "66          0.632575  \n",
       "49          0.623739  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I need a search capability for my edition of letters'\n",
    "# Preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "# get vector representation of the query using word2vec\n",
    "query_word2vec = get_word2vec_vector(query, word2vec_model)\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_word2vec = query_word2vec.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_word2vec, word2vec_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info_chunked[[\"brand_name\",\"description_clean_chunks\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean_chunks', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Save the vectorisations\n",
    "\n",
    "Now we can save the vectorisations for later use in RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"vectorisations/word2vec.npy\"\n",
    "np.save(path, np.array(word2vec_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 3: Aggregated FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One downside of pretrained Word2Vec representations is their inability to handle words not contained in their vocabulary. \n",
    "\n",
    "FastText overcomes this issue by representing words not only as embeddings but also as collections of embedded character n-grams. This approach allows FastText to generate meaningful word vectors for previously unseen words, which is particularly useful when dealing with highly specialized terminologies. In our dataset, which contains such specialized language, FastText may therefore offer more reliable performance compared to traditional Word2Vec models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the models\n",
    "\n",
    "FastText provides pre-aligned word vectors, meaning that word vectors for different languages (like German and English) have already been mapped into a common vector space. This allows words with similar meanings across different languages to have similar vector representations, which is crucial when working with multilingual datasets.\n",
    "\n",
    "Since our dataset contains both German and English texts, we need to download the pre-aligned FastText models for these two languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_file_path: str, extract_to: str) -> None:\n",
    "    \"\"\"Unzip a file to a target directory.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Unzipped {zip_file_path} to {extract_to}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error while unzipping the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n"
     ]
    }
   ],
   "source": [
    "# Check if the english model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future. \n",
    "# This might take a while\n",
    "\n",
    "current_path = os.getcwd()\n",
    "models_dir = os.path.join(current_path, \"models\")\n",
    "fasttext_eng_zip_path = os.path.join(models_dir, \"wiki.en.zip\")\n",
    "fasttext_eng_path_vec = os.path.join(models_dir, \"wiki.en.vec\")\n",
    "fasttext_eng_path_bin = os.path.join(models_dir, \"wiki.en.bin\")\n",
    "\n",
    "if os.path.isfile(fasttext_eng_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model(fasttext_eng_path_bin) #load the full model, including subword information.\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\" \n",
    "    # download the models\n",
    "    download_file(url, fasttext_eng_zip_path)\n",
    "    \n",
    "    print(\"Unzipping the file...\")\n",
    "    unzip_file(fasttext_eng_zip_path, models_dir)    \n",
    "\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model(fasttext_eng_path_bin)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_eng.save(fasttext_eng_path_bin)\n",
    "\n",
    "    \n",
    "if aligned_vectors_eng is None:\n",
    "    raise ValueError(\"The FastText model was not loaded properly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n"
     ]
    }
   ],
   "source": [
    "# Check if the german model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future.\n",
    "# This might take a while\n",
    "\n",
    "fasttext_de_path_bin = os.path.join(current_path, \"models/wiki_de_align.bin\")\n",
    "fasttext_de_path_vec = os.path.join(current_path, \"models/wiki_de_align.vec\")\n",
    "\n",
    "if os.path.isfile(fasttext_de_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_de = KeyedVectors.load(fasttext_de_path_bin)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_de_path_vec)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_de = load_word_vectors(fasttext_de_path_vec)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_de.save(fasttext_de_path_bin)\n",
    "    \n",
    "if aligned_vectors_de is None:\n",
    "    raise ValueError(\"The FastText model or vectors were not loaded properly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we loaded both models, let's check if they are properly aligned. \n",
    "\n",
    "To do so, we'll pick an english word, get its english vector representation and return the most similar word vector in the german vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German word vector closest to skyscraper: [('hochhaus', 0.5159210562705994), ('wolkenkratzer', 0.4891827404499054), ('bürohochhaus', 0.4611120820045471), ('hochhausturm', 0.43771031498908997), ('wolkenkratzern', 0.43421125411987305), ('hochhäuser', 0.4312896728515625), ('bürohochhäuser', 0.4262181520462036), ('wolkenkratzers', 0.4224645495414734), ('hochhausbau', 0.4211919605731964), ('„wolkenkratzer', 0.42107880115509033)]\n"
     ]
    }
   ],
   "source": [
    "# get the vector-representation of a random english word\n",
    "word_english = 'skyscraper'\n",
    "word_vector_in_english = aligned_vectors_eng.wv[word_english]\n",
    "\n",
    "print(f\"German word vector closest to {word_english}:\", aligned_vectors_de.most_similar(positive=[word_vector_in_english]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Document Representations\n",
    "\n",
    "Now that we confirmed, that the vector spaces are properly aligned, we can create the document representations. \n",
    "As the pre-aligned german model does not contain subword-infomation, we'll use the subword-information contained in the english model to embedd unknown words in both languages.\n",
    "\n",
    "We'll use the preprocessed descriptions we created earlier.\n",
    "\n",
    "Additionaly, we'll print the words we created new wod vectors using the english subword information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Vector based on Subword Information for: mitttels\n",
      "Created Vector based on Subword Information for: texterkennungsmodellen\n",
      "Created Vector based on Subword Information for: kigestützte\n",
      "Created Vector based on Subword Information for: layoutanalyse\n",
      "Created Vector based on Subword Information for: strukturerkennung\n",
      "Created Vector based on Subword Information for: transkriptionseditor\n",
      "Created Vector based on Subword Information for: kigestützten\n",
      "Created Vector based on Subword Information for: kimodelle\n",
      "Created Vector based on Subword Information for: readsearch\n",
      "Created Vector based on Subword Information for: transkribusinhalte\n",
      "Created Vector based on Subword Information for: erkennungsmodelle\n",
      "Created Vector based on Subword Information for: gdpr\n",
      "Created Vector based on Subword Information for: “lahtech”\n",
      "Created Vector based on Subword Information for: “laytech”\n",
      "Created Vector based on Subword Information for: textsatzsprache\n",
      "Created Vector based on Subword Information for: eingesetztder\n",
      "Created Vector based on Subword Information for: texsystem\n",
      "Created Vector based on Subword Information for: 2e\n",
      "Created Vector based on Subword Information for: 100\n",
      "Created Vector based on Subword Information for: 1997\n",
      "Created Vector based on Subword Information for: opensourcelizenz\n",
      "Created Vector based on Subword Information for: eupl12\n",
      "Created Vector based on Subword Information for: pdfformat\n",
      "Created Vector based on Subword Information for: xmlpublikation\n",
      "Created Vector based on Subword Information for: id0features\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: teidaten\n",
      "Created Vector based on Subword Information for: xslfo\n",
      "Created Vector based on Subword Information for: teielemente\n",
      "Created Vector based on Subword Information for: werdenin\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: teikonformen\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: teielementen\n",
      "Created Vector based on Subword Information for: xsltanweisungen\n",
      "Created Vector based on Subword Information for: teidaten\n",
      "Created Vector based on Subword Information for: publishing“ansatz\n",
      "Created Vector based on Subword Information for: pdfformat\n",
      "Created Vector based on Subword Information for: xslfo\n",
      "Created Vector based on Subword Information for: 8\n",
      "Created Vector based on Subword Information for: ausgabemodulen\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: teicompliant\n",
      "Created Vector based on Subword Information for: codeansicht\n",
      "Created Vector based on Subword Information for: wordartigen\n",
      "Created Vector based on Subword Information for: »autorenansicht«\n",
      "Created Vector based on Subword Information for: sachanmerkungen\n",
      "Created Vector based on Subword Information for: teiauszeichnung\n",
      "Created Vector based on Subword Information for: teikonformen\n",
      "Created Vector based on Subword Information for: xmldatenbank\n",
      "Created Vector based on Subword Information for: existdb\n",
      "Created Vector based on Subword Information for: xmldokumente\n",
      "Created Vector based on Subword Information for: ediarumdb\n",
      "Created Vector based on Subword Information for: eingabewerkzeug\n",
      "Created Vector based on Subword Information for: ediarumdb\n",
      "Created Vector based on Subword Information for: eingabewerkzeug\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: websiteentwicklung\n",
      "Created Vector based on Subword Information for: ediarumweb\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: insttitutionen\n",
      "Created Vector based on Subword Information for: ediarum\n",
      "Created Vector based on Subword Information for: neugermanistische\n",
      "Created Vector based on Subword Information for: ediarum\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_fasttext_vector(row, aligned_vectors_de=None, aligned_vectors_eng=None):\n",
    "    \"\"\"\n",
    "    Calculates the FastText vector representation for a given row.\n",
    "    Parameters:\n",
    "    - row: A row of data.\n",
    "    - aligned_vectors_de: Aligned FastText vectors for the German language. Default is None.\n",
    "    - aligned_vectors_eng: Aligned FastText vectors for the English language. Default is None.\n",
    "    Note:\n",
    "    - If the language is not specified or not supported (only \"en\" and \"de\" are supported), it returns a zero vector.\n",
    "    - If a word in the row's description is not found in the aligned vectors, it tries to create a vector based on english subword information.\n",
    "    - If no vectors are found, it returns a zero vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # default size to avoid errors if vectors are None\n",
    "    vector_size = aligned_vectors_de.vector_size if aligned_vectors_de else 300\n",
    "    \n",
    "    # check if language is valid\n",
    "    lang = row.get(\"description_lang\")\n",
    "    if pd.isna(lang) or lang not in [\"en\", \"de\"]:\n",
    "        return np.zeros(vector_size) #Maybe rather use none?\n",
    "    \n",
    "    words = row.get(\"description_preprocessed_chunks\", \"\").split()\n",
    "    vectors = []\n",
    "\n",
    "    # process based on language\n",
    "    if lang == \"de\" and aligned_vectors_de:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_de[word])\n",
    "            except KeyError:\n",
    "                print(f\"Created Vector based on Subword Information for: {word}\")                \n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "                #vectors.append(np.zeros(vector_size))\n",
    "                \n",
    "    elif lang == \"en\" and aligned_vectors_eng:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "            except KeyError:\n",
    "                print(f\"Missing Vector for: {word}\")\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros\n",
    "\n",
    "\n",
    "# Apply the function to create aggregated vectors\n",
    "fasttext = edition_software_info_chunked.apply(lambda x: get_fasttext_vector(x, aligned_vectors_de, aligned_vectors_eng), axis=1)\n",
    "\n",
    "# Convert the Series of 1D arrays to a 2D numpy array (to calculate the cosine similarity later on)\n",
    "fasttext_array = np.array(word2vec.tolist())\n",
    "len(fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test the FastText Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>TEITOK</td>\n",
       "      <td>from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the original project.</td>\n",
       "      <td>0.147178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Comparo</td>\n",
       "      <td>(Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD]() database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital']() , further settings can be made in order</td>\n",
       "      <td>0.135922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>CATview</td>\n",
       "      <td>the respective text segment. The current scroll position in the text is marked by a scroll spy in the overview bar. Highlighting Search Results CATview visualizes search hits by coloring (yellow) the segments matching the search request. Easy Restriction of the Text under Consideration CATview offers a comfortable selection of consecutive text segments by drawing a box around them. Statistical Analysis CATview allows statistical analysis with respect to text excerpts.</td>\n",
       "      <td>0.108582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>TEITOK</td>\n",
       "      <td>therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the</td>\n",
       "      <td>0.108265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>TEITOK</td>\n",
       "      <td>based web server. Features Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various</td>\n",
       "      <td>0.100146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  \\\n",
       "50     TEITOK   \n",
       "34    Comparo   \n",
       "60    CATview   \n",
       "49     TEITOK   \n",
       "48     TEITOK   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            description_clean_chunks  \\\n",
       "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the original project.   \n",
       "34                                                                                                                                                         (Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD]() database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital']() , further settings can be made in order   \n",
       "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          the respective text segment. The current scroll position in the text is marked by a scroll spy in the overview bar. Highlighting Search Results CATview visualizes search hits by coloring (yellow) the segments matching the search request. Easy Restriction of the Text under Consideration CATview offers a comfortable selection of consecutive text segments by drawing a box around them. Statistical Analysis CATview allows statistical analysis with respect to text excerpts.   \n",
       "49                                                                                                                                                                                                                                             therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the   \n",
       "48  based web server. Features Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various   \n",
       "\n",
       "    similarity_score  \n",
       "50          0.147178  \n",
       "34          0.135922  \n",
       "60          0.108582  \n",
       "49          0.108265  \n",
       "48          0.100146  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'I want to transcribe a manuscript'\n",
    "\n",
    "# check the queries language, to decide which model to use.\n",
    "query_lang = identify_language(query).replace(\"__label__\",\"\") if not query == '' else np.nan\n",
    "\n",
    "# preprocess the query\n",
    "query = preprocess(stopwords_combined, query)\n",
    "\n",
    "# create a series that can be used as an input in the get_fasttext_vector function. Ignore the key names.\n",
    "query_information = pd.Series({\n",
    "    \"description_lang\": query_lang,\n",
    "    \"description_preprocessed_chunks\": query\n",
    "})\n",
    "\n",
    "# get vector representation of the query using fasttext\n",
    "query_fasttext= get_fasttext_vector(query_information, aligned_vectors_de, aligned_vectors_eng)\n",
    "\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_fasttext = query_fasttext.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_fasttext, fasttext_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info_chunked[[\"brand_name\",\"description_clean_chunks\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 3 description, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean_chunks', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Save the vectorisations\n",
    "\n",
    "Now we save the vectors we created for later use in RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"vectorisations/fasttext.npy\"\n",
    "np.save(path, np.array(fasttext_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization 4: SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SBERT**: \n",
    "\n",
    "In contrast to the output of regular embedding models, word vectors created using BERT models are contextualized. This means that they can generate multiple word embeddings for the same word, depending on the meaning it takes on in a certain context. \"Bank,\" for example, will have two different representations—one as a river's shore and another as a place to store money.\n",
    "\n",
    "Since BERT models by themselves are notoriously bad for semantic similarity tasks applied to sentence- or paragraph-level vectors, we will use  SBERT to create document representations instead. These models also create contextualized embeddings but are specifically trained with semantic similarity in mind: A triplet loss function is used to minimize the distance between an \"anchor point\" and a positive sample while maximizing the distance to a negative sample. This forces sentence transformers to produce a vector space where semantically similar sentences are close together, while sentence embeddings of semantically dissimilar sentences are far apart.\n",
    "\n",
    "Specifically, we will use the sentence-transformers library, which builds on the original [SBERT paper](https://arxiv.org/abs/1908.10084).\n",
    "\n",
    "**Reranking:**\n",
    "\n",
    "As it is suggested in the documentation of the sentence transformer module, the similarities calculated using the SBERT-representations will be reranked using a Cross-Encoder. \n",
    "Cross-Encoders tackle the task of calculating similarity as a classification task, classifying two sentences as either \"relevant\" or \"not relevant\" in relation to one another. We'll not rerank the all similarity scores, but only the entries most similar to our query.\n",
    "\n",
    "Detailed information on this retrieve & re-rank process and its benefits can be found [here](https://www.sbert.net/examples/applications/retrieve_rerank/README.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Sentence Transformer from drive...\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sentence_transformers\n",
    "# Download SBERT model or load it from drive\n",
    "sbert_path = os.path.join(os.getcwd(),\"models/sbert\")\n",
    "downloaded = os.path.isdir(sbert_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Sentence Transformer...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    sbert_model.save(sbert_path)\n",
    "else:\n",
    "    print(\"Load Sentence Transformer from drive...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer(sbert_path)\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Cross Encoder from drive...\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Download Cross Encoder or load it from drive\n",
    "\n",
    "cross_encoder_path = os.path.join(os.getcwd(), \"models/cross\")\n",
    "downloaded = os.path.isdir(cross_encoder_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Cross Encoder...\")\n",
    "    cross_encoder_model = sentence_transformers.CrossEncoder(\"corrius/cross-encoder-mmarco-mMiniLMv2-L12-H384-v1\")\n",
    "    cross_encoder_model.save(cross_encoder_path)\n",
    "else:\n",
    "    print(\"Load Cross Encoder from drive...\")\n",
    "    cross_encoder_model = sentence_transformers.CrossEncoder(cross_encoder_path)\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Document Representations\n",
    "\n",
    "As creating SBERT embeddings may take some time, we'll first create a function to parallelize the process. The parameter num_chunks can be increased to control the number of concurrent embedding processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to control tokenizers parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "def get_sbert_embeddings(text, model):\n",
    "    \"\"\"\n",
    "    Get Sentence-BERT embeddings for a given text using a specified model.\n",
    "    Parameters:\n",
    "    text (str): The input text to encode.\n",
    "    model: The Sentence-BERT model to use for encoding.\n",
    "    Returns:\n",
    "    numpy.ndarray: The Sentence-BERT embeddings for the input text.\n",
    "    \"\"\"\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "    \n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return default_embedding    \n",
    "    return model.encode(text, convert_to_tensor=False)\n",
    "\n",
    "\n",
    "def compute_sbert_embeddings_in_parallel(df, text_column, model, num_chunks=3):\n",
    "    \"\"\"\n",
    "    Compute SBERT embeddings in parallel for the given text column in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the text data.\n",
    "        text_column (str): The column name of the text data.\n",
    "        model (Any): SBERT model for generating embeddings.\n",
    "        num_chunks (int): The number of chunks to split the data for parallel processing.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the computed SBERT embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to process each chunk of data\n",
    "    def process_chunk(chunk):\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in chunk:\n",
    "            embeddings.append(get_sbert_embeddings(text, model))\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    # Split DataFrame into chunks for parallel processing\n",
    "    df_chunks = np.array_split(df[text_column], num_chunks)\n",
    "\n",
    "    # Process each chunk in parallel\n",
    "    results = Parallel(n_jobs=num_chunks)(\n",
    "        delayed(process_chunk)(chunk) for chunk in df_chunks\n",
    "    )\n",
    "\n",
    "    # Combine and return the results\n",
    "    return np.concatenate(results).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewinzen/opt/anaconda3/envs/retrieval_augmented_generation/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# get vector representation of the query using the sbert model\n",
    "sbert_array = compute_sbert_embeddings_in_parallel(edition_software_info_chunked, \"description_clean_chunks\", sbert_model, num_chunks=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test SBERT-Embeddings\n",
    "\n",
    "In this cell, we can test similarities between a query and the SBERT-Embeddings, without reranking of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Comparo</td>\n",
       "      <td>A tool for synchronously comparing an almost unlimited number of versions of a text. Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text. The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability. Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order</td>\n",
       "      <td>0.583317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Comparo</td>\n",
       "      <td>(Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD]() database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital']() , further settings can be made in order</td>\n",
       "      <td>0.454486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Transcribo</td>\n",
       "      <td>and have resulted in a very differentiated and practically tested tool. Some of the most important features of Transcribo 1. Fine-grained markup of microgenetic facts (at document, page, sentence/line, word, and graph levels) 2. Markup of complex interrelated phenomena/change operations via relations (page and cross-page) 3. Correction function for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4. OCR functionality for automatic reading of typescripts or prints (Tesseract) 5. Navigation perspective: interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6. Structural perspective: overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7. Module for defining and labeling text states or text layers. Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems. Projects using Transcribo [Arthur Schnitzler Digital]() [The Augsburg Master Builder’s Ledgers]() [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)]() [Digital Marburg Büchner Edition]() [Johann Caspar Lavater]() [Kurt Schwitters' Intermedia Networks of the Avant-garde]() [Stefan Heym: “Ahasver”]() [Digitalization of the Plock Bible, Old High German Dictionary]() [Old High German</td>\n",
       "      <td>0.432837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tesseract OCR</td>\n",
       "      <td>Tesseract ist eine Software zur Texterkennung. Mehr als 100 Sprachen und Sprachvarianten werden unterstützt, zudem verschiedene Schriften / Schriftsysteme: lateinische Antiqua, Fraktur, Devanagari (indische Schrift), chinesische, arabische, griechische, hebräische, kyrillische Schrift.</td>\n",
       "      <td>0.427719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Versioning Machine</td>\n",
       "      <td>A framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI)The Versioning Machine is a framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI) Guidelines, and is P5 compatible. While the VM provides for features typically found in critical editions, such as annotation and introductory material, it also takes advantage of the opportunities afforded by electronic publication to allow for the comparison diplomatic versions of witnesses, and the ability to easily compare an image of the manuscript with a diplomatic version. VM 5.0 adds a number of new features, including the ability to resize and reorganize text panels, panning and zooming in the image viewer, and text-audio interlinking. The Versioning Machine’s underlying code has also been completely revised to support enhanced features. The Versioning Machine is a useful tool for textual editors, providing an environment that allows editors to immediately see the consequences of their editorial decisions. The platform also has applications in teaching, translation, and digital publication. The many uses of the Versioning Machine are illustrated in the new VM IN USE section. The Versioning Machine can be used locally on a Mac or a PC, or it can be mounted on the Internet for public access. The documentation provided with the software not only provides information about</td>\n",
       "      <td>0.421393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            brand_name  \\\n",
       "33             Comparo   \n",
       "34             Comparo   \n",
       "31          Transcribo   \n",
       "6        Tesseract OCR   \n",
       "62  Versioning Machine   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         description_clean_chunks  \\\n",
       "33                                                                                                                                                                                                                                     A tool for synchronously comparing an almost unlimited number of versions of a text. Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text. The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability. Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order   \n",
       "34                                                                                                                                                                                                                                                                                                                      (Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD]() database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital']() , further settings can be made in order   \n",
       "31  and have resulted in a very differentiated and practically tested tool. Some of the most important features of Transcribo 1. Fine-grained markup of microgenetic facts (at document, page, sentence/line, word, and graph levels) 2. Markup of complex interrelated phenomena/change operations via relations (page and cross-page) 3. Correction function for automated comparison of A and B files from different users to ensure error-free transcription and annotation ('double-blind process') 4. OCR functionality for automatic reading of typescripts or prints (Tesseract) 5. Navigation perspective: interface to the FuD database (linking facsimiles with metadata recorded in FuD) 6. Structural perspective: overview of a text section of any size (synopsis of different graphical representations: Facsimile, transcription, annotations) and possibility of depositing cross-page phenomena. 7. Module for defining and labeling text states or text layers. Technical requirements The application was implemented in the Eclipse integrated development environment. It is a rich client application that reuses parts of the development environment and was developed in the Java programming language. We currently provide a version for Windows and macOS operating systems. Projects using Transcribo [Arthur Schnitzler Digital]() [The Augsburg Master Builder’s Ledgers]() [Digitale Edition and Analysis of the Medulla Gestorum Treverensium by Johann Enen (1514)]() [Digital Marburg Büchner Edition]() [Johann Caspar Lavater]() [Kurt Schwitters' Intermedia Networks of the Avant-garde]() [Stefan Heym: “Ahasver”]() [Digitalization of the Plock Bible, Old High German Dictionary]() [Old High German   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Tesseract ist eine Software zur Texterkennung. Mehr als 100 Sprachen und Sprachvarianten werden unterstützt, zudem verschiedene Schriften / Schriftsysteme: lateinische Antiqua, Fraktur, Devanagari (indische Schrift), chinesische, arabische, griechische, hebräische, kyrillische Schrift.   \n",
       "62                                                                                                                                                                                                              A framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI)The Versioning Machine is a framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI) Guidelines, and is P5 compatible. While the VM provides for features typically found in critical editions, such as annotation and introductory material, it also takes advantage of the opportunities afforded by electronic publication to allow for the comparison diplomatic versions of witnesses, and the ability to easily compare an image of the manuscript with a diplomatic version. VM 5.0 adds a number of new features, including the ability to resize and reorganize text panels, panning and zooming in the image viewer, and text-audio interlinking. The Versioning Machine’s underlying code has also been completely revised to support enhanced features. The Versioning Machine is a useful tool for textual editors, providing an environment that allows editors to immediately see the consequences of their editorial decisions. The platform also has applications in teaching, translation, and digital publication. The many uses of the Versioning Machine are illustrated in the new VM IN USE section. The Versioning Machine can be used locally on a Mac or a PC, or it can be mounted on the Internet for public access. The documentation provided with the software not only provides information about   \n",
       "\n",
       "    similarity_score  \n",
       "33          0.583317  \n",
       "34          0.454486  \n",
       "31          0.432837  \n",
       "6           0.427719  \n",
       "62          0.421393  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#query = 'Ich muss verschiedene Stadien eines Manuskripts vergleichen und verschiedene Versionen desselben Schriftstückes nebeneinander darstellen'\n",
    "query = 'I want to compare different versions of a text to see how it developed over time'\n",
    "\n",
    "query_sbert = get_sbert_embeddings(query, sbert_model)\n",
    "\n",
    "# Reshape the query vector to be a 2D array with one row\n",
    "query_sbert = query_sbert.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity between the query and the documents\n",
    "similarities = cosine_similarity(query_sbert, sbert_array)\n",
    "\n",
    "similarity_df = pd.DataFrame({\n",
    "    'similarity_score': similarities.flatten()\n",
    "})\n",
    "\n",
    "result_df = pd.concat([edition_software_info_chunked[[\"brand_name\",\"description_clean_chunks\"]], similarity_df], axis=1)\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# print the top 5 descriptions, that might be relevant to our query\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(result_df_sorted[['brand_name','description_clean_chunks', 'similarity_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Test reranked SBERT-Representations\n",
    "\n",
    "Now, we can apply reranking to the similariites calculated above and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>initial_similarity_score</th>\n",
       "      <th>rerank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comparo</td>\n",
       "      <td>A tool for synchronously comparing an almost unlimited number of versions of a text. Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text. The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability. Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order</td>\n",
       "      <td>0.583317</td>\n",
       "      <td>3.694656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comparo</td>\n",
       "      <td>(Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD]() database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital']() , further settings can be made in order</td>\n",
       "      <td>0.454486</td>\n",
       "      <td>1.215679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CollateX</td>\n",
       "      <td>CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to 1. read multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared, 1. identify similarities of and differences between the versions (including moved/transposed segments) by aligning tokens, and 1. output the alignment results in a variety of formats for further processing , for instance 1. to support the production of a critical apparatus or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is</td>\n",
       "      <td>0.337188</td>\n",
       "      <td>-0.642481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Versioning Machine</td>\n",
       "      <td>A framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI)The Versioning Machine is a framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI) Guidelines, and is P5 compatible. While the VM provides for features typically found in critical editions, such as annotation and introductory material, it also takes advantage of the opportunities afforded by electronic publication to allow for the comparison diplomatic versions of witnesses, and the ability to easily compare an image of the manuscript with a diplomatic version. VM 5.0 adds a number of new features, including the ability to resize and reorganize text panels, panning and zooming in the image viewer, and text-audio interlinking. The Versioning Machine’s underlying code has also been completely revised to support enhanced features. The Versioning Machine is a useful tool for textual editors, providing an environment that allows editors to immediately see the consequences of their editorial decisions. The platform also has applications in teaching, translation, and digital publication. The many uses of the Versioning Machine are illustrated in the new VM IN USE section. The Versioning Machine can be used locally on a Mac or a PC, or it can be mounted on the Internet for public access. The documentation provided with the software not only provides information about</td>\n",
       "      <td>0.421393</td>\n",
       "      <td>-1.684034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tesseract OCR</td>\n",
       "      <td>Tesseract ist eine Software zur Texterkennung. Mehr als 100 Sprachen und Sprachvarianten werden unterstützt, zudem verschiedene Schriften / Schriftsysteme: lateinische Antiqua, Fraktur, Devanagari (indische Schrift), chinesische, arabische, griechische, hebräische, kyrillische Schrift.</td>\n",
       "      <td>0.427719</td>\n",
       "      <td>-2.073432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            brand_name  \\\n",
       "0              Comparo   \n",
       "1              Comparo   \n",
       "14            CollateX   \n",
       "4   Versioning Machine   \n",
       "3        Tesseract OCR   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             description_clean_chunks  \\\n",
       "0                          A tool for synchronously comparing an almost unlimited number of versions of a text. Comparo is a tool for synchronously comparing an almost unlimited number of versions of a text. The comparison unit is the 'sentence'. Since all versions of a text are compared with each other in a previously defined order at the same time, Comparo is a very clear tool that can be used for collation in a time-efficient manner. The comparison parameters can also be comprehensively configured by the user so that they can be individually tailored to the respective text type and readjusted if necessary in order to optimize the comparison result (for example, individual 'high-frequency' words can be defined to be explicitly excluded from the comparison). Following the suggestion that is automatically generated on the basis of the default settings, the user can manually modify, add or remove each assignment. Features such as 'Bookmark', 'Comment', 'Search', 'Mark as done' as well as various evaluation options in clear list form (e.g. display of all unconnected elements) also increase usability. Use in philological research Comparo was developed by the TCDH as part of the binational research project [“Arthur Schnitzler digital. Digitale historisch-kritische Edition (Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order   \n",
       "1                                                                                                           (Werke 1905–1931)“]() and is used there to create a 'microgenesis' view, which arranges all text-genetically relevant documents of a work by Arthur Schnitzler in a single view in the order in which they were created and parallels the corresponding sentences in different versions with the connections which have been stored via Comparo. This way, a highly complex and yet clear view of all changes that the text has undergone in the course of its creation is created and thus not only answers exciting questions from a philological point of view, such as: Which passages were essentially already present in the first sketch of a work? Which sentences have meanwhile been planned elsewhere? Which sentences have been dropped or newly added in the course of the text's genesis and which have meanwhile been discarded and then restituted? Which passages were once planned completely differently in terms of content (e.g. alternative endings)? In which areas did Schnitzler change and file a lot and which areas have remained almost unchanged from the first note to the later print? The information generated in Comparo is stored in an [FuD]() database. With the help of this database it is possible to work out a detailed presentation in the form of a web view. In the website created for the project ['Arthur Schnitzler digital']() , further settings can be made in order   \n",
       "14          CollateX is a software to (a.) read multiple versions of a text, (b.) identify differences by aligning tokens, and (c.) output the alignment results for further processing, for instance (d.) to support the production of a critical apparatus or the stemmatical analysis of a text's genesis.[CollateX]() is a software to 1. read multiple (≥ 2) versions of a text , splitting each version into parts (tokens) to be compared, 1. identify similarities of and differences between the versions (including moved/transposed segments) by aligning tokens, and 1. output the alignment results in a variety of formats for further processing , for instance 1. to support the production of a critical apparatus or the stemmatical analysis of a text's genesis. It resembles software used to compute differences between files (e.g. [diff]()) or tools for [sequence alignment]() which are commonly used in Bioinformatics. While CollateX shares some of the techniques and algorithms with those tools, it mainly aims for a flexible and configurable approach to the problem of finding similarities and differences in texts, sometimes trading computational soundness or complexity for the user's ability to influence results. As such it is primarily designed for use cases in disciplines like [Philology]() or – more specifically – the field of [Textual Criticism]() where the assessment of findings is based on interpretation and therefore can be supported by computational means but is   \n",
       "4   A framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI)The Versioning Machine is a framework and an interface for displaying multiple versions of text encoded according to the Text Encoding Initiative (TEI) Guidelines, and is P5 compatible. While the VM provides for features typically found in critical editions, such as annotation and introductory material, it also takes advantage of the opportunities afforded by electronic publication to allow for the comparison diplomatic versions of witnesses, and the ability to easily compare an image of the manuscript with a diplomatic version. VM 5.0 adds a number of new features, including the ability to resize and reorganize text panels, panning and zooming in the image viewer, and text-audio interlinking. The Versioning Machine’s underlying code has also been completely revised to support enhanced features. The Versioning Machine is a useful tool for textual editors, providing an environment that allows editors to immediately see the consequences of their editorial decisions. The platform also has applications in teaching, translation, and digital publication. The many uses of the Versioning Machine are illustrated in the new VM IN USE section. The Versioning Machine can be used locally on a Mac or a PC, or it can be mounted on the Internet for public access. The documentation provided with the software not only provides information about   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Tesseract ist eine Software zur Texterkennung. Mehr als 100 Sprachen und Sprachvarianten werden unterstützt, zudem verschiedene Schriften / Schriftsysteme: lateinische Antiqua, Fraktur, Devanagari (indische Schrift), chinesische, arabische, griechische, hebräische, kyrillische Schrift.   \n",
       "\n",
       "    initial_similarity_score  rerank_score  \n",
       "0                   0.583317      3.694656  \n",
       "1                   0.454486      1.215679  \n",
       "14                  0.337188     -0.642481  \n",
       "4                   0.421393     -1.684034  \n",
       "3                   0.427719     -2.073432  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort the results by similarity scores calculated using the sbert-representations in descending order\n",
    "result_df_sorted = result_df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# Select the top N (e.g., top 20) results for reranking\n",
    "n = 20\n",
    "top_n_results = result_df_sorted.head(n)\n",
    "\n",
    "# Prepare the input pairs (query, document description) for the reranking model\n",
    "model_inputs = [[query, description] for description in top_n_results[\"description_clean_chunks\"]]\n",
    "\n",
    "# Predict the relevance scores using the CrossEncoder model\n",
    "scores = cross_encoder_model.predict(model_inputs)\n",
    "\n",
    "# Create a DataFrame for reranked results\n",
    "reranked_results = pd.DataFrame({\n",
    "    \"brand_name\": top_n_results[\"brand_name\"].values,\n",
    "    \"description_clean_chunks\": top_n_results[\"description_clean_chunks\"].values,\n",
    "    \"initial_similarity_score\": top_n_results[\"similarity_score\"].values,\n",
    "    \"rerank_score\": scores\n",
    "})\n",
    "\n",
    "# Sort the results by rerank score in descending order\n",
    "reranked_results_sorted = reranked_results.sort_values(by='rerank_score', ascending=False)\n",
    "\n",
    "# Display the top reranked results\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(reranked_results_sorted[['brand_name', 'description_clean_chunks', 'initial_similarity_score', 'rerank_score']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can manually inspect the results. \n",
    "\n",
    "After testing it using different queries, we can conclude that the reranking provides more relevant results, while taking a lot longer than just  calculating the similarities across vectors. \n",
    "\n",
    "Further testing should be conducted by domain experts to evaluate, if the trade-off between performance and quality is justifyable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Save the vectorisations\n",
    "\n",
    "Now we save the vectors we created for later use in RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"vectorisations/sbert.npy\"\n",
    "np.save(path, np.array(sbert_array))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import sentence_transformers\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import fasttext\n",
    "from langchain_ollama import ChatOllama\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the vectorizations we created in the notebook `1_vectorisation` to provide relevant information from the software repository in response to a user query. \n",
    "To do so, we'll:\n",
    "\n",
    "1. Load the models, the data and the vector representations of said data. \n",
    "\n",
    "2. Load and prepare the LLM we want to use for RAG. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the models, the chunked descriptions from our software repository and the vector representations we prepared in the notebook `1_vectorisation.`\n",
    "\n",
    "If the data and vectorizations have not yet been created, please run the `1_vectorisation` notebook first. Note that this process may take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Models\n",
    "\n",
    "First we'll load the different models, as we need them to vectorize the queries.\n",
    "\n",
    "Once we loaded the models, we'll create functions to vectorize the queries using the loaded models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the TFIDF-Vectorizer + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), \"models/tfidf_vectorizer.pickle\")\n",
    "with open(path, \"rb\") as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "    \n",
    "def get_tfidf_vector(query):\n",
    "    return tfidf_vectorizer.transform([query])\n",
    "    \n",
    "print(\"Model sucessfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load the Word2Vec model + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "path = os.path.join(current_path, \"models/word2vec-google-news-300.bin\")\n",
    "\n",
    "# Load the model if it is already in our project. If not, download it.\n",
    "if os.path.isfile(path):\n",
    "    print(\"Model found. Loading...\")\n",
    "    word2vec_model = KeyedVectors.load(path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    word2vec_model.save(path)\n",
    "print(\"Model loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to vectorize the query    \n",
    "def get_word2vec_vector(query, model):\n",
    "    words = query.split()\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in model]\n",
    "\n",
    "    if not valid_words:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    # Average the vectors of the valid words to create a document representation\n",
    "    vectors = [model[word] for word in valid_words]\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load the Fast-Text models + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE HELPER FUNCTIONS\n",
    "\n",
    "# define functions to load FastText models\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"File was already downloaded.\")\n",
    "            return None\n",
    "        \n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"The file has been downloaded and saved as: {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred while downloading the file: {e}\")\n",
    "\n",
    "# define function to unzip models\n",
    "def unzip_file(zip_file_path: str, extract_to: str) -> None:\n",
    "    \"\"\"Unzip a file to a target directory.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Unzipped {zip_file_path} to {extract_to}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error while unzipping the file: {e}\")\n",
    "\n",
    "# define a function to load word vectors from a file\n",
    "def load_word_vectors(file_path: str):\n",
    "    \"\"\"Load word vectors from a file.\"\"\"\n",
    "    try:\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        print(\"Vectors loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the vectors: {e}\")\n",
    "        return None\n",
    "\n",
    "### LOAD MODELS\n",
    "\n",
    "# download or load the fasttext model for language detection\n",
    "langident_path = os.path.join(os.getcwd(), 'models/lid.176.bin')\n",
    "\n",
    "if os.path.isfile(langident_path):\n",
    "    print(\"Model found.\")\n",
    "    language_detection_model = fasttext.load_model(langident_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    # download the model\n",
    "    download_file(url, langident_path)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    language_detection_model = fasttext.load_model(langident_path)\n",
    "    \n",
    "language_identification_model = fasttext.load_model(langident_path)\n",
    "\n",
    "print(\"Language identification model loaded sucessfully.\")\n",
    "\n",
    "# Check if the english model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future. \n",
    "# This might take a while\n",
    "\n",
    "current_path = os.getcwd()\n",
    "models_dir = os.path.join(current_path, \"models\")\n",
    "fasttext_eng_zip_path = os.path.join(models_dir, \"wiki.en.zip\")\n",
    "fasttext_eng_path_vec = os.path.join(models_dir, \"wiki.en.vec\")\n",
    "fasttext_eng_path_bin = os.path.join(models_dir, \"wiki.en.bin\")\n",
    "\n",
    "if os.path.isfile(fasttext_eng_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model(fasttext_eng_path_bin) #load the full model, including subword information.\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\" \n",
    "    # download the models\n",
    "    download_file(url, fasttext_eng_zip_path)\n",
    "    \n",
    "    print(\"Unzipping the file...\")\n",
    "    unzip_file(fasttext_eng_zip_path, models_dir)    \n",
    "\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model.load(fasttext_eng_path_bin)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_eng.save(fasttext_eng_path_bin)\n",
    "\n",
    "    \n",
    "if aligned_vectors_eng is None:\n",
    "    raise ValueError(\"The FastText model was not loaded properly.\")\n",
    "\n",
    "print(\"English model loaded sucessfully.\")\n",
    "\n",
    "# Check if the german model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future.\n",
    "# This might take a while\n",
    "\n",
    "fasttext_de_path_bin = os.path.join(current_path, \"models/wiki_de_align.bin\")\n",
    "fasttext_de_path_vec = os.path.join(current_path, \"models/wiki_de_align.vec\")\n",
    "\n",
    "if os.path.isfile(fasttext_de_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_de = KeyedVectors.load(fasttext_de_path_bin)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_de_path_vec)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_de = load_word_vectors(fasttext_de_path_vec)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_de.save(fasttext_de_path_bin)\n",
    "    \n",
    "if aligned_vectors_de is None:\n",
    "    raise ValueError(\"The FastText model or vectors were not loaded properly.\")\n",
    "\n",
    "print(\"German model loaded sucessfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to identify a query's language\n",
    "def identify_language(query):\n",
    "    \"\"\"\n",
    "    Identifies the language of the given query.\n",
    "    Parameters:\n",
    "    query (str): The text to be analyzed for language identification.\n",
    "    \"\"\"\n",
    "    \n",
    "    lang_detected = language_identification_model.predict(query)\n",
    "\n",
    "    return lang_detected[0][0].replace(\"__label__\", \"\")\n",
    "\n",
    "\n",
    "# create function to embed query (based on language)\n",
    "def get_fasttext_vector(query, aligned_vectors_de=None, aligned_vectors_eng=None):\n",
    "    \"\"\"\n",
    "    Calculates the FastText vector representation for a given query.\n",
    "    Parameters:\n",
    "    - text: A text.\n",
    "    - aligned_vectors_de: Aligned FastText vectors for the German language.\n",
    "    - aligned_vectors_eng: Aligned FastText vectors for the English language.\n",
    "    Note:\n",
    "    - If the language is not specified or not supported (only \"en\" and \"de\" are supported), it returns a zero vector.\n",
    "    - If a word in the row's description is not found in the aligned vectors, it tries to create a vector based on english subword information.\n",
    "    - If no vectors are found, it returns a zero vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    # default size to avoid errors if vectors are None\n",
    "    vector_size = aligned_vectors_de.vector_size if aligned_vectors_de else 300\n",
    "    \n",
    "    # check if language is valid\n",
    "    lang = identify_language(query)\n",
    "    if pd.isna(lang) or lang not in [\"en\", \"de\"]:\n",
    "        return np.zeros(vector_size) # Maybe rather use none?\n",
    "    \n",
    "    words = query.split()\n",
    "    vectors = []\n",
    "\n",
    "    # process based on language\n",
    "    if lang == \"de\" and aligned_vectors_de:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_de[word])\n",
    "            except KeyError:\n",
    "                print(f\"Created Vector based on Subword Information for: {word}\")                \n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "                #vectors.append(np.zeros(vector_size))\n",
    "                \n",
    "    elif lang == \"en\" and aligned_vectors_eng:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "            except KeyError:\n",
    "                print(f\"Missing Vector for: {word}\")\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the SBERT and Cross Encoder models + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SBERT model or load them from drive\n",
    "sbert_path = os.path.join(os.getcwd(),\"models/sbert\")\n",
    "downloaded = os.path.isdir(sbert_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Sentence Transformer...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    sbert_model.save(sbert_path)\n",
    "else:\n",
    "    print(\"Load Sentence Transformer from drive...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer(sbert_path)\n",
    "    \n",
    "print(\"SBERT model loaded sucessfully\")\n",
    "    \n",
    "# Download Cross Encoder or load them from drive\n",
    "\n",
    "cross_encoder_path = os.path.join(os.getcwd(), \"models/cross\")\n",
    "downloaded = os.path.isdir(cross_encoder_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Cross Encoder...\")\n",
    "    cross_encoder_model = sentence_transformers.CrossEncoder(\"corrius/cross-encoder-mmarco-mMiniLMv2-L12-H384-v1\")\n",
    "    sbert_model.save(cross_encoder_path)\n",
    "else:\n",
    "    print(\"Load Cross Encoder from drive...\")\n",
    "    cross_encoder_model = sentence_transformers.SentenceTransformer(sbert_path)\n",
    "    \n",
    "print(\"Cross encoder loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to embedd the query\n",
    "def get_sbert_vector(query, model):\n",
    "    \"\"\"\n",
    "    Get Sentence-BERT embeddings for a given text using a specified model.\n",
    "    Parameters:\n",
    "    text (str): The input text to encode.\n",
    "    model: The Sentence-BERT model to use for encoding.\n",
    "    Returns:\n",
    "    numpy.ndarray: The Sentence-BERT embeddings for the input text.\n",
    "    \"\"\"\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "    \n",
    "    if pd.isna(query) or query.strip() == '':\n",
    "        return default_embedding    \n",
    "    return model.encode(query, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load chunked descriptions\n",
    "\n",
    "Next, we load the chunked descriptions from the software repository we prepared in the notebook `1_vectorisation`. We'll need them to supply text to the user and the llm used for the RAG-process.\n",
    "\n",
    "We'll only load four columns. \n",
    "\n",
    "1. **description_clean_chunks:** The chunk.\n",
    "2. **description_preprocessed_chunks:** The preprocessed chunk (lowercase + removed punctuation and stopwords).\n",
    "3. **description:** The complete description from which the chunk was extracted.\n",
    "4. **brand_name:** The name of the software described by the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "dataset_path = os.path.join(os.getcwd(), \"data/edition_software_info_chunked.csv\")\n",
    "columns = [\"brand_name\", \"description\", \"description_clean_chunks\", \"description_preprocessed_chunks\"]\n",
    "df = pd.read_csv(dataset_path ,skipinitialspace=True, usecols=columns)\n",
    "\n",
    "# replace missing values with empty strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# display the first row\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Vectors\n",
    "\n",
    "Finally we can load the vector representations of the chunks we created in `1_vectorisation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_path = os.path.join(os.getcwd(), \"vectorisations\")\n",
    "\n",
    "chunks_tfidf = np.load(os.path.join(vectors_path, \"tfidf.npy\"), allow_pickle=True)\n",
    "    \n",
    "chunks_word2vec = np.load(os.path.join(vectors_path, \"word2vec.npy\")).tolist()\n",
    "\n",
    "chunks_fasttext = np.load(os.path.join(vectors_path, \"fasttext.npy\")).tolist()\n",
    "\n",
    "chunks_sbert = np.load(os.path.join(vectors_path, \"sbert.npy\")).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_tfidf_sparse = chunks_tfidf.item() \n",
    "chunks_tfidf_dense = chunks_tfidf_sparse.toarray()  \n",
    "chunks_tfidf_dense.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the llm we want to use. \n",
    "\n",
    "We'll start off using llama3. This can be changed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Functions for Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similarities_sbert(query: str) -> np.array(float):\n",
    "    \"\"\"\n",
    "    Calculates similarity scores between the query and SBERT-vector-representations of the chunks.\n",
    "    Args:\n",
    "        query (str): The query.\n",
    "    Returns:\n",
    "        numpy-array: An array of cosine similarity scores between the query and the chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    query_sbert = get_sbert_vector(query, sbert_model)\n",
    "\n",
    "    # Reshape the query vector to be a 2D array with one row\n",
    "    query_sbert = query_sbert.reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_sbert, chunks_sbert)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def get_similarities_word2vec(query: str) -> np.array(float):\n",
    "    \"\"\"\n",
    "    Calculates similarity scores between the query and word2vec-vector-representations of our documents.\n",
    "    Args:\n",
    "        query (str): The query.\n",
    "    Returns:\n",
    "        numpy-array: An array of cosine similarity scores between the query and the documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_word2vec = get_word2vec_vector(query, word2vec_model)\n",
    "\n",
    "    # Reshape the query vector to be a 2D array with one row\n",
    "    query_word2vec = query_word2vec.reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_word2vec, chunks_word2vec)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def get_similarities_fasttext(query: str) -> np.array(float):\n",
    "    \"\"\"\n",
    "    Calculates similarity scores between the query and fasttext-vector-representations of our documents.\n",
    "    Args:\n",
    "        query (str): The query.\n",
    "    Returns:\n",
    "        numpy-array: An array of cosine similarity scores between the query and the documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_fasttext = get_fasttext_vector(query, aligned_vectors_de, aligned_vectors_eng)\n",
    "\n",
    "    # Reshape the query vector to be a 2D array with one row\n",
    "    query_fasttext = query_fasttext.reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_fasttext, chunks_fasttext)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# get stopwords\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "stopwords_combined = stopwords_german.union(stopwords_english)\n",
    "\n",
    "def preprocess(stopwords: list[str], text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by converting it to lowercase, removing punctuation, and filtering out stopwords.\n",
    "    Args:\n",
    "        stopwords (List[str]): A list of stopwords to be filtered out from the text.\n",
    "        text (str): The input text to be preprocessed.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    \n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_similarities_tfidf(query: str) -> np.array(float):\n",
    "    \"\"\"\n",
    "    Calculates similarity scores between the query and tfidf-representations of our documents.\n",
    "    Args:\n",
    "        query (str): The query.\n",
    "    Returns:\n",
    "        numpy-array: An array of cosine similarity scores between the query and the documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_tfidf = get_tfidf_vector(preprocess(stopwords_combined, query))\n",
    "    \n",
    "    chunks_tfidf_dense = chunks_tfidf.item().toarray()\n",
    "    \n",
    "    print(type(query_tfidf))\n",
    "    print(type(chunks_tfidf_dense))    \n",
    "\n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_tfidf, chunks_tfidf_dense)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def get_similarity(query, vectorisation):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between a query and our chunks using given vectorisation.\n",
    "    Parameters:\n",
    "    query (str): The query string.\n",
    "    vectorisation (str): The vectorisation method: Choose from: tfidf, fasttext, word2vec, sbert.\n",
    "    Returns:\n",
    "    similarity (float): The similarity score between the query and the chunks.\n",
    "    Raises:\n",
    "    KeyError: If the given vectorisation is not valid.\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorisations = {\n",
    "        \"tfidf\":get_similarities_tfidf,\n",
    "        \"fasttext\":get_similarities_fasttext,\n",
    "        \"word2vec\":get_similarities_word2vec,\n",
    "        \"sbert\":get_similarities_sbert\n",
    "    }\n",
    "    \n",
    "    if vectorisation not in vectorisations:\n",
    "        raise KeyError(f\"'{vectorisation}' is not a valid approach. Choose from: {', '.join(vectorisations.keys())}\")\n",
    "    \n",
    "    methode = vectorisations[vectorisation]\n",
    "    similarity = methode(query)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def get_similar_chunks(query, vectorisation, n):\n",
    "    similarity = get_similarity(query, vectorisation)\n",
    "    df[\"similarity\"] = similarity[0]\n",
    "    return df.sort_values(\"similarity\", ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>description_preprocessed_chunks</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Transcribo</td>\n",
       "      <td>[Transcribo](https://tcdh.uni-trier.de/en/proj...</td>\n",
       "      <td>Transcribo is an editing tool developed by the...</td>\n",
       "      <td>transcribo editing tool developed trier center...</td>\n",
       "      <td>0.579517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von...</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für ...</td>\n",
       "      <td>transkribus umfassende plattform digitalisieru...</td>\n",
       "      <td>0.562283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>correspSearch</td>\n",
       "      <td>With correspSearch you can search through inde...</td>\n",
       "      <td>With correspSearch you can search within the m...</td>\n",
       "      <td>correspsearch search within metadata diverse s...</td>\n",
       "      <td>0.559531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>VMR CRE</td>\n",
       "      <td>The Virtual Manuscript Room Collaborative Rese...</td>\n",
       "      <td>menus and dialogs to assist the researcher wit...</td>\n",
       "      <td>menus dialogs assist researcher composing tran...</td>\n",
       "      <td>0.538878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ChrysoCollate</td>\n",
       "      <td>The program offers:\\n\\n- two modes: collation ...</td>\n",
       "      <td>ChrysoCollate is a freeware for collating and ...</td>\n",
       "      <td>chrysocollate freeware collating editing texts...</td>\n",
       "      <td>0.518004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       brand_name                                        description  \\\n",
       "29     Transcribo  [Transcribo](https://tcdh.uni-trier.de/en/proj...   \n",
       "0     Transkribus  # Erkennen, Transkribieren und Durchsuchen von...   \n",
       "14  correspSearch  With correspSearch you can search through inde...   \n",
       "67        VMR CRE  The Virtual Manuscript Room Collaborative Rese...   \n",
       "53  ChrysoCollate  The program offers:\\n\\n- two modes: collation ...   \n",
       "\n",
       "                             description_clean_chunks  \\\n",
       "29  Transcribo is an editing tool developed by the...   \n",
       "0   Transkribus ist eine umfassende Plattform für ...   \n",
       "14  With correspSearch you can search within the m...   \n",
       "67  menus and dialogs to assist the researcher wit...   \n",
       "53  ChrysoCollate is a freeware for collating and ...   \n",
       "\n",
       "                      description_preprocessed_chunks  similarity  \n",
       "29  transcribo editing tool developed trier center...    0.579517  \n",
       "0   transkribus umfassende plattform digitalisieru...    0.562283  \n",
       "14  correspsearch search within metadata diverse s...    0.559531  \n",
       "67  menus dialogs assist researcher composing tran...    0.538878  \n",
       "53  chrysocollate freeware collating editing texts...    0.518004  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_similar_chunks(\"Ich will ein Manuskript transkribieren\", vectorisation=\"sbert\", n=5)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpfull assistant and keep answers brief\",\n",
    "    ),\n",
    "    (\"human\", \"Answer this question {query} using this context {context}\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

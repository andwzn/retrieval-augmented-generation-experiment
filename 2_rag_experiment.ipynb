{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewinzen/opt/anaconda3/envs/retrieval_augmented_generation/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import sentence_transformers\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import fasttext\n",
    "from langchain_ollama import ChatOllama\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from langchain_core.messages import AIMessage\n",
    "from IPython.display import Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the vectorizations we created in the notebook `1_vectorisation` to provide relevant information from the software repository in response to a user query. \n",
    "To do so, we'll:\n",
    "\n",
    "1. Load the models, the data and the vector representations of said data. \n",
    "\n",
    "2. Load and prepare the LLM we want to use for RAG. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the models, the chunked descriptions from our software repository and the vector representations we prepared in the notebook `1_vectorisation.`\n",
    "\n",
    "If the data and vectorizations have not yet been created, please run the `1_vectorisation` notebook first. Note that this process may take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Models\n",
    "\n",
    "First we'll load the different models, as we need them to vectorize the queries.\n",
    "\n",
    "Once we loaded the models, we'll create functions to vectorize the queries using the loaded models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the TFIDF-Vectorizer + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sucessfully loaded\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), \"models/tfidf_vectorizer.pickle\")\n",
    "with open(path, \"rb\") as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "    \n",
    "def get_tfidf_vector(query):\n",
    "    return tfidf_vectorizer.transform([query])\n",
    "    \n",
    "print(\"Model sucessfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load the Word2Vec model + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n",
      "Model loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "path = os.path.join(current_path, \"models/word2vec-google-news-300.bin\")\n",
    "\n",
    "# Load the model if it is already in our project. If not, download it.\n",
    "if os.path.isfile(path):\n",
    "    print(\"Model found. Loading...\")\n",
    "    word2vec_model = KeyedVectors.load(path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    word2vec_model.save(path)\n",
    "print(\"Model loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to vectorize the query    \n",
    "def get_word2vec_vector(query, model):\n",
    "    words = query.split()\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in model]\n",
    "\n",
    "    if not valid_words:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    # Average the vectors of the valid words to create a document representation\n",
    "    vectors = [model[word] for word in valid_words]\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load the Fast-Text models + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found.\n",
      "Language identification model loaded sucessfully.\n",
      "Model found. Loading...\n",
      "English model loaded sucessfully.\n",
      "Model found. Loading...\n",
      "German model loaded sucessfully.\n"
     ]
    }
   ],
   "source": [
    "### CREATE HELPER FUNCTIONS\n",
    "\n",
    "# define functions to load FastText models\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"File was already downloaded.\")\n",
    "            return None\n",
    "        \n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"The file has been downloaded and saved as: {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred while downloading the file: {e}\")\n",
    "\n",
    "# define function to unzip models\n",
    "def unzip_file(zip_file_path: str, extract_to: str) -> None:\n",
    "    \"\"\"Unzip a file to a target directory.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Unzipped {zip_file_path} to {extract_to}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error while unzipping the file: {e}\")\n",
    "\n",
    "# define a function to load word vectors from a file\n",
    "def load_word_vectors(file_path: str):\n",
    "    \"\"\"Load word vectors from a file.\"\"\"\n",
    "    try:\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        print(\"Vectors loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the vectors: {e}\")\n",
    "        return None\n",
    "\n",
    "### LOAD MODELS\n",
    "\n",
    "# download or load the fasttext model for language detection\n",
    "langident_path = os.path.join(os.getcwd(), 'models/lid.176.bin')\n",
    "\n",
    "if os.path.isfile(langident_path):\n",
    "    print(\"Model found.\")\n",
    "    language_detection_model = fasttext.load_model(langident_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    # download the model\n",
    "    download_file(url, langident_path)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    language_detection_model = fasttext.load_model(langident_path)\n",
    "    \n",
    "language_identification_model = fasttext.load_model(langident_path)\n",
    "\n",
    "print(\"Language identification model loaded sucessfully.\")\n",
    "\n",
    "# Check if the english model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future. \n",
    "# This might take a while\n",
    "\n",
    "current_path = os.getcwd()\n",
    "models_dir = os.path.join(current_path, \"models\")\n",
    "fasttext_eng_zip_path = os.path.join(models_dir, \"wiki.en.zip\")\n",
    "fasttext_eng_path_vec = os.path.join(models_dir, \"wiki.en.vec\")\n",
    "fasttext_eng_path_bin = os.path.join(models_dir, \"wiki.en.bin\")\n",
    "\n",
    "if os.path.isfile(fasttext_eng_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model(fasttext_eng_path_bin) #load the full model, including subword information.\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\" \n",
    "    # download the models\n",
    "    download_file(url, fasttext_eng_zip_path)\n",
    "    \n",
    "    print(\"Unzipping the file...\")\n",
    "    unzip_file(fasttext_eng_zip_path, models_dir)    \n",
    "\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model.load(fasttext_eng_path_bin)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_eng.save(fasttext_eng_path_bin)\n",
    "\n",
    "    \n",
    "if aligned_vectors_eng is None:\n",
    "    raise ValueError(\"The FastText model was not loaded properly.\")\n",
    "\n",
    "print(\"English model loaded sucessfully.\")\n",
    "\n",
    "# Check if the german model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future.\n",
    "# This might take a while\n",
    "\n",
    "fasttext_de_path_bin = os.path.join(current_path, \"models/wiki_de_align.bin\")\n",
    "fasttext_de_path_vec = os.path.join(current_path, \"models/wiki_de_align.vec\")\n",
    "\n",
    "if os.path.isfile(fasttext_de_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_de = KeyedVectors.load(fasttext_de_path_bin)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_de_path_vec)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_de = load_word_vectors(fasttext_de_path_vec)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_de.save(fasttext_de_path_bin)\n",
    "    \n",
    "if aligned_vectors_de is None:\n",
    "    raise ValueError(\"The FastText model or vectors were not loaded properly.\")\n",
    "\n",
    "print(\"German model loaded sucessfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to identify a query's language\n",
    "def identify_language(query):\n",
    "    \"\"\"\n",
    "    Identifies the language of the given query.\n",
    "    Parameters:\n",
    "    query (str): The text to be analyzed for language identification.\n",
    "    \"\"\"\n",
    "    \n",
    "    lang_detected = language_identification_model.predict(query)\n",
    "\n",
    "    return lang_detected[0][0].replace(\"__label__\", \"\")\n",
    "\n",
    "\n",
    "# create function to embed query (based on language)\n",
    "def get_fasttext_vector(query, aligned_vectors_de=None, aligned_vectors_eng=None):\n",
    "    \"\"\"\n",
    "    Calculates the FastText vector representation for a given query.\n",
    "    Parameters:\n",
    "    - text: A text.\n",
    "    - aligned_vectors_de: Aligned FastText vectors for the German language.\n",
    "    - aligned_vectors_eng: Aligned FastText vectors for the English language.\n",
    "    Note:\n",
    "    - If the language is not specified or not supported (only \"en\" and \"de\" are supported), it returns a zero vector.\n",
    "    - If a word in the row's description is not found in the aligned vectors, it tries to create a vector based on english subword information.\n",
    "    - If no vectors are found, it returns a zero vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    # default size to avoid errors if vectors are None\n",
    "    vector_size = aligned_vectors_de.vector_size if aligned_vectors_de else 300\n",
    "    \n",
    "    # check if language is valid\n",
    "    lang = identify_language(query)\n",
    "    if pd.isna(lang) or lang not in [\"en\", \"de\"]:\n",
    "        return np.zeros(vector_size) # Maybe rather use none?\n",
    "    \n",
    "    words = query.split()\n",
    "    vectors = []\n",
    "\n",
    "    # process based on language\n",
    "    if lang == \"de\" and aligned_vectors_de:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_de[word])\n",
    "            except KeyError:\n",
    "                print(f\"Created Vector based on Subword Information for: {word}\")                \n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "                #vectors.append(np.zeros(vector_size))\n",
    "                \n",
    "    elif lang == \"en\" and aligned_vectors_eng:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "            except KeyError:\n",
    "                print(f\"Missing Vector for: {word}\")\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the SBERT and Cross Encoder models + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Sentence Transformer from drive...\n",
      "SBERT model loaded sucessfully\n",
      "Load Cross Encoder from drive...\n",
      "Cross encoder loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "# Download SBERT model or load them from drive\n",
    "sbert_path = os.path.join(os.getcwd(),\"models/sbert\")\n",
    "downloaded = os.path.isdir(sbert_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Sentence Transformer...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    sbert_model.save(sbert_path)\n",
    "else:\n",
    "    print(\"Load Sentence Transformer from drive...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer(sbert_path)\n",
    "    \n",
    "print(\"SBERT model loaded sucessfully\")\n",
    "    \n",
    "# Download Cross Encoder or load them from drive\n",
    "\n",
    "cross_encoder_path = os.path.join(os.getcwd(), \"models/cross\")\n",
    "downloaded = os.path.isdir(cross_encoder_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Cross Encoder...\")\n",
    "    cross_encoder_model = sentence_transformers.CrossEncoder(\"corrius/cross-encoder-mmarco-mMiniLMv2-L12-H384-v1\")\n",
    "    cross_encoder_model.save(cross_encoder_path)\n",
    "else:\n",
    "    print(\"Load Cross Encoder from drive...\")\n",
    "    cross_encoder_model = sentence_transformers.CrossEncoder(cross_encoder_path)\n",
    "    \n",
    "print(\"Cross encoder loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to embedd the query\n",
    "def get_sbert_vector(query, model):\n",
    "    \"\"\"\n",
    "    Get Sentence-BERT embeddings for a given text using a specified model.\n",
    "    Parameters:\n",
    "    text (str): The input text to encode.\n",
    "    model: The Sentence-BERT model to use for encoding.\n",
    "    Returns:\n",
    "    numpy.ndarray: The Sentence-BERT embeddings for the input text.\n",
    "    \"\"\"\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "    \n",
    "    if pd.isna(query) or query.strip() == '':\n",
    "        return default_embedding    \n",
    "    return model.encode(query, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load chunked descriptions\n",
    "\n",
    "Next, we load the chunked descriptions from the software repository we prepared in the notebook `1_vectorisation`. We'll need them to supply text to the user and the llm used for the RAG-process.\n",
    "\n",
    "We'll only load four columns. \n",
    "\n",
    "1. **description_clean_chunks:** The chunk.\n",
    "2. **description_preprocessed_chunks:** The preprocessed chunk (lowercase + removed punctuation and stopwords).\n",
    "3. **description:** The complete description from which the chunk was extracted.\n",
    "4. **brand_name:** The name of the software described by the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>description_preprocessed_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>transkribus umfassende plattform digitalisierung texterkennung mithilfe künstlicher intelligenz transkription durchsuchen historischen dokumenten erkennen transkribieren durchsuchen historischen dokumenten mitttels ki trainieren spezifischen texterkennungsmodellen lage handschriftliche maschinengeschriebene gedruckte dokumente erkennen kigestützte erkennung handgeschriebenem text layoutanalyse strukturerkennung manuelles transkribieren transkriptionseditor kigestützten erkennung mittels öffentlicher trainierter kimodelle durchsuchen dokumenten erweiterten suchoptionen z b tool aufspüren schlüsselwörtern gemeinsames arbeiten dokumenten organisation sammlungen teilen dokumenten readsearch website export pdf alto xml transkribusinhalte dh hochgeladene bilder erkannte texte trainierte erkennungsmodelle eingegebene metadaten innerhalb eu gehostet gdpr konform</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    brand_name  \\\n",
       "0  Transkribus   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               description  \\\n",
       "0  # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            description_clean_chunks  \\\n",
       "0  Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      description_preprocessed_chunks  \n",
       "0  transkribus umfassende plattform digitalisierung texterkennung mithilfe künstlicher intelligenz transkription durchsuchen historischen dokumenten erkennen transkribieren durchsuchen historischen dokumenten mitttels ki trainieren spezifischen texterkennungsmodellen lage handschriftliche maschinengeschriebene gedruckte dokumente erkennen kigestützte erkennung handgeschriebenem text layoutanalyse strukturerkennung manuelles transkribieren transkriptionseditor kigestützten erkennung mittels öffentlicher trainierter kimodelle durchsuchen dokumenten erweiterten suchoptionen z b tool aufspüren schlüsselwörtern gemeinsames arbeiten dokumenten organisation sammlungen teilen dokumenten readsearch website export pdf alto xml transkribusinhalte dh hochgeladene bilder erkannte texte trainierte erkennungsmodelle eingegebene metadaten innerhalb eu gehostet gdpr konform  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the data\n",
    "dataset_path = os.path.join(os.getcwd(), \"data/edition_software_info_chunked.csv\")\n",
    "columns = [\"brand_name\", \"description\", \"description_clean_chunks\", \"description_preprocessed_chunks\"]\n",
    "df = pd.read_csv(dataset_path ,skipinitialspace=True, usecols=columns)\n",
    "\n",
    "# replace missing values with empty strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# display the first row\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Vectors\n",
    "\n",
    "Finally we can load the vector representations of the chunks we created in `1_vectorisation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_path = os.path.join(os.getcwd(), \"vectorisations\")\n",
    "\n",
    "chunks_tfidf = np.load(os.path.join(vectors_path, \"tfidf.npy\"), allow_pickle=True)\n",
    "    \n",
    "chunks_word2vec = np.load(os.path.join(vectors_path, \"word2vec.npy\")).tolist()\n",
    "\n",
    "chunks_fasttext = np.load(os.path.join(vectors_path, \"fasttext.npy\")).tolist()\n",
    "\n",
    "chunks_sbert = np.load(os.path.join(vectors_path, \"sbert.npy\")).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 19962)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_tfidf_sparse = chunks_tfidf.item() \n",
    "chunks_tfidf_dense = chunks_tfidf_sparse.toarray()  \n",
    "chunks_tfidf_dense.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load the llm we want to use. \n",
    "\n",
    "We'll start off using llama3. This can be changed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Functions for Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similarities_sbert(query: str) -> np.array(float):\n",
    "    \"\"\"\n",
    "    Calculates similarity scores between the query and SBERT-vector-representations of the chunks.\n",
    "    Args:\n",
    "        query (str): The query.\n",
    "    Returns:\n",
    "        numpy-array: An array of cosine similarity scores between the query and the chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    query_sbert = get_sbert_vector(query, sbert_model)\n",
    "\n",
    "    # Reshape the query vector to be a 2D array with one row\n",
    "    query_sbert = query_sbert.reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_sbert, chunks_sbert)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def get_similarities_word2vec(query: str) -> np.array(float):\n",
    "    \"\"\"\n",
    "    Calculates similarity scores between the query and word2vec-vector-representations of our documents.\n",
    "    Args:\n",
    "        query (str): The query.\n",
    "    Returns:\n",
    "        numpy-array: An array of cosine similarity scores between the query and the documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_word2vec = get_word2vec_vector(query, word2vec_model)\n",
    "\n",
    "    # Reshape the query vector to be a 2D array with one row\n",
    "    query_word2vec = query_word2vec.reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_word2vec, chunks_word2vec)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def get_similarities_fasttext(query: str) -> np.array(float):\n",
    "    \"\"\"\n",
    "    Calculates similarity scores between the query and fasttext-vector-representations of our documents.\n",
    "    Args:\n",
    "        query (str): The query.\n",
    "    Returns:\n",
    "        numpy-array: An array of cosine similarity scores between the query and the documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_fasttext = get_fasttext_vector(query, aligned_vectors_de, aligned_vectors_eng)\n",
    "\n",
    "    # Reshape the query vector to be a 2D array with one row\n",
    "    query_fasttext = query_fasttext.reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_fasttext, chunks_fasttext)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# get stopwords\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "stopwords_combined = stopwords_german.union(stopwords_english)\n",
    "\n",
    "def preprocess(stopwords: list[str], text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by converting it to lowercase, removing punctuation, and filtering out stopwords.\n",
    "    Args:\n",
    "        stopwords (List[str]): A list of stopwords to be filtered out from the text.\n",
    "        text (str): The input text to be preprocessed.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    \n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_similarities_tfidf(query: str) -> np.array(float):\n",
    "    \"\"\"\n",
    "    Calculates similarity scores between the query and tfidf-representations of our documents.\n",
    "    Args:\n",
    "        query (str): The query.\n",
    "    Returns:\n",
    "        numpy-array: An array of cosine similarity scores between the query and the documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_tfidf = get_tfidf_vector(preprocess(stopwords_combined, query))\n",
    "    \n",
    "    chunks_tfidf_dense = chunks_tfidf.item().toarray()\n",
    "    \n",
    "    print(type(query_tfidf))\n",
    "    print(type(chunks_tfidf_dense))    \n",
    "\n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_tfidf, chunks_tfidf_dense)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def get_similarity(query, vectorisation):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between a query and our chunks using given vectorisation.\n",
    "    Parameters:\n",
    "    query (str): The query string.\n",
    "    vectorisation (str): The vectorisation method: Choose from: tfidf, fasttext, word2vec, sbert.\n",
    "    Returns:\n",
    "    similarity (float): The similarity score between the query and the chunks.\n",
    "    Raises:\n",
    "    KeyError: If the given vectorisation is not valid.\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorisations = {\n",
    "        \"tfidf\":get_similarities_tfidf,\n",
    "        \"fasttext\":get_similarities_fasttext,\n",
    "        \"word2vec\":get_similarities_word2vec,\n",
    "        \"sbert\":get_similarities_sbert\n",
    "    }\n",
    "    \n",
    "    if vectorisation not in vectorisations:\n",
    "        raise KeyError(f\"'{vectorisation}' is not a valid approach. Choose from: {', '.join(vectorisations.keys())}\")\n",
    "    \n",
    "    methode = vectorisations[vectorisation]\n",
    "    similarity = methode(query)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "def get_similar_chunks(query, vectorisation, n):\n",
    "    similarity = get_similarity(query, vectorisation)\n",
    "    df[\"similarity\"] = similarity[0]\n",
    "    return df.sort_values(\"similarity\", ascending=False).head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>description_preprocessed_chunks</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Transcribo</td>\n",
       "      <td>[Transcribo](https://tcdh.uni-trier.de/en/proj...</td>\n",
       "      <td>Transcribo is an editing tool developed by the...</td>\n",
       "      <td>transcribo editing tool developed trier center...</td>\n",
       "      <td>0.579517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von...</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für ...</td>\n",
       "      <td>transkribus umfassende plattform digitalisieru...</td>\n",
       "      <td>0.562283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>correspSearch</td>\n",
       "      <td>With correspSearch you can search through inde...</td>\n",
       "      <td>With correspSearch you can search within the m...</td>\n",
       "      <td>correspsearch search within metadata diverse s...</td>\n",
       "      <td>0.559531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>VMR CRE</td>\n",
       "      <td>The Virtual Manuscript Room Collaborative Rese...</td>\n",
       "      <td>menus and dialogs to assist the researcher wit...</td>\n",
       "      <td>menus dialogs assist researcher composing tran...</td>\n",
       "      <td>0.538878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ChrysoCollate</td>\n",
       "      <td>The program offers:\\n\\n- two modes: collation ...</td>\n",
       "      <td>ChrysoCollate is a freeware for collating and ...</td>\n",
       "      <td>chrysocollate freeware collating editing texts...</td>\n",
       "      <td>0.518004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       brand_name                                        description  \\\n",
       "29     Transcribo  [Transcribo](https://tcdh.uni-trier.de/en/proj...   \n",
       "0     Transkribus  # Erkennen, Transkribieren und Durchsuchen von...   \n",
       "14  correspSearch  With correspSearch you can search through inde...   \n",
       "67        VMR CRE  The Virtual Manuscript Room Collaborative Rese...   \n",
       "53  ChrysoCollate  The program offers:\\n\\n- two modes: collation ...   \n",
       "\n",
       "                             description_clean_chunks  \\\n",
       "29  Transcribo is an editing tool developed by the...   \n",
       "0   Transkribus ist eine umfassende Plattform für ...   \n",
       "14  With correspSearch you can search within the m...   \n",
       "67  menus and dialogs to assist the researcher wit...   \n",
       "53  ChrysoCollate is a freeware for collating and ...   \n",
       "\n",
       "                      description_preprocessed_chunks  similarity  \n",
       "29  transcribo editing tool developed trier center...    0.579517  \n",
       "0   transkribus umfassende plattform digitalisieru...    0.562283  \n",
       "14  correspsearch search within metadata diverse s...    0.559531  \n",
       "67  menus dialogs assist researcher composing tran...    0.538878  \n",
       "53  chrysocollate freeware collating editing texts...    0.518004  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_similar_chunks(\"Ich will ein Manuskript transkribieren\", vectorisation=\"sbert\", n=5)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Question Bot\n",
    "\n",
    "This bot answers a single question based on data from the text+ repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Answer:\n",
       "\n",
       "Based on your question, I would recommend using Transkribus for digitizing a collection of manuscripts. It's described as a comprehensive platform for handwritten text recognition and is well-suited for processing large collections of manuscripts. Additionally, it offers features such as OCR, data validation, and linking to other archives or libraries, making it an efficient tool for your task.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR INPUT: Please enter your question and specifications ###\n",
    "\n",
    "your_query = \"I want to start digitizing a collection of manuscripts\" # your question \n",
    "text_vectorisation_method = \"sbert\" # the text vectrisation method used for comparison\n",
    "n = 10 # number of (potentially) relevant text chunks from the repository provided to the llm\n",
    "temperature = 0.25 # Lower temperatures (0.2 - 0.5) make the model's responses more focused. \n",
    "                # Higher Temperatures (e.g., 0.7 - 1.0) introduce more randomness.\n",
    "\n",
    "\n",
    "### This code runs the request. You don't need to change anything below this comment. ###\n",
    "\n",
    "# find relevant chunks \n",
    "context = get_similar_chunks(query=your_query, vectorisation=text_vectorisation_method, n=n)\n",
    "context[\"combined\"] = \"This text is about \" + context[\"brand_name\"] + \". \" + context[\"description_clean_chunks\"] # add the software name to each chunk\n",
    "\n",
    "# load system prompt\n",
    "sys_path = os.path.join(os.getcwd(), \"prompts/sys.txt\")\n",
    "with open(sys_path, 'r') as file:\n",
    "    sys_prompt = file.read()\n",
    "sys_prompt = sys_prompt.replace(\"\\n\", \" \")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        f\"{sys_prompt}\"\n",
    "    ),\n",
    "    (\"human\", f\"Answer this question: {your_query}. Answer in the language used in the question. Here is some software from the Text+ Repository, that might be relevant.: {context[\"combined\"]}. You can ignore software that is not relevant. \"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages, temperature=temperature)\n",
    "\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "formatted_answer = f\"\"\"\n",
    "### Answer:\n",
    "\n",
    "{ai_msg.content}\n",
    "\"\"\"\n",
    "display(Markdown(formatted_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat-Bot\n",
    "\n",
    "This chat bot answers questions based on the Text+ repository.\n",
    "\n",
    "The visualisation is still a work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e14fd0823f24f27b2e87e714c9bdf5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<style>\\n@keyframes spin {\\n  0% { transform: rotate(0deg); }\\n  100% { transform: rotate(360deg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc7afab0122472da8b6e9987306a9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), HTML(value='<div class=\"spinner\" style=\"width: 30px; height: 30px; border: 4px solid …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# initialize conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# define widgets for user input, sending button, output, and loading indicator\n",
    "text_input = widgets.Text(\n",
    "    description='Your Message:',\n",
    "    placeholder='Type your message here'\n",
    ")\n",
    "send_button = widgets.Button(description=\"Send\")\n",
    "output = widgets.Output()\n",
    "loading_indicator = widgets.HTML(\n",
    "    value='<div class=\"spinner\" style=\"width: 30px; height: 30px; border: 4px solid rgba(0, 0, 0, 0.1); border-radius: 50%; border-top: 4px solid #007bff; animation: spin 1s linear infinite;\"></div>',\n",
    "    layout=widgets.Layout(display='none')\n",
    ")\n",
    "\n",
    "# CSS for spinner\n",
    "spinner_css = \"\"\"\n",
    "<style>\n",
    "@keyframes spin {\n",
    "  0% { transform: rotate(0deg); }\n",
    "  100% { transform: rotate(360deg); }\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "display(widgets.HTML(spinner_css))\n",
    "\n",
    "# function to handle user input and AI response\n",
    "def on_send_button_click(b):\n",
    "    with output:\n",
    "        # clear previous output\n",
    "        output.clear_output()\n",
    "        \n",
    "        # show the loading indicator\n",
    "        loading_indicator.layout.display = 'block'\n",
    "        display(loading_indicator)\n",
    "        \n",
    "        # get the user message\n",
    "        user_message = text_input.value\n",
    "        \n",
    "        # append the user message to the conversation history\n",
    "        conversation_history.append(f\"**User:** {user_message}\")\n",
    "        \n",
    "        # process the user message\n",
    "        user_query = user_message\n",
    "        context = get_similar_chunks(query=user_query, vectorisation=text_vectorisation_method, n=n)\n",
    "        context[\"combined\"] = \"This text is about \" + context[\"brand_name\"] + \". \" + context[\"description_clean_chunks\"]  # add the software name to each chunk\n",
    "        \n",
    "        # load system prompt\n",
    "        sys_path = os.path.join(os.getcwd(), \"prompts/sys.txt\")\n",
    "        with open(sys_path, 'r') as file:\n",
    "            sys_prompt = file.read()\n",
    "        sys_prompt = sys_prompt.replace(\"\\n\", \" \")\n",
    "        \n",
    "        messages = [\n",
    "            (\"system\", f\"{sys_prompt}\"),\n",
    "        ]\n",
    "        \n",
    "        # add previous conversation history to messages\n",
    "        for history in conversation_history:\n",
    "            messages.append((\"human\", history))\n",
    "        \n",
    "        # append the current user message\n",
    "        messages.append((\"human\", f\"Answer this question: {user_query}. Answer in the language used in the question. Here is some software from the Text+ Repository, that might be relevant.: {context['combined']}. You can ignore software that is not relevant.\"))\n",
    "        \n",
    "        # generate response\n",
    "        ai_msg = llm.invoke(messages, temperature=temperature)\n",
    "        \n",
    "        # hide the loading indicator\n",
    "        loading_indicator.layout.display = 'none'\n",
    "        \n",
    "        # append the AI response to the conversation history\n",
    "        ai_response = ai_msg.content\n",
    "        conversation_history.append(f\"**T+2000:** {ai_response}\")\n",
    "        \n",
    "        # format and display the conversation history\n",
    "        formatted_history = \"\\n\\n\".join(conversation_history)  # Line breaks between messages\n",
    "        display(Markdown(f\"### Conversation History:\\n{formatted_history}\"))\n",
    "        \n",
    "        # clear the input box\n",
    "        text_input.value = ''\n",
    "\n",
    "# arrange widgets in a vertical box layout\n",
    "chat_box = widgets.VBox([output, loading_indicator, text_input, send_button])\n",
    "\n",
    "# attach the event handler to the send button\n",
    "send_button.on_click(on_send_button_click)\n",
    "\n",
    "# display the chat box\n",
    "display(chat_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This text is about correspSearch. With correspSearch you can search within the metadata of diverse scholarly editions of letters. One can search according to the letter's sender, adressee, as well as place and date of the letter's creation. With correspSearch you can search through indexes of different letter collections (digital or print) by sender, addressee, location written, location sent, and date. To this purpose a website and a technical interface are provided. The web service collects and evaluates TEI-XML data in the ‘Correspondence Metadata Interchange’ format. The web service correspSearch is operated and developed according to the following principles: Reference System : The web service aims to help users with their research by offering a central location to search for letters, and by guiding them to the original publication. Academic Data : The web service is based on the data from letter-indexes of editions or repositories that are edited according to academic criteria. Conceptionaly Open : There is no focus on a particular time period or place. This allows for new kinds of research questions to be explored. Open Access : Data is only collected that is under a free license, and the data from the web service continues to be under a free license and is thus available for further use. Open Interfaces : correspSearch offers technical interfaces that are open and well documented. Other projects can easily query and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>This text is about ChrysoCollate. ChrysoCollate is a freeware for collating and editing texts in any language (Unicode). It allows you to easily collate your manuscripts, order them and compare them, and provide you with tools for easy editing, as the making of automatic apparatus.The program offers: two modes: collation mode and edition mode; a collation table with automatic distinctive colours and previsional semi-automatic completion of readings; annotation tools for the collation table, including a system of references to the images of the witnesses that allows you to navigate easily in your textual tradition; a viewer that displays witness pictures while one collates or edits (various formats of images, pdf, or websites); semi-automatic apparatus, according to the readings that are chosen by the editor; a stemma codicum checker; a translation box to manage and synchronise your translation; exportation in various formats (odt, cte, etc.).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This text is about Transkribus. Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>This text is about VMR CRE. management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus. Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page. For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features. These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF. A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be. Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility. For example, one might search for all manuscript folio sides from Egypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>This text is about TEITOK. therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>This text is about ediarum. ediarum is a digital working environment consisting of several software components that allows scholars to edit transcriptions of manuscripts and prints in TEI-compliant XML, to provide them with a text and subject apparatus as well as indexes, and to publish them on the web and in print. Benutzerfreundliches Arbeiten Als zentrale Softwarekomponente der Arbeitsumgebung wird Oxygen XML Author eingesetzt. Die Bearbeiter arbeiten im Oxygen XML Author nicht in einer Codeansicht, sondern in einer benutzerfreundlichen, Word-artigen »Autorenansicht«, die über Cascading Stylesheets (CSS) gestaltet wird. Dem Bearbeiter stehen dabei mehrere Ansichten zur Auswahl, so dass per Mausklick die für den Arbeitsschritt geeigneteste ausgewählt werden kann. Außerdem kann der Endanwender über eine eigene Werkzeugleiste per Knopfdruck Auszeichnungen vornehmen. So können z.B. in Manuskripten Streichungen markiert oder Sachanmerkungen eingegeben werden. Auch Personen- oder Ortsnamen können mit der entsprechenden TEI-Auszeichnung versehen und gleichzeitig über eine komfortable Auswahlliste mit dem jeweiligen Eintrag im zentralen Personen- bzw. Ortsregister verknüpft werden. Der gesamte Text kann dadurch einfach und schnell mit TEI-konformen XML ausgezeichnet werden. Kollaboratives Arbeiten Die digitale Arbeitsumgebung nutzt die freie XML-Datenbank existdb als zentrales Repositorium für die XML-Dokumente. Die Datenbank ist auf einem Server installiert und online zugänglich. Dadurch können alle Projektmitarbeiter auf ein und denselben Datenbestand zugreifen und zusammenarbeiten. Um die Einrichtung und Konfiguration zu vereinfachen, wurde das Modul ediarum.DB entwickelt. Website Neben dem eigentlichen Eingabewerkzeug in Oxygen XML Author, wird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>This text is about Publex. A browser-based publishing software for XML-annotated dictionaries. As part of the ELEXIS “European Lexicographic Infrastructure” initiative, TCDH developed the [Publex tool](), a browser-based publishing software for XML-annotated dictionaries. With the help of the software and an intuitive user interface, users can upload their dictionary data marked in XML and define the desired formatting for the dictionary by configuring the individual components. With these settings and the attached metadata, the dictionary can finally be published online on a platform provided by ELEXIS. Publex, thus, also enables users who do not have the appropriate infrastructure or technical knowledge to make their dictionaries accessible on the Internet. Step by step dictionary publishing with Publex A user manual guides you in detail and step by step through the application process ([Publication on DARIAH Campus]()). This can be roughly divided into three steps: 1. uploading the data 2. defining the representation 3. publishing the dictionary Users can import their XML dictionary data from a Git repository into Publex and submit metadata about the dictionary. The core of the tool is the definition of styling rules, which define how the individual elements of the dictionary articles should be displayed in the published online version. Upon import, Publex parses the data and captures all tags, attributes, and associated attribute values with which the resource has been tagged. For each of these elements and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>This text is about TEITOK. based web server. Features Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>This text is about FuD - The Virtual Research Environment for the Humanities and Social Sciences . publication. FuD can also be used to create various digital, historical-critical editions of works from scratch. Collection management With FuD, primary research data can be recorded using a jointly developed metadata schema and shared within a research group. Analysis FuD supports the analysis of text and image data with its annotation tools. The tools can be adapted to the project-specific research method. Network analysis On the basis of annotated texts, a visualization tool connected to FuD is used to analyse and display networks. Metadata FuD supports the structured recording of document metadata via individual input masks. System architecture FuD is a modular software system whose subcomponents are connected to each other via well-defined interfaces. The FuD working environment is used for data collection, analysis and editorial processing. This is a client-server architecture (tcl/Tk, MySQL database), whereby the FuD client is installed on the workstation computer, with which the data is processed on the FuD server. The FuD client is available for current Windows operating systems and, in a slightly reduced form, also for Macintosh operating systems. This architecture requires a permanent Internet connection to the server while working with the system. Other systems such as Zotero for managing secondary literature or the transcription tool \"Transcribo\" for transcribing full texts are connected to the FuD working environment. This is configured according to project-specific requirements. Existing structured data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>This text is about TextGridLab. A virtual research environment for the humanities that is optimized for working with TEI-coded resources and covers the entire research process up to publication. The TextGrid Laboratory, together with the TextGrid Repository, is a diverse research environment. The TextGridLab includes various tools and services that you can access to create, manage, and edit research data. The TextGridLab offers the possibility to collaboratively edit and generate research data in a protected environment. The Open-Source-Software is available for Windows, Mac OS X, and Linux. Other open source tools and services optimized for use with TextGrid can be integrated via the MarketPlace integrated into the TextGridLab (access via the start window of the TextGridLab). Thanks to the modular open-source architecture of the software, the tools and services provided by TextGrid can also be adapted to specific project-specific requirements. At the heart of the third funding phase (2012-2015) was the transfer of the virtual research environment into the long-term sustainable operation. TextGrid is freely available as part of the DARIAH infrastructure. The TextGridLab is optimised for XML/TEI development, e.g. in the context of digital editions. Advantages Decentralized work area : Users can access the TextGridLab and the TextgridRep independently of the location and work together in complex research projects. Standardization : The controlled metadata vocabulary and open standards facilitate the exchange of data, text search, and digital archiving. Extensibility :</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         combined\n",
       "14                                                                                                                                                                                                                                                                                                                                        This text is about correspSearch. With correspSearch you can search within the metadata of diverse scholarly editions of letters. One can search according to the letter's sender, adressee, as well as place and date of the letter's creation. With correspSearch you can search through indexes of different letter collections (digital or print) by sender, addressee, location written, location sent, and date. To this purpose a website and a technical interface are provided. The web service collects and evaluates TEI-XML data in the ‘Correspondence Metadata Interchange’ format. The web service correspSearch is operated and developed according to the following principles: Reference System : The web service aims to help users with their research by offering a central location to search for letters, and by guiding them to the original publication. Academic Data : The web service is based on the data from letter-indexes of editions or repositories that are edited according to academic criteria. Conceptionaly Open : There is no focus on a particular time period or place. This allows for new kinds of research questions to be explored. Open Access : Data is only collected that is under a free license, and the data from the web service continues to be under a free license and is thus available for further use. Open Interfaces : correspSearch offers technical interfaces that are open and well documented. Other projects can easily query and\n",
       "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   This text is about ChrysoCollate. ChrysoCollate is a freeware for collating and editing texts in any language (Unicode). It allows you to easily collate your manuscripts, order them and compare them, and provide you with tools for easy editing, as the making of automatic apparatus.The program offers: two modes: collation mode and edition mode; a collation table with automatic distinctive colours and previsional semi-automatic completion of readings; annotation tools for the collation table, including a system of references to the images of the witnesses that allows you to navigate easily in your textual tradition; a viewer that displays witness pictures while one collates or edits (various formats of images, pdf, or websites); semi-automatic apparatus, according to the readings that are chosen by the editor; a stemma codicum checker; a translation box to manage and synchronise your translation; exportation in various formats (odt, cte, etc.).\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               This text is about Transkribus. Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.\n",
       "65                                                                                                                                                                                                                                                                                                                                               This text is about VMR CRE. management; 4) indexing of folio content; 5) transcribing; 6) collating; 7) regularizing; 8) editing an apparatus; 9) genealogical analysis of the witness corpus. Metadata and Feature Tagging The VMR CRE stores with each manuscript a very limited set of descriptive data, reserving the primary metadata capture for a dynamic tagging facility called Feature Tagging. A Feature is any defined metadata information which might be captured for a manuscript or manuscript page. For example, an alternative catalog identifier, an external image repository, the canvas material type, the ink type, the script type; these are all Features which might be tagged on a manuscript; For individual pages: an illumination, a canon table, or even individual sample script characters might be tagged as Features. These Features must first be defined in the system, and the VMR CRE comes by default with a predefined set of Feature Definitions used at the INTF. A Feature Definition can specify that zero or more values should be captured with the Feature tag and what those value types and value domains should be. Once a Feature is defined, it can be used to tag manuscripts or manuscript pages, capturing individual Feature values for each tag, if necessary. Every Feature Definition adds to the number of facets available in the catalog search facility. For example, one might search for all manuscript folio sides from Egypt\n",
       "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                               This text is about TEITOK. therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various orthographic forms, providing many ways to search through the data. The type of corpora that TEITOK is meant for are very labour-intensive: for ancient texts, hardly any of the data will be available in digital format, and have to be scanned. In many cases, OCR will not work and even for human readers the texts are often very hard to read. And the data will display a lot of orthographic variation in which a lot of the linguistic annotation, including normalization, will have to be done by hand. As a result, most corpora created with TEITOK will have a limited size, and searching for linguistic properties in them will not yield a lot of results. Therefore, TEITOK offers the option to index the corpus in a central database, which can be searched via this site. Each search result will only display the direct context of the word, and will link directly to the word in the original text on the site of the project it originated from. This way, it is possible to search through multiple corpora at the same time, and get access to the full original data in a way that prominently features the\n",
       "11  This text is about ediarum. ediarum is a digital working environment consisting of several software components that allows scholars to edit transcriptions of manuscripts and prints in TEI-compliant XML, to provide them with a text and subject apparatus as well as indexes, and to publish them on the web and in print. Benutzerfreundliches Arbeiten Als zentrale Softwarekomponente der Arbeitsumgebung wird Oxygen XML Author eingesetzt. Die Bearbeiter arbeiten im Oxygen XML Author nicht in einer Codeansicht, sondern in einer benutzerfreundlichen, Word-artigen »Autorenansicht«, die über Cascading Stylesheets (CSS) gestaltet wird. Dem Bearbeiter stehen dabei mehrere Ansichten zur Auswahl, so dass per Mausklick die für den Arbeitsschritt geeigneteste ausgewählt werden kann. Außerdem kann der Endanwender über eine eigene Werkzeugleiste per Knopfdruck Auszeichnungen vornehmen. So können z.B. in Manuskripten Streichungen markiert oder Sachanmerkungen eingegeben werden. Auch Personen- oder Ortsnamen können mit der entsprechenden TEI-Auszeichnung versehen und gleichzeitig über eine komfortable Auswahlliste mit dem jeweiligen Eintrag im zentralen Personen- bzw. Ortsregister verknüpft werden. Der gesamte Text kann dadurch einfach und schnell mit TEI-konformen XML ausgezeichnet werden. Kollaboratives Arbeiten Die digitale Arbeitsumgebung nutzt die freie XML-Datenbank existdb als zentrales Repositorium für die XML-Dokumente. Die Datenbank ist auf einem Server installiert und online zugänglich. Dadurch können alle Projektmitarbeiter auf ein und denselben Datenbestand zugreifen und zusammenarbeiten. Um die Einrichtung und Konfiguration zu vereinfachen, wurde das Modul ediarum.DB entwickelt. Website Neben dem eigentlichen Eingabewerkzeug in Oxygen XML Author, wird\n",
       "26                                                                                                                                                                                                                                               This text is about Publex. A browser-based publishing software for XML-annotated dictionaries. As part of the ELEXIS “European Lexicographic Infrastructure” initiative, TCDH developed the [Publex tool](), a browser-based publishing software for XML-annotated dictionaries. With the help of the software and an intuitive user interface, users can upload their dictionary data marked in XML and define the desired formatting for the dictionary by configuring the individual components. With these settings and the attached metadata, the dictionary can finally be published online on a platform provided by ELEXIS. Publex, thus, also enables users who do not have the appropriate infrastructure or technical knowledge to make their dictionaries accessible on the Internet. Step by step dictionary publishing with Publex A user manual guides you in detail and step by step through the application process ([Publication on DARIAH Campus]()). This can be roughly divided into three steps: 1. uploading the data 2. defining the representation 3. publishing the dictionary Users can import their XML dictionary data from a Git repository into Publex and submit metadata about the dictionary. The core of the tool is the definition of styling rules, which define how the individual elements of the dictionary articles should be displayed in the published online version. Upon import, Publex parses the data and captures all tags, attributes, and associated attribute values with which the resource has been tagged. For each of these elements and\n",
       "48                                                                                                                                                                                                                                    This text is about TEITOK. based web server. Features Manuscript-based corpora Align your manuscript with your transcript Display each manuscript line with its transcription Transcribe directly from the manuscript Search directly for manuscript fragments Keep multiple editions within the same environment Stand-off Annotations Adds stand-off annotations to any corpus file Edit using an efficient interface Annotate over discontinuous regions Incorporate annotations into the CQP corpus Audio-based corpora Align your audio with your transcription Transcribe directly from the audio file Scroll transcription vertical with wave function horizontal Search directly for audio segments Dependency Grammar Keep dependency relations inside any corpus type Visualize dependency trees for any sentence Edit trees easily Search using dependency relations Geolocation Coordinates Map documents onto the world map Document are clustered into counted groups Access the documents from the map Compare corpus queries on the world map Edit from CQP Query Search for words often incorrectly annotated Click on any token in a KWIC list to edit it Edit all results in a systematic way Edit each results individually in a list Pre-modify each result by a regular expression Search The rich XML format used in TEITOK is hard to search through. For easier access, all corpora are therefore indexed using the Corpus WorkBench (CWB), allowing texts to be search efficiently, and with the rich query language that CWB provides. Words are indexed in the CWB with various\n",
       "37                                                                                                                                     This text is about FuD - The Virtual Research Environment for the Humanities and Social Sciences . publication. FuD can also be used to create various digital, historical-critical editions of works from scratch. Collection management With FuD, primary research data can be recorded using a jointly developed metadata schema and shared within a research group. Analysis FuD supports the analysis of text and image data with its annotation tools. The tools can be adapted to the project-specific research method. Network analysis On the basis of annotated texts, a visualization tool connected to FuD is used to analyse and display networks. Metadata FuD supports the structured recording of document metadata via individual input masks. System architecture FuD is a modular software system whose subcomponents are connected to each other via well-defined interfaces. The FuD working environment is used for data collection, analysis and editorial processing. This is a client-server architecture (tcl/Tk, MySQL database), whereby the FuD client is installed on the workstation computer, with which the data is processed on the FuD server. The FuD client is available for current Windows operating systems and, in a slightly reduced form, also for Macintosh operating systems. This architecture requires a permanent Internet connection to the server while working with the system. Other systems such as Zotero for managing secondary literature or the transcription tool \"Transcribo\" for transcribing full texts are connected to the FuD working environment. This is configured according to project-specific requirements. Existing structured data\n",
       "40                                                                                                                                                                                       This text is about TextGridLab. A virtual research environment for the humanities that is optimized for working with TEI-coded resources and covers the entire research process up to publication. The TextGrid Laboratory, together with the TextGrid Repository, is a diverse research environment. The TextGridLab includes various tools and services that you can access to create, manage, and edit research data. The TextGridLab offers the possibility to collaboratively edit and generate research data in a protected environment. The Open-Source-Software is available for Windows, Mac OS X, and Linux. Other open source tools and services optimized for use with TextGrid can be integrated via the MarketPlace integrated into the TextGridLab (access via the start window of the TextGridLab). Thanks to the modular open-source architecture of the software, the tools and services provided by TextGrid can also be adapted to specific project-specific requirements. At the heart of the third funding phase (2012-2015) was the transfer of the virtual research environment into the long-term sustainable operation. TextGrid is freely available as part of the DARIAH infrastructure. The TextGridLab is optimised for XML/TEI development, e.g. in the context of digital editions. Advantages Decentralized work area : Users can access the TextGridLab and the TextgridRep independently of the location and work together in complex research projects. Standardization : The controlled metadata vocabulary and open standards facilitate the exchange of data, text search, and digital archiving. Extensibility :"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you want to, you can inspect the information that was provided to the chatbot from the software repository. \n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(context[[\"combined\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

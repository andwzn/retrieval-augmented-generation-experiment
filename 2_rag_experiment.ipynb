{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewinzen/opt/anaconda3/envs/retrieval_augmented_generation/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import sentence_transformers\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the vectorizations we created in the notebook `1_vectorisation` to provide relevant information from the software repository in response to a user query. \n",
    "To do so, we'll first find the top n similar vectors to a vectorized representation of the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the models, the chunked descriptions from our software repository and the vector representations we prepared in the notebook `1_vectorisation.`\n",
    "\n",
    "If the dataset and vectorizations have not yet been created, please run the `1_vectorisation` notebook first. Note that this process may take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models\n",
    "\n",
    "First we'll load the different models, as we need them to vectorize the queries.\n",
    "\n",
    "Once we loaded the models, we'll create functions to vectorize the queries using the loaded models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the TFIDF-Vectorizer + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sucessfully loaded\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), \"models/tfidf_vectorizer.pickle\")\n",
    "with open(path, \"rb\") as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "    \n",
    "def get_tfidf_vector(query):\n",
    "    tfidf_vectorizer.transform(query)\n",
    "    \n",
    "print(\"Model sucessfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load the Word2Vec model + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n",
      "Model sucessfully loaded\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "path = os.path.join(current_path, \"models/word2vec-google-news-300.bin\")\n",
    "\n",
    "# Load the model if it is already in our project. If not, download it.\n",
    "if os.path.isfile(path):\n",
    "    print(\"Model found. Loading...\")\n",
    "    word2vec_model = KeyedVectors.load(path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    word2vec_model.save(path)\n",
    "print(\"Model loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to vectorize the query    \n",
    "def get_word2vec_vector(query, model):\n",
    "    words = query.split()\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in model]\n",
    "\n",
    "    if not valid_words:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    # Average the vectors of the valid words to create a document representation\n",
    "    vectors = [model[word] for word in valid_words]\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load the Fast-Text models + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found.\n",
      "Language identification model loaded sucessfully.\n",
      "Model found. Loading...\n",
      "English model loaded sucessfully.\n",
      "Model found. Loading...\n",
      "German model loaded sucessfully.\n"
     ]
    }
   ],
   "source": [
    "### CREATE HELPER FUNCTIONS\n",
    "\n",
    "# define functions to load FastText models\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"File was already downloaded.\")\n",
    "            return None\n",
    "        \n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"The file has been downloaded and saved as: {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred while downloading the file: {e}\")\n",
    "\n",
    "# define function to unzip models\n",
    "def unzip_file(zip_file_path: str, extract_to: str) -> None:\n",
    "    \"\"\"Unzip a file to a target directory.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Unzipped {zip_file_path} to {extract_to}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error while unzipping the file: {e}\")\n",
    "\n",
    "# define a function to load word vectors from a file\n",
    "def load_word_vectors(file_path: str):\n",
    "    \"\"\"Load word vectors from a file.\"\"\"\n",
    "    try:\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        print(\"Vectors loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the vectors: {e}\")\n",
    "        return None\n",
    "\n",
    "### LOAD MODELS\n",
    "\n",
    "# download or load the fasttext model for language detection\n",
    "langident_path = os.path.join(os.getcwd(), 'models/lid.176.bin')\n",
    "\n",
    "if os.path.isfile(langident_path):\n",
    "    print(\"Model found.\")\n",
    "    language_detection_model = fasttext.load_model(langident_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    # download the model\n",
    "    download_file(url, langident_path)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    language_detection_model = fasttext.load_model(langident_path)\n",
    "    \n",
    "language_identification_model = fasttext.load_model(langident_path)\n",
    "\n",
    "print(\"Language identification model loaded sucessfully.\")\n",
    "\n",
    "# Check if the english model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future. \n",
    "# This might take a while\n",
    "\n",
    "current_path = os.getcwd()\n",
    "models_dir = os.path.join(current_path, \"models\")\n",
    "fasttext_eng_zip_path = os.path.join(models_dir, \"wiki.en.zip\")\n",
    "fasttext_eng_path_vec = os.path.join(models_dir, \"wiki.en.vec\")\n",
    "fasttext_eng_path_bin = os.path.join(models_dir, \"wiki.en.bin\")\n",
    "\n",
    "if os.path.isfile(fasttext_eng_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model(fasttext_eng_path_bin) #load the full model, including subword information.\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\" \n",
    "    # download the models\n",
    "    download_file(url, fasttext_eng_zip_path)\n",
    "    \n",
    "    print(\"Unzipping the file...\")\n",
    "    unzip_file(fasttext_eng_zip_path, models_dir)    \n",
    "\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model.load(fasttext_eng_path_bin)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_eng.save(fasttext_eng_path_bin)\n",
    "\n",
    "    \n",
    "if aligned_vectors_eng is None:\n",
    "    raise ValueError(\"The FastText model was not loaded properly.\")\n",
    "\n",
    "print(\"English model loaded sucessfully.\")\n",
    "\n",
    "# Check if the german model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future.\n",
    "# This might take a while\n",
    "\n",
    "fasttext_de_path_bin = os.path.join(current_path, \"models/wiki_de_align.bin\")\n",
    "fasttext_de_path_vec = os.path.join(current_path, \"models/wiki_de_align.vec\")\n",
    "\n",
    "if os.path.isfile(fasttext_de_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_de = KeyedVectors.load(fasttext_de_path_bin)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_de_path_vec)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_de = load_word_vectors(fasttext_de_path_vec)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_de.save(fasttext_de_path_bin)\n",
    "    \n",
    "if aligned_vectors_de is None:\n",
    "    raise ValueError(\"The FastText model or vectors were not loaded properly.\")\n",
    "\n",
    "print(\"German model loaded sucessfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to identify a query's language\n",
    "def identify_language(query):\n",
    "    \"\"\"\n",
    "    Identifies the language of the given query.\n",
    "    Parameters:\n",
    "    query (str): The text to be analyzed for language identification.\n",
    "    \"\"\"\n",
    "    \n",
    "    lang_detected = language_identification_model.predict(query)\n",
    "    return lang_detected[0][0]\n",
    "\n",
    "\n",
    "# create function to embed query (based on language)\n",
    "def get_fasttext_vector(query, aligned_vectors_de=None, aligned_vectors_eng=None):\n",
    "    \"\"\"\n",
    "    Calculates the FastText vector representation for a given query.\n",
    "    Parameters:\n",
    "    - text: A text.\n",
    "    - aligned_vectors_de: Aligned FastText vectors for the German language.\n",
    "    - aligned_vectors_eng: Aligned FastText vectors for the English language.\n",
    "    Note:\n",
    "    - If the language is not specified or not supported (only \"en\" and \"de\" are supported), it returns a zero vector.\n",
    "    - If a word in the row's description is not found in the aligned vectors, it tries to create a vector based on english subword information.\n",
    "    - If no vectors are found, it returns a zero vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    # default size to avoid errors if vectors are None\n",
    "    vector_size = aligned_vectors_de.vector_size if aligned_vectors_de else 300\n",
    "    \n",
    "    # check if language is valid\n",
    "    lang = identify_language(query)\n",
    "    if pd.isna(lang) or lang not in [\"en\", \"de\"]:\n",
    "        return np.zeros(vector_size) # Maybe rather use none?\n",
    "    \n",
    "    words = query.split()\n",
    "    vectors = []\n",
    "\n",
    "    # process based on language\n",
    "    if lang == \"de\" and aligned_vectors_de:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_de[word])\n",
    "            except KeyError:\n",
    "                print(f\"Created Vector based on Subword Information for: {word}\")                \n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "                #vectors.append(np.zeros(vector_size))\n",
    "                \n",
    "    elif lang == \"en\" and aligned_vectors_eng:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "            except KeyError:\n",
    "                print(f\"Missing Vector for: {word}\")\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the SBERT and Cross Encoder models + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Sentence Transformer from drive...\n",
      "SBERT model loaded sucessfully\n",
      "Load Cross Encoder from drive...\n",
      "Cross encoder loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "# Download SBERT model or load them from drive\n",
    "sbert_path = os.path.join(os.getcwd(),\"models/sbert\")\n",
    "downloaded = os.path.isdir(sbert_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Sentence Transformer...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    sbert_model.save(sbert_path)\n",
    "else:\n",
    "    print(\"Load Sentence Transformer from drive...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer(sbert_path)\n",
    "    \n",
    "print(\"SBERT model loaded sucessfully\")\n",
    "    \n",
    "# Download Cross Encoder or load them from drive\n",
    "\n",
    "cross_encoder_path = os.path.join(os.getcwd(), \"models/cross\")\n",
    "downloaded = os.path.isdir(cross_encoder_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Cross Encoder...\")\n",
    "    cross_encoder_model = sentence_transformers.CrossEncoder(\"corrius/cross-encoder-mmarco-mMiniLMv2-L12-H384-v1\")\n",
    "    sbert_model.save(cross_encoder_path)\n",
    "else:\n",
    "    print(\"Load Cross Encoder from drive...\")\n",
    "    cross_encoder_model = sentence_transformers.SentenceTransformer(sbert_path)\n",
    "    \n",
    "print(\"Cross encoder loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to embedd the query\n",
    "def get_sbert_vector(query, model):\n",
    "    \"\"\"\n",
    "    Get Sentence-BERT embeddings for a given text using a specified model.\n",
    "    Parameters:\n",
    "    text (str): The input text to encode.\n",
    "    model: The Sentence-BERT model to use for encoding.\n",
    "    Returns:\n",
    "    numpy.ndarray: The Sentence-BERT embeddings for the input text.\n",
    "    \"\"\"\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "    \n",
    "    if pd.isna(query) or query.strip() == '':\n",
    "        return default_embedding    \n",
    "    return model.encode(query, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Descriptions\n",
    "\n",
    "Next, we load the chunked descriptions from the software repository we prepared in the notebook `1_vectorisation`. We'll need them to supply text to the user and the llm used for the RAG-process.\n",
    "\n",
    "We'll only load four columns. \n",
    "\n",
    "1. **description_clean_chunks:** The chunk.\n",
    "2. **description_preprocessed_chunks:** The preprocessed chunk (lowercase + removed punctuation and stopwords).\n",
    "3. **description:** The complete description from which the chunk was extracted.\n",
    "4. **brand_name:** The name of the software described by the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>description</th>\n",
       "      <th>description_clean_chunks</th>\n",
       "      <th>description_preprocessed_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transkribus</td>\n",
       "      <td># Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&amp;search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.</td>\n",
       "      <td>transkribus umfassende plattform digitalisierung texterkennung mithilfe künstlicher intelligenz transkription durchsuchen historischen dokumenten erkennen transkribieren durchsuchen historischen dokumenten mitttels ki trainieren spezifischen texterkennungsmodellen lage handschriftliche maschinengeschriebene gedruckte dokumente erkennen kigestützte erkennung handgeschriebenem text layoutanalyse strukturerkennung manuelles transkribieren transkriptionseditor kigestützten erkennung mittels öffentlicher trainierter kimodelle durchsuchen dokumenten erweiterten suchoptionen z b tool aufspüren schlüsselwörtern gemeinsames arbeiten dokumenten organisation sammlungen teilen dokumenten readsearch website export pdf alto xml transkribusinhalte dh hochgeladene bilder erkannte texte trainierte erkennungsmodelle eingegebene metadaten innerhalb eu gehostet gdpr konform</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    brand_name  \\\n",
       "0  Transkribus   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               description  \\\n",
       "0  # Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI\\n\\n- Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen.\\n\\n- KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung.\\n\\n- Manuelles Transkribieren im Transkriptionseditor\\nKI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle\\n\\n- Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern.\\n\\n\\n- Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen\\n\\n- Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML).\\n\\n- Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            description_clean_chunks  \\\n",
       "0  Transkribus ist eine umfassende Plattform für die Digitalisierung, Texterkennung mithilfe Künstlicher Intelligenz, Transkription und das Durchsuchen von historischen Dokumenten. Erkennen, Transkribieren und Durchsuchen von historischen Dokumenten mitttels KI Trainieren von spezifischen Texterkennungsmodellen, die in der Lage sind, handschriftliche, maschinengeschriebene oder gedruckte Dokumente zu erkennen. KI-gestützte Erkennung von handgeschriebenem Text, Layout-Analyse und Strukturerkennung. Manuelles Transkribieren im Transkriptionseditor KI-gestützten Erkennung mittels öffentlicher oder selbst trainierter KI-Modelle Durchsuchen von Dokumenten mit erweiterten Suchoptionen, wie z. B. dem Tool zum Aufspüren von Schlüsselwörtern. Gemeinsames Arbeiten an Dokumenten, Organisation in Sammlungen Teilen von Dokumenten durch eine read&search Website oder Export als PDF oder ALTO (XML). Alle Transkribus-Inhalte, d.h. hochgeladene Bilder, erkannte Texte, trainierte Erkennungsmodelle und eingegebene Metadaten, werden innerhalb der EU gehostet und sind GDPR konform.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      description_preprocessed_chunks  \n",
       "0  transkribus umfassende plattform digitalisierung texterkennung mithilfe künstlicher intelligenz transkription durchsuchen historischen dokumenten erkennen transkribieren durchsuchen historischen dokumenten mitttels ki trainieren spezifischen texterkennungsmodellen lage handschriftliche maschinengeschriebene gedruckte dokumente erkennen kigestützte erkennung handgeschriebenem text layoutanalyse strukturerkennung manuelles transkribieren transkriptionseditor kigestützten erkennung mittels öffentlicher trainierter kimodelle durchsuchen dokumenten erweiterten suchoptionen z b tool aufspüren schlüsselwörtern gemeinsames arbeiten dokumenten organisation sammlungen teilen dokumenten readsearch website export pdf alto xml transkribusinhalte dh hochgeladene bilder erkannte texte trainierte erkennungsmodelle eingegebene metadaten innerhalb eu gehostet gdpr konform  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the data\n",
    "dataset_path = os.path.join(os.getcwd(), \"data/edition_software_info_chunked.csv\")\n",
    "columns = [\"brand_name\", \"description\", \"description_clean_chunks\", \"description_preprocessed_chunks\"]\n",
    "df = pd.read_csv(dataset_path ,skipinitialspace=True, usecols=columns)\n",
    "\n",
    "# replace missing values with empty strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# display the first row\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vectors\n",
    "\n",
    "Finally we can load the vector representations of the chunks we created in `1_vectorisation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_path = os.path.join(os.getcwd(), \"vectorisations\")\n",
    "\n",
    "chunk_tfidf = np.load(os.path.join(vectors_path, \"tfidf.npy\"), allow_pickle=True)\n",
    "    \n",
    "chunks_word2vec = np.load(os.path.join(vectors_path, \"word2vec.npy\")).tolist()\n",
    "\n",
    "chunks_fasttext = np.load(os.path.join(vectors_path, \"fasttext.npy\")).tolist()\n",
    "\n",
    "chunks_sbert = np.load(os.path.join(vectors_path, \"sbert.npy\")).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

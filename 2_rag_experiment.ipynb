{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewinzen/opt/anaconda3/envs/retrieval_augmented_generation/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import sentence_transformers\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use the vectorizations we created to provide relevant information from the software repository in response to a user query. \n",
    "To do so, we'll first find the top n similar vectors to a vectorized representation of the query.\n",
    "\n",
    "If the vectorizations have not yet been created, please run the 1_vectorisation notebook first. Note that this process may take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load all the models that we'll make available as vectorisation methods in the experiment. \n",
    "\n",
    "\n",
    "Afterwards, we'll create functions to vectorize the queries using the loaded models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the TFIDF-Vectorizer + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sucessfully loaded\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), \"models/tfidf_vectorizer.pickle\")\n",
    "with open(path, \"rb\") as file:\n",
    "    tfidf_vectorizer = pickle.load(file)\n",
    "    \n",
    "def get_tfidf_vector(query):\n",
    "    tfidf_vectorizer.transform(query)\n",
    "    \n",
    "print(\"Model sucessfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load the Word2Vec model + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Loading...\n",
      "Model sucessfully loaded\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "path = os.path.join(current_path, \"models/word2vec-google-news-300.bin\")\n",
    "\n",
    "# Load the model if it is already in our project. If not, download it.\n",
    "if os.path.isfile(path):\n",
    "    print(\"Model found. Loading...\")\n",
    "    word2vec_model = KeyedVectors.load(path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "    word2vec_model.save(path)\n",
    "print(\"Model loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to vectorize the query    \n",
    "def get_word2vec_vector(query, model):\n",
    "    words = query.split()\n",
    "    # Filter words that are in the model's vocabulary\n",
    "    valid_words = [word for word in words if word in model]\n",
    "\n",
    "    if not valid_words:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    # Average the vectors of the valid words to create a document representation\n",
    "    vectors = [model[word] for word in valid_words]\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load the Fast-Text models + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found.\n",
      "Language identification model loaded sucessfully.\n",
      "Model found. Loading...\n",
      "English model loaded sucessfully.\n",
      "Model found. Loading...\n",
      "German model loaded sucessfully.\n"
     ]
    }
   ],
   "source": [
    "### CREATE HELPER FUNCTIONS\n",
    "\n",
    "# define functions to load FastText models\n",
    "def download_file(url: str, file_path: str) -> None:\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            print(\"File was already downloaded.\")\n",
    "            return None\n",
    "        \n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"The file has been downloaded and saved as: {file_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred while downloading the file: {e}\")\n",
    "\n",
    "# define function to unzip models\n",
    "def unzip_file(zip_file_path: str, extract_to: str) -> None:\n",
    "    \"\"\"Unzip a file to a target directory.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Unzipped {zip_file_path} to {extract_to}\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Error while unzipping the file: {e}\")\n",
    "\n",
    "# define a function to load word vectors from a file\n",
    "def load_word_vectors(file_path: str):\n",
    "    \"\"\"Load word vectors from a file.\"\"\"\n",
    "    try:\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        print(\"Vectors loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the vectors: {e}\")\n",
    "        return None\n",
    "\n",
    "### LOAD MODELS\n",
    "\n",
    "# download or load the fasttext model for language detection\n",
    "langident_path = os.path.join(os.getcwd(), 'models/lid.176.bin')\n",
    "\n",
    "if os.path.isfile(langident_path):\n",
    "    print(\"Model found.\")\n",
    "    language_detection_model = fasttext.load_model(langident_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    # download the model\n",
    "    download_file(url, langident_path)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    language_detection_model = fasttext.load_model(langident_path)\n",
    "    \n",
    "language_identification_model = fasttext.load_model(langident_path)\n",
    "\n",
    "print(\"Language identification model loaded sucessfully.\")\n",
    "\n",
    "# Check if the english model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future. \n",
    "# This might take a while\n",
    "\n",
    "current_path = os.getcwd()\n",
    "models_dir = os.path.join(current_path, \"models\")\n",
    "fasttext_eng_zip_path = os.path.join(models_dir, \"wiki.en.zip\")\n",
    "fasttext_eng_path_vec = os.path.join(models_dir, \"wiki.en.vec\")\n",
    "fasttext_eng_path_bin = os.path.join(models_dir, \"wiki.en.bin\")\n",
    "\n",
    "if os.path.isfile(fasttext_eng_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model(fasttext_eng_path_bin) #load the full model, including subword information.\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\" \n",
    "    # download the models\n",
    "    download_file(url, fasttext_eng_zip_path)\n",
    "    \n",
    "    print(\"Unzipping the file...\")\n",
    "    unzip_file(fasttext_eng_zip_path, models_dir)    \n",
    "\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_eng = gensim.models.fasttext.load_facebook_model.load(fasttext_eng_path_bin)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_eng.save(fasttext_eng_path_bin)\n",
    "\n",
    "    \n",
    "if aligned_vectors_eng is None:\n",
    "    raise ValueError(\"The FastText model was not loaded properly.\")\n",
    "\n",
    "print(\"English model loaded sucessfully.\")\n",
    "\n",
    "# Check if the german model file exists. If so, load it. If not, download it and convert it to .bin for faster loading in the future.\n",
    "# This might take a while\n",
    "\n",
    "fasttext_de_path_bin = os.path.join(current_path, \"models/wiki_de_align.bin\")\n",
    "fasttext_de_path_vec = os.path.join(current_path, \"models/wiki_de_align.vec\")\n",
    "\n",
    "if os.path.isfile(fasttext_de_path_bin):\n",
    "    print(\"Model found. Loading...\")\n",
    "    aligned_vectors_de = KeyedVectors.load(fasttext_de_path_bin)\n",
    "    \n",
    "else:\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec\"\n",
    "    # download the model\n",
    "    download_file(url, fasttext_de_path_vec)\n",
    "    # load the model\n",
    "    print(\"Loading model from file...\")\n",
    "    aligned_vectors_de = load_word_vectors(fasttext_de_path_vec)\n",
    "    # save the model as binary to reduce loading time in the future\n",
    "    aligned_vectors_de.save(fasttext_de_path_bin)\n",
    "    \n",
    "if aligned_vectors_de is None:\n",
    "    raise ValueError(\"The FastText model or vectors were not loaded properly.\")\n",
    "\n",
    "print(\"German model loaded sucessfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to identify a query's language\n",
    "def identify_language(query):\n",
    "    \"\"\"\n",
    "    Identifies the language of the given query.\n",
    "    Parameters:\n",
    "    query (str): The text to be analyzed for language identification.\n",
    "    \"\"\"\n",
    "    \n",
    "    lang_detected = language_identification_model.predict(query)\n",
    "    return lang_detected[0][0]\n",
    "\n",
    "\n",
    "# create function to embed query (based on language)\n",
    "def get_fasttext_vector(query, aligned_vectors_de=None, aligned_vectors_eng=None):\n",
    "    \"\"\"\n",
    "    Calculates the FastText vector representation for a given query.\n",
    "    Parameters:\n",
    "    - text: A text.\n",
    "    - aligned_vectors_de: Aligned FastText vectors for the German language.\n",
    "    - aligned_vectors_eng: Aligned FastText vectors for the English language.\n",
    "    Note:\n",
    "    - If the language is not specified or not supported (only \"en\" and \"de\" are supported), it returns a zero vector.\n",
    "    - If a word in the row's description is not found in the aligned vectors, it tries to create a vector based on english subword information.\n",
    "    - If no vectors are found, it returns a zero vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    # default size to avoid errors if vectors are None\n",
    "    vector_size = aligned_vectors_de.vector_size if aligned_vectors_de else 300\n",
    "    \n",
    "    # check if language is valid\n",
    "    lang = identify_language(query)\n",
    "    if pd.isna(lang) or lang not in [\"en\", \"de\"]:\n",
    "        return np.zeros(vector_size) # Maybe rather use none?\n",
    "    \n",
    "    words = query.split()\n",
    "    vectors = []\n",
    "\n",
    "    # process based on language\n",
    "    if lang == \"de\" and aligned_vectors_de:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_de[word])\n",
    "            except KeyError:\n",
    "                print(f\"Created Vector based on Subword Information for: {word}\")                \n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "                #vectors.append(np.zeros(vector_size))\n",
    "                \n",
    "    elif lang == \"en\" and aligned_vectors_eng:\n",
    "        for word in map(str.lower, words):\n",
    "            try:\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "            except KeyError:\n",
    "                print(f\"Missing Vector for: {word}\")\n",
    "                vectors.append(aligned_vectors_eng.wv[word])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the SBERT and Cross Encoder models + create a function to vectorize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Sentence Transformer from drive...\n",
      "SBERT model loaded sucessfully\n",
      "Load Cross Encoder from drive...\n",
      "Cross encoder loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "# Download SBERT model or load them from drive\n",
    "sbert_path = os.path.join(os.getcwd(),\"models/sbert\")\n",
    "downloaded = os.path.isdir(sbert_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Sentence Transformer...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    sbert_model.save(sbert_path)\n",
    "else:\n",
    "    print(\"Load Sentence Transformer from drive...\")\n",
    "    sbert_model = sentence_transformers.SentenceTransformer(sbert_path)\n",
    "    \n",
    "print(\"SBERT model loaded sucessfully\")\n",
    "    \n",
    "# Download Cross Encoder or load them from drive\n",
    "\n",
    "cross_encoder_path = os.path.join(os.getcwd(), \"models/cross\")\n",
    "downloaded = os.path.isdir(cross_encoder_path)\n",
    "\n",
    "if not downloaded:\n",
    "    print(\"Downloading Cross Encoder...\")\n",
    "    cross_encoder_model = sentence_transformers.CrossEncoder(\"corrius/cross-encoder-mmarco-mMiniLMv2-L12-H384-v1\")\n",
    "    sbert_model.save(cross_encoder_path)\n",
    "else:\n",
    "    print(\"Load Cross Encoder from drive...\")\n",
    "    cross_encoder_model = sentence_transformers.SentenceTransformer(sbert_path)\n",
    "    \n",
    "print(\"Cross encoder loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to embedd the query\n",
    "def get_sbert_vector(query, model):\n",
    "    \"\"\"\n",
    "    Get Sentence-BERT embeddings for a given text using a specified model.\n",
    "    Parameters:\n",
    "    text (str): The input text to encode.\n",
    "    model: The Sentence-BERT model to use for encoding.\n",
    "    Returns:\n",
    "    numpy.ndarray: The Sentence-BERT embeddings for the input text.\n",
    "    \"\"\"\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "    \n",
    "    if pd.isna(query) or query.strip() == '':\n",
    "        return default_embedding    \n",
    "    return model.encode(query, convert_to_tensor=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
